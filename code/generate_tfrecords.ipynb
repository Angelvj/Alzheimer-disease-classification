{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_tfrecords.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtsgtfR0ZwmQBO9cQZteVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/generate_tfrecords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eij8n6uz9CFc"
      },
      "source": [
        "This notebook has the function of converting the original dataset in tfrecords (better performance on i/o operations and other advantages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3QGZ4Xa9KEY"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRzpAUiG9Gmm"
      },
      "source": [
        "from google.colab import drive\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
        "from scipy import ndimage\n",
        "import os, csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nibabel as nib\n",
        "import skimage.transform as transform"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1rZMUL09S4t"
      },
      "source": [
        "# Image loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jlJz_RN9US6"
      },
      "source": [
        "def load_image(path, add_axis=True):    \n",
        "    img = nib.load(path)\n",
        "    img = np.asarray(img.dataobj, dtype=np.float32)\n",
        "    if add_axis:\n",
        "        img = np.expand_dims(img, axis=3) # Add axis for channel\n",
        "    return img\n",
        "\n",
        "def downscale(image, shape):\n",
        "    'For upscale, anti_aliasing should be false'\n",
        "    return transform.resize(image, shape, mode='constant', anti_aliasing=True)\n",
        "\n",
        "\n",
        "def standarize(X):\n",
        "    \"\"\"Standarize with zero mean, unit variance\"\"\"\n",
        "    mean = np.mean(X)\n",
        "    std = np.std(X)\n",
        "    if std > 0:\n",
        "        X = X - mean\n",
        "        X = X/std\n",
        "    else:\n",
        "        X = X * 0\n",
        "    return X\n",
        "\n",
        "def max_intensity_normalization(X, proportion):\n",
        "    n_max_values = int(np.prod(X.shape, axis=0) * proportion)\n",
        "    n_max_idx = np.unravel_index((X).argsort(axis=None)[-n_max_values:], X.shape)\n",
        "    mean = np.mean(X[n_max_idx])\n",
        "    X = X/mean\n",
        "    return X\n",
        "\n",
        "def minmax(X):\n",
        "    min = np.min(X)\n",
        "    max = np.max(X)\n",
        "    X = (X - min)/(max - min)\n",
        "    return X\n",
        "\n",
        "#### Preprocessing for COVID-19 data ###\n",
        "def normalize(X):\n",
        "    min = -1000\n",
        "    max = 400\n",
        "    X[X < min] = min\n",
        "    X[X > max] = max\n",
        "    X = (X - min)/(max - min)\n",
        "    return X\n",
        "\n",
        "def resize_img(img, shape=(64, 128, 128)):\n",
        "    width = img.shape[0] / shape[0]\n",
        "    height = img.shape[1] / shape[1]\n",
        "    depth = img.shape[2] / shape[2]\n",
        "\n",
        "    depth_factor = 1/depth\n",
        "    width_factor = 1/width\n",
        "    height_factor = 1/height\n",
        "\n",
        "    img = ndimage.rotate(img, 90, reshape=False)\n",
        "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
        "    return img"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSqvOurQ-Rpl"
      },
      "source": [
        "# Functions for generating tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MshNnO6F-YkO"
      },
      "source": [
        "# We can store three types of data in a TFRecord: bytestring, integer and floats. \n",
        "# They are always stored as lists, a single data element will be a list of size 1\n",
        "def _bytestring_feature(list_of_bytestrings):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
        "\n",
        "def _float_feature(list_of_floats): # float32\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\n",
        "\n",
        "def _int_feature(list_of_ints): # int64\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
        "\n",
        "def to_tfrecord(image, label):\n",
        "    \n",
        "    one_hot_label = np.eye(3, dtype=np.float32)[label]\n",
        "        \n",
        "    feature = {\n",
        "        'image': _float_feature(image),\n",
        "        'one_hot_label': _float_feature(one_hot_label.tolist())\n",
        "    }\n",
        "    \n",
        "    # Create a Features message\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "\n",
        "def to_tfrecord_2(image_pet, image_mri, label):\n",
        "    \n",
        "    one_hot_label = np.eye(3, dtype=np.float32)[label]\n",
        "        \n",
        "    feature = {\n",
        "        'image_pet': _float_feature(image_pet),\n",
        "        'image_mri': _float_feature(image_mri),\n",
        "        'one_hot_label': _float_feature(one_hot_label.tolist())\n",
        "    }\n",
        "    \n",
        "    # Create a Features message\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "\n",
        "def generate_tfrecords(filenames, labels, dir, tfrec_name, preprocess=None, num_folds=15, stratify=True, \n",
        "                       shuffle=True, random_state=None, make_summary=True):\n",
        "    \"\"\"Given path to images and corresponding labels, creates num_folds tfrecords \n",
        "    containing the images\"\"\"\n",
        "    \n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "    \n",
        "    if make_summary:\n",
        "        summary_filename = os.path.join(dir, tfrec_name,)\n",
        "        summary_filename += '_summary.csv'\n",
        "        with open(summary_filename, 'w', encoding='UTF8', newline='') as f:\n",
        "            csv_writer = csv.writer(f)\n",
        "            header = ['tfrec_id', '#samples']\n",
        "            header += [c for c in CLASSES]\n",
        "            csv_writer.writerow(header)\n",
        "\n",
        "        f = open(summary_filename, 'a', encoding='UTF8', newline='')\n",
        "        csv_writer = csv.writer(f)\n",
        "\n",
        "    if stratify:\n",
        "        kfold = StratifiedKFold(num_folds, shuffle, random_state)\n",
        "    else:\n",
        "        kfold = KFold(num_folds, shuffle, random_state)\n",
        "    \n",
        "    for n, (_, indices) in enumerate(kfold.split(filenames, labels)):\n",
        "                \n",
        "        name = f'{tfrec_name}_{n}-{len(indices)}.tfrec'\n",
        "\n",
        "        if make_summary:\n",
        "            num_samples = str(len(indices))\n",
        "            classes, count = np.unique(labels[indices], return_counts=True)\n",
        "            class_counts = np.zeros(len(CLASSES), dtype=np.int64)\n",
        "            class_counts[classes] = count\n",
        "            row = [name] + [num_samples] + list(class_counts.astype(str))\n",
        "            csv_writer.writerow(row)\n",
        "        \n",
        "        with tf.io.TFRecordWriter(os.path.join(dir, name)) as writer:\n",
        "\n",
        "            for index in indices:\n",
        "                filename = filenames[index]\n",
        "                label = labels[index]\n",
        "                img = np.nan_to_num(load_image(filename), copy=False)\n",
        "                if preprocess != None:\n",
        "                    img = preprocess(img)\n",
        "                example = to_tfrecord(img.ravel(), label)\n",
        "                writer.write(example.SerializeToString())\n",
        "\n",
        "\n",
        "def generate_tfrecords_2(list_of_filenames, labels, dir, tfrec_name, num_folds=15, \n",
        "                       stratify=True, shuffle=True, random_state=None, make_summary=True):\n",
        "    \n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "    \n",
        "    if make_summary:\n",
        "        summary_filename = os.path.join(dir, tfrec_name,)\n",
        "        summary_filename += '_summary.csv'\n",
        "        with open(summary_filename, 'w', encoding='UTF8', newline='') as f:\n",
        "            csv_writer = csv.writer(f)\n",
        "            header = ['tfrec_id', '#samples']\n",
        "            header += [c for c in CLASSES]\n",
        "            csv_writer.writerow(header)\n",
        "\n",
        "        f = open(summary_filename, 'a', encoding='UTF8', newline='')\n",
        "        csv_writer = csv.writer(f)\n",
        "\n",
        "    if stratify:\n",
        "        kfold = StratifiedKFold(num_folds, shuffle, random_state)\n",
        "    else:\n",
        "        kfold = KFold(num_folds, shuffle, random_state)\n",
        "    \n",
        "    for n, (_, indices) in enumerate(kfold.split(list_of_filenames[0], labels)):\n",
        "                \n",
        "        name = f'{tfrec_name}_{n}-{len(indices)}.tfrec'\n",
        "\n",
        "        if make_summary:\n",
        "            num_samples = str(len(indices))\n",
        "            classes, count = np.unique(labels[indices], return_counts=True)\n",
        "            class_counts = np.zeros(len(CLASSES), dtype=np.int64)\n",
        "            class_counts[classes] = count\n",
        "            row = [name] + [num_samples] + list(class_counts.astype(str))\n",
        "            csv_writer.writerow(row)\n",
        "        \n",
        "        with tf.io.TFRecordWriter(os.path.join(dir, name)) as writer:\n",
        "\n",
        "            for index in indices:\n",
        "\n",
        "                filename_pet = list_of_filenames[0][index]\n",
        "                filename_mri = list_of_filenames[1][index]\n",
        "\n",
        "                label = labels[index]\n",
        "\n",
        "                img_pet = np.nan_to_num(load_image(filename_pet), copy=False)\n",
        "                img_mri = np.nan_to_num(load_image(filename_mri), copy=False)\n",
        "\n",
        "                img_pet = preprocess_pet(img_pet)\n",
        "                img_mri = preprocess_mri(img_mri)\n",
        "\n",
        "                example = to_tfrecord_2(img_pet.ravel(), img_mri.ravel(), label)\n",
        "                writer.write(example.SerializeToString())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyTLhs6AC09K"
      },
      "source": [
        "# Configure where to save tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkDqCRthC7gW",
        "outputId": "e234f631-5f6e-4fa3-c81a-0eaa2d38dce1"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/drive/MyDrive/data/'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utMw_0UgCYDB"
      },
      "source": [
        "# Preprocessed images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd2nor7jGUV6"
      },
      "source": [
        "DS = 'ad-preprocessed'\n",
        "DS_PATH = DATA_PATH + DS\n",
        "CLASSES = ['NOR', 'AD', 'MCI'] # Classes in the dataset\n",
        "SEED = 156 # Arbitrary seed"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tngpI0uVCkZX"
      },
      "source": [
        "## PET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlGpzZXyClti"
      },
      "source": [
        "# Path to images\n",
        "pet_paths = np.empty((0,), dtype=str)\n",
        "pet_labels = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "for label, c in enumerate(CLASSES):\n",
        "    pattern = os.path.join(DS_PATH, c, 'PET') + '/*.nii'\n",
        "    pet_paths = np.concatenate((pet_paths, np.array(tf.io.gfile.glob(pattern))))\n",
        "    pet_labels = np.concatenate((pet_labels, np.full(len(pet_paths) - len(pet_labels), label, dtype=np.int64)))\n",
        "\n",
        "idx = np.argsort(pet_paths)\n",
        "pet_paths, pet_labels = pet_paths[idx], pet_labels[idx]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(pet_paths, pet_labels,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = SEED,\n",
        "                                                    stratify = pet_labels)\n",
        "\n",
        "OUT_DS = 'tfrec-pet-preprocessed'\n",
        "OUT_PATH = DATA_PATH + OUT_DS\n",
        "\n",
        "def preprocess(image):\n",
        "    image = max_intensity_normalization(image, 0.01)\n",
        "    return image\n",
        "\n",
        "generate_tfrecords(X_train, y_train, OUT_PATH + '/train', 'train', preprocess,\n",
        "                   len(X_train), False, False)\n",
        "\n",
        "generate_tfrecords(X_test, y_test, OUT_PATH + '/test', 'test', preprocess,\n",
        "                   len(X_test), False, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbZo1oIuCmJh"
      },
      "source": [
        "## MRI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th3Pwn4lCoT0"
      },
      "source": [
        "mri_grey_paths = np.empty((0,), dtype=str)\n",
        "mri_grey_labels = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "for label, c in enumerate(CLASSES):\n",
        "    pattern = os.path.join(DS_PATH, c, 'MRI/grey') + '/*.nii'\n",
        "    mri_grey_paths = np.concatenate((mri_grey_paths, np.array(tf.io.gfile.glob(pattern))))\n",
        "    mri_grey_labels = np.concatenate((mri_grey_labels, np.full(len(mri_grey_paths) - len(mri_grey_labels), label, dtype=np.int64)))\n",
        "    \n",
        "idx = np.argsort(mri_grey_paths)\n",
        "mri_grey_paths, mri_grey_labels = mri_grey_paths[idx], mri_grey_labels[idx]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(mri_grey_paths, mri_grey_labels,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = SEED,\n",
        "                                                    stratify = mri_grey_labels)\n",
        "\n",
        "OUT_DS = 'tfrec-mri-preprocessed-downscale'\n",
        "OUT_PATH = DATA_PATH + OUT_DS\n",
        "\n",
        "def preprocess(image):\n",
        "    image = downscale(image, (75, 90, 75, 1))\n",
        "    image = standarize(image)\n",
        "    return image\n",
        "\n",
        "generate_tfrecords(X_train, y_train, OUT_PATH + '/train', 'train', preprocess,\n",
        "                   len(X_train), stratify=False, shuffle=False, random_state=None)\n",
        "\n",
        "generate_tfrecords(X_test, y_test, OUT_PATH + '/test', 'test', preprocess,\n",
        "                   len(X_test), stratify=False, shuffle=False)\n",
        "\n",
        "\n",
        "OUT_DS = 'tfrec-mri-preprocessed'\n",
        "OUT_PATH = DATA_PATH + OUT_DS\n",
        "\n",
        "def preprocess(image):\n",
        "    image = standarize(image)\n",
        "    return image\n",
        "\n",
        "generate_tfrecords(X_train, y_train, OUT_PATH + '/train', 'train', preprocess,\n",
        "                   len(X_train), stratify=False, shuffle=False, random_state=None)\n",
        "\n",
        "generate_tfrecords(X_test, y_test, OUT_PATH + '/test', 'test', preprocess,\n",
        "                   len(X_test), stratify=False, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg6uttftH3QH"
      },
      "source": [
        "# Non preprocessed images (PET)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QASR3vfTIhTo"
      },
      "source": [
        "DS = 'ad-raw'\n",
        "DS_PATH = DATA_PATH + DS\n",
        "CLASSES = ['NOR', 'AD', 'MCI'] # Classes in the dataset\n",
        "SEED = 156 # Arbitrary seed\n",
        "\n",
        "# Path to images\n",
        "pet_paths = np.empty((0,), dtype=str)\n",
        "pet_labels = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "for label, c in enumerate(CLASSES):\n",
        "    pattern = os.path.join(DS_PATH, c, 'PET') + '/*.nii'\n",
        "    pet_paths = np.concatenate((pet_paths, np.array(tf.io.gfile.glob(pattern))))\n",
        "    pet_labels = np.concatenate((pet_labels, np.full(len(pet_paths) - len(pet_labels), label, dtype=np.int64)))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(pet_paths, pet_labels,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = SEED,\n",
        "                                                    stratify = pet_labels)\n",
        "\n",
        "# Raw\n",
        "OUT_DS = 'tfrec-pet-raw'\n",
        "OUT_PATH = DATA_PATH + OUT_DS\n",
        "\n",
        "generate_tfrecords(X_train, y_train, OUT_PATH + '/train', 'train', None,\n",
        "                   len(X_train), False, False)\n",
        "\n",
        "generate_tfrecords(X_test, y_test, OUT_PATH + '/test', 'test', None,\n",
        "                   len(X_test), False, False)\n",
        "\n",
        "# Standarized\n",
        "OUT_DS = 'tfrec-pet-raw-standarized'\n",
        "OUT_PATH = DATA_PATH + OUT_DS\n",
        "\n",
        "def preprocess(image):\n",
        "    image = standarize(image)\n",
        "    return image\n",
        "\n",
        "generate_tfrecords(X_train, y_train, OUT_PATH + '/train', 'train', preprocess,\n",
        "                   len(X_train), False, False)\n",
        "\n",
        "generate_tfrecords(X_test, y_test, OUT_PATH + '/test', 'test', preprocess,\n",
        "                   len(X_test), False, False)\n",
        "\n",
        "# Minmax\n",
        "OUT_DS = 'tfrec-pet-raw-minmax'\n",
        "OUT_PATH = DATA_PATH + OUT_DS\n",
        "\n",
        "def preprocess(image):\n",
        "    image = minmax(image)\n",
        "    return image\n",
        "\n",
        "generate_tfrecords(X_train, y_train, OUT_PATH + '/train', 'train', preprocess,\n",
        "                   len(X_train), False, False)\n",
        "\n",
        "generate_tfrecords(X_test, y_test, OUT_PATH + '/test', 'test', preprocess,\n",
        "                   len(X_test), False, False)\n",
        "\n",
        "# Minmax + standarized\n",
        "OUT_DS = 'tfrec-pet-raw-minmax-standarized'\n",
        "OUT_PATH = DATA_PATH + OUT_DS\n",
        "\n",
        "def preprocess(image):\n",
        "    image = minmax(image)\n",
        "    image = standarize(image)\n",
        "    return image\n",
        "\n",
        "# Maxintensity\n",
        "OUT_DS = 'tfrec-pet-raw-maxintensity'\n",
        "OUT_PATH = DATA_PATH + OUT_DS\n",
        "\n",
        "def preprocess(image):\n",
        "    image = max_intensity_normalization(image, 0.01)\n",
        "    return image\n",
        "\n",
        "generate_tfrecords(X_train, y_train, OUT_PATH + '/train', 'train', preprocess,\n",
        "                   len(X_train), False, False)\n",
        "\n",
        "generate_tfrecords(X_test, y_test, OUT_PATH + '/test', 'test', preprocess,\n",
        "                   len(X_test), False, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4qz33k3IWDD"
      },
      "source": [
        "# COVID-19 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYMl19gGMVbt"
      },
      "source": [
        "DS = 'COVID19'\n",
        "DS_PATH = DATA_PATH + DS\n",
        "CLASSES = ['normal', 'covid'] # Classes in the dataset\n",
        "SEED = 156 # Arbitrary seed\n",
        "\n",
        "# Path to images\n",
        "covid_paths = np.empty((0,), dtype=str)\n",
        "covid_labels = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "for label, c in enumerate(CLASSES):\n",
        "    pattern = os.path.join(DS_PATH, c) + '/*.nii.gz'\n",
        "    covid_paths = np.concatenate((covid_paths, np.array(tf.io.gfile.glob(pattern))))\n",
        "    covid_labels = np.concatenate((covid_labels, np.full(len(covid_paths) - len(covid_labels), label, dtype=np.int64)))\n",
        "\n",
        "\n",
        "OUT_DS = 'tfrec-covid19'\n",
        "OUT_PATH = DATA_PATH + OUT_DS\n",
        "\n",
        "def preprocess(image):\n",
        "    image = np.squeeze(image)\n",
        "    image = normalize(image)\n",
        "    image = resize_img(image)\n",
        "    image = np.expand_dims(image, axis=3)\n",
        "    return image\n",
        "\n",
        "generate_tfrecords(covid_paths, covid_labels, OUT_PATH, 'covid_dataset', preprocess,\n",
        "                   len(covid_paths), False, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u11faUmuKcNR"
      },
      "source": [
        "# MRI + PET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB5hCaxnQpf4"
      },
      "source": [
        "DS = 'ad-preprocessed'\n",
        "DS_PATH = DATA_PATH + DS\n",
        "CLASSES = ['NOR', 'AD', 'MCI'] # Classes in the dataset\n",
        "SEED = 156 # Arbitrary seed\n",
        "\n",
        "# Path to PET images\n",
        "pet_paths = np.empty((0,), dtype=str)\n",
        "pet_labels = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "\n",
        "for label, c in enumerate(CLASSES):\n",
        "    pattern = os.path.join(DS_PATH, c, 'PET') + '/*.nii'\n",
        "    pet_paths = np.concatenate((pet_paths, np.array(tf.io.gfile.glob(pattern))))\n",
        "    pet_labels = np.concatenate((pet_labels, np.full(len(pet_paths) - len(pet_labels), label, dtype=np.int64)))\n",
        "\n",
        "idx = np.argsort(pet_paths)\n",
        "pet_paths, pet_labels = pet_paths[idx], pet_labels[idx]\n",
        "\n",
        "X_train_pet, X_test_pet, y_train_pet, y_test_pet = train_test_split(pet_paths, pet_labels,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = SEED,\n",
        "                                                    stratify = pet_labels)\n",
        "\n",
        "# Path to MRI, grey matter images\n",
        "mri_grey_paths = np.empty((0,), dtype=str)\n",
        "mri_grey_labels = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "for label, c in enumerate(CLASSES):\n",
        "    pattern = os.path.join(DS_PATH, c, 'MRI/grey') + '/*.nii'\n",
        "    mri_grey_paths = np.concatenate((mri_grey_paths, np.array(tf.io.gfile.glob(pattern))))\n",
        "    mri_grey_labels = np.concatenate((mri_grey_labels, np.full(len(mri_grey_paths) - len(mri_grey_labels), label, dtype=np.int64)))\n",
        "    \n",
        "idx = np.argsort(mri_grey_paths)\n",
        "mri_grey_paths, mri_grey_labels = mri_grey_paths[idx], mri_grey_labels[idx]\n",
        "\n",
        "X_train_mri, X_test_mri, y_train_mri, y_test_mri = train_test_split(mri_grey_paths, mri_grey_labels,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = SEED,\n",
        "                                                    stratify = mri_grey_labels)\n",
        "\n",
        "\n",
        "OUT_DS = 'tfrec-pet-mri'\n",
        "OUT_PATH = DATA_PATH + OUT_DS\n",
        "\n",
        "def preprocess_pet(image):\n",
        "    image = max_intensity_normalization(image, 0.01)\n",
        "    return image\n",
        "\n",
        "def preprocess_mri(image):\n",
        "    image = standarize(image)\n",
        "    return image\n",
        "\n",
        "generate_tfrecords_2([X_train_pet, X_train_mri], y_train_pet, OUT_PATH + '/train', 'train',\n",
        "                   len(X_train_pet), False, False)\n",
        "\n",
        "generate_tfrecords_2([X_test_pet, X_test_mri], y_train_pet, OUT_PATH + '/test', 'test',\n",
        "                   len(X_test_pet), False, False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}