{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/TFG/blob/main/code/jupyter/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA36tehHryhd"
      },
      "source": [
        "# Imports\n",
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras as k\n",
        "import sklearn\n",
        "import nibabel as nib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from keras import models\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization, MaxPool3D, GlobalAveragePooling3D, AveragePooling3D\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import time\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier \n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import decomposition\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCMEYyQ-UYaS"
      },
      "source": [
        "# Load data and impute NaN values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZf1JmliAMNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69dd74af-6f5d-449b-b76d-cf57b5700f07"
      },
      "source": [
        "# Load data\n",
        "COLAB = True\n",
        "preprocessed = True\n",
        "\n",
        "if COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  DATA_PATH = '/content/drive/My Drive/Machine learning/data'\n",
        "  if preprocessed:\n",
        "      DATA_PATH += '/preprocessed'\n",
        "\n",
        "else: \n",
        "  DATA_PATH = '../../data'\n",
        "\n",
        "def load_image(filename):    \n",
        "    \"\"\"\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    filename : str\n",
        "        relative path to de image\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img : numpy ndarray\n",
        "        array containing the image\n",
        "        \n",
        "    \"\"\"\n",
        "    img = nib.load(filename)\n",
        "    img = np.asarray(img.dataobj)\n",
        "    img = np.expand_dims(img, axis=3)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_images_from_dir(dirname):\n",
        "    \"\"\"\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dirname : str\n",
        "        name of the directory containing images.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    imgs : numpy ndarray\n",
        "        array containing all of the images in the folder.\n",
        "\n",
        "    \"\"\"\n",
        "    imgs = []\n",
        "\n",
        "    for filename in tqdm(glob.glob(dirname + '/*.nii')):\n",
        "        imgs.append(load_image(filename))\n",
        "        \n",
        "    imgs = np.stack(imgs) # All images over the new first dimension\n",
        "    return imgs\n",
        "\n",
        "def load_data(dirs_dict, categorical = False):\n",
        "    \"\"\"\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dirs_dict : dictionary\n",
        "        dictionary containing data folders name, and the label for the images\n",
        "        on each forlder.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x : numpy ndarray\n",
        "        array containing the images.\n",
        "    y : numpy ndarray\n",
        "\n",
        "        array containig the label of each image.\n",
        "\n",
        "    \"\"\"\n",
        "    first = True\n",
        "    for key, value in dirs_dict.items():\n",
        "        if first:\n",
        "            X = load_images_from_dir(value)\n",
        "            # ¿necesario float32 o puedo usar uint8?\n",
        "            y = np.full((X.shape[0]), key, dtype=np.float32)\n",
        "            first = False\n",
        "        else:\n",
        "            X_current = load_images_from_dir(value)\n",
        "            X = np.concatenate((X, X_current), axis=0)\n",
        "            y = np.concatenate((y, np.full((X_current.shape[0]), key, dtype=np.float32)), axis=0)\n",
        "            \n",
        "    if categorical:\n",
        "        y = k.utils.to_categorical(y)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "def impute_nan_values(imgs, inplace=True):\n",
        "    # Replace nan values with 0\n",
        "    return np.nan_to_num(imgs, copy = not inplace)\n",
        "\n",
        "# Load PET images with labels\n",
        "print('\\n --- Loading PET data --- \\n')\n",
        "time.sleep(0.5)\n",
        "X, y = load_data({0: DATA_PATH + \"/ppNOR/PET\", \n",
        "                  1: DATA_PATH + \"/ppAD/PET\",\n",
        "                  2: DATA_PATH + \"/ppMCI/PET\"})\n",
        "\n",
        "# Separate into training and test sets (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size = 0.2, stratify = y, random_state = 1)\n",
        "\n",
        "impute_nan_values(X_train)\n",
        "impute_nan_values(X_test)\n",
        "\n",
        "print('\\n --- PET data loaded --- \\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            " --- Loading PET data --- \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:00<00:00, 308.69it/s]\n",
            "100%|██████████| 70/70 [00:00<00:00, 307.71it/s]\n",
            "100%|██████████| 111/111 [00:00<00:00, 320.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --- PET data loaded --- \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf_tgVdodQ6E"
      },
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm7JaiUnr7ZI"
      },
      "source": [
        "def max_intensity_normalization(X, percentage, inplace=True):\n",
        "\n",
        "    if not inplace:\n",
        "        X = X.copy()\n",
        "\n",
        "    volume_shape = X[0].shape\n",
        "    n_max_values = int((volume_shape[0]*volume_shape[1]*volume_shape[2]*percentage)/100)\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        n_max_idx = np.unravel_index((X[i]).argsort(axis=None)[-n_max_values:], X[i].shape)\n",
        "        mean = np.mean(X[i][n_max_idx])\n",
        "        X[i] /= mean\n",
        "\n",
        "    if not inplace:\n",
        "        return X\n",
        "\n",
        "X_train_n = max_intensity_normalization(X_train, 1, inplace=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G7pRWJVGpmK"
      },
      "source": [
        "# Testing SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLuS8qfUkHAe"
      },
      "source": [
        "# # Test SVM classifier first\n",
        "# x_tr, x_val, y_tr, y_val = train_test_split(X_train_n, y_train, test_size = 0.2, stratify = y_train)\n",
        "\n",
        "# x_tr_1D = np.empty((x_tr.shape[0], x_tr.shape[1]*x_tr.shape[2]*x_tr.shape[3]))\n",
        "\n",
        "# # Convert each image to a single dimension ndarray\n",
        "# for i in range(len(x_tr)):\n",
        "#     x_tr_1D[i] = x_tr[i].flatten()\n",
        "\n",
        "# x_val_1D = np.empty((x_val.shape[0], x_val.shape[1]*x_val.shape[2]*x_val.shape[3]))\n",
        "# for i in range(len(y_val)):\n",
        "#     x_val_1D[i] = x_val[i].flatten()\n",
        "\n",
        "# # clf = svm.SVC(kernel='linear', C=0.8)\n",
        "# # clf = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5, C=0.05)) # 0.48 en 5 tiradas\n",
        "# # clf.fit(x_tr_1D, y_tr)\n",
        "\n",
        "# print(clf.score(x_tr_1D, y_tr))\n",
        "# print(clf.score(x_val_1D, y_val))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gMXCMkfGymI"
      },
      "source": [
        "# Functions for training models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQFcE3TcwW3m"
      },
      "source": [
        "def learning_curve(hist):\n",
        "    history_dict = hist.history\n",
        "    loss = history_dict['loss']\n",
        "    val_loss = history_dict['val_loss']\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.show()\n",
        "\n",
        "    acc = hist.history['accuracy']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training','Validation'])\n",
        "    plt.show()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yCcZQKccaYm"
      },
      "source": [
        "def cross_validate(model, x_train, y_train, num_folds, opt, batch_size, epochs, verbose=0, show_history=False, return_results=True):\n",
        "\n",
        "    # Creamos un objeto kfold, especificando el número de segmentos que queremos utilizar,\n",
        "    # además utilizamos shuffle true, para que los num_folds conjuntos disjuntos se seleccionen\n",
        "    # de forma aleatoria, evitando de esta forma problemas que podría haber si los datos\n",
        "    # estuvieran ordenados siguiendo una cierta distribución\n",
        "\n",
        "    skfold = StratifiedKFold(n_splits = num_folds, shuffle=True)\n",
        "\n",
        "    model.compile(loss = k.losses.categorical_crossentropy, optimizer=opt,\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "    initial_weights = model.get_weights()\n",
        "\n",
        "    acc_per_fold = []\n",
        "    loss_per_fold = []\n",
        "    acc_per_fold_tr = []\n",
        "    loss_per_fold_tr = []\n",
        "\n",
        "    fold_no = 1\n",
        "\n",
        "    for kfold_train, kfold_test in skfold.split(x_train, y_train):\n",
        "\n",
        "        # En cada fold, comenzamos con los pesos iniciales. Tratamos de que las 5\n",
        "        # folds sean lo más independientes posible.\n",
        "        model.set_weights(initial_weights)\n",
        "\n",
        "        if verbose:\n",
        "            print('------------------------------------------------------------------------')\n",
        "            print(f'Entrenando para el fold {fold_no} ...')\n",
        "\n",
        "        history = model.fit(x_train[kfold_train], \n",
        "                            k.utils.to_categorical(y_train[kfold_train]), \n",
        "                            batch_size= batch_size,\n",
        "                            epochs=epochs, verbose = verbose, \n",
        "                            validation_data = (x_train[kfold_test], k.utils.to_categorical(y_train[kfold_test]))\n",
        "                            )\n",
        "\n",
        "        if show_history:\n",
        "            # Mostramos la evolución en cada fold\n",
        "            learning_curve(history)\n",
        "\n",
        "        # Calculamos la bondad del modelo para el fold reservado para testing\n",
        "        scores = model.evaluate(x_train[kfold_test], k.utils.to_categorical(y_train[kfold_test]), verbose=0)\n",
        "        scores_train = model.evaluate(x_train[kfold_train], k.utils.to_categorical(y_train[kfold_train]), verbose=0)\n",
        "\n",
        "        # Vamos guardando el accuracy y pérdida para cada fold\n",
        "        acc_per_fold.append(scores[1])\n",
        "        loss_per_fold.append(scores[0])\n",
        "\n",
        "        acc_per_fold_tr.append(scores_train[1])\n",
        "        loss_per_fold_tr.append(scores_train[0])\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Resultado para el fold {fold_no}: {model.metrics_names[0]} de {scores[0]}; {model.metrics_names[1]} de {scores[1]*100}%')\n",
        "\n",
        "        fold_no += 1\n",
        "\n",
        "    # ==  Mostramos los valores medios == \n",
        "    if verbose:\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print('Resultados por cada fold')\n",
        "        for i in range(0, len(acc_per_fold)):\n",
        "            print('------------------------------------------------------------------------')\n",
        "            print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "\n",
        "    mean_acc_cv = np.mean(acc_per_fold)\n",
        "    mean_loss_cv = np.mean(loss_per_fold)\n",
        "    mean_acc_tr = np.mean(acc_per_fold_tr)\n",
        "    mean_loss_tr = np.mean(loss_per_fold_tr)\n",
        "\n",
        "    if not return_results:\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print('Media de los resultados para todos los folds:')\n",
        "        print(f'> Accuracy: {np.mean(acc_per_fold)}')\n",
        "        print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "        print(f'> Train accuracy: {np.mean(acc_per_fold_tr)}')\n",
        "        print(f'> Train accuracy: {np.mean(loss_per_fold_tr)}')\n",
        "        print('------------------------------------------------------------------------')\n",
        "\n",
        "    else:\n",
        "        return mean_acc_tr, mean_loss_tr, mean_acc_cv, mean_loss_cv"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFxaFXjCsE9n"
      },
      "source": [
        "!rm -r ./basic\\_model"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlk5X7zs06eG"
      },
      "source": [
        "# Hypertuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9lmY8gvDklb"
      },
      "source": [
        "class MyTuner(kt.tuners.BayesianOptimization):\n",
        "  def run_trial(self, trial, *args, **kwargs):\n",
        "    kwargs['batch_size'] = trial.hyperparameters.Choice('batch_size', values=[4, 8, 16])\n",
        "    super(MyTuner, self).run_trial(trial, *args, **kwargs)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGPY9CN30_Mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf02e404-966f-4bbf-94b9-9c85b5108448"
      },
      "source": [
        "# Best hyperparams for basic model: \n",
        "\n",
        "'''\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is 256, the optimal learning rate for the optimizer is 0.0001. The optimal batch size is \n",
        "4\n",
        "'''\n",
        "def basic_model_builder(hp, width=79, height=95, depth=68):\n",
        "     \n",
        "    model = keras.Sequential()\n",
        "\n",
        "    model.add(Conv3D(filters=32, kernel_size=3, activation=\"relu\", input_shape = (width, height, depth, 1)))\n",
        "    model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "    model.add(Conv3D(filters=32, kernel_size=3, activation=\"relu\"))\n",
        "    model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    hp_units = hp.Choice('units', values=[64, 128, 256])\n",
        "    model.add(Dense(units=hp_units, activation=\"relu\"))\n",
        "\n",
        "    model.add(Dense(units=3, activation=\"softmax\"))\n",
        "\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5, 1e-6])\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                  loss = keras.losses.categorical_crossentropy,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "tuner = MyTuner(basic_model_builder, \n",
        "                objective='val_loss',\n",
        "                project_name = 'basic_model',\n",
        "                max_trials = 100)\n",
        "\n",
        "tuner.search(X_train_n, k.utils.to_categorical(y_train), epochs=30, validation_split=0.2)\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')}. The optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}. The optimal batch size is {best_hps.get('batch_size')}\n",
        "\"\"\")\n",
        "\n",
        "# Best hyperparameters for second model\n",
        "# 32 32 128\n",
        "# 5 3 5\n",
        "def second_model_builder(hp, width=79, height=95, depth=68):\n",
        "    inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "    hp_filter_size_1 = hp.Choice('ks_1', values=[3,5])\n",
        "    hp_filters_1 = hp.Choice('filters_1', values=[8, 16, 32])\n",
        "    x = Conv3D(filters=hp_filters_1, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    hp_filter_size_2 = hp.Choice('ks_2', values=[3,5])\n",
        "    hp_filters_2 = hp.Choice('filters_2', values=[16, 32, 64])\n",
        "    x = Conv3D(filters=hp_filters_2, kernel_size=hp_filter_size_2, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    hp_filter_size_3 = hp.Choice('ks_3', values=[3,5])\n",
        "    hp_filters_3 = hp.Choice('filters_3', values=[16, 64, 128])\n",
        "    x = Conv3D(filters=hp_filters_3, kernel_size=hp_filter_size_3, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = k.Model(inputs, outputs, name=\"second_model_builder\")\n",
        "\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                  loss = keras.losses.categorical_crossentropy,\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# print(best_hps.get('filters_1'), best_hps.get('filters_2'), best_hps.get('filters_3'))\n",
        "# print(best_hps.get('ks_1'), best_hps.get('ks_2'), best_hps.get('ks_3'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 1 Complete [00h 01m 26s]\n",
            "val_loss: 0.8973021507263184\n",
            "\n",
            "Best val_loss So Far: 0.8973021507263184\n",
            "Total elapsed time: 00h 01m 26s\n",
            "\n",
            "Search: Running Trial #2\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "units             |256               |256               \n",
            "learning_rate     |1e-05             |1e-05             \n",
            "batch_size        |16                |4                 \n",
            "\n",
            "Epoch 1/30\n",
            " 6/10 [=================>............] - ETA: 0s - loss: 1.1012 - accuracy: 0.3698WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0505s vs `on_train_batch_end` time: 0.0769s). Check your callbacks.\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1023 - accuracy: 0.3876 - val_loss: 1.0511 - val_accuracy: 0.5000\n",
            "Epoch 2/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.0922 - accuracy: 0.4349 - val_loss: 1.0450 - val_accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.0488 - accuracy: 0.4590 - val_loss: 1.0524 - val_accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.0644 - accuracy: 0.4631 - val_loss: 1.0352 - val_accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.0337 - accuracy: 0.4799 - val_loss: 1.0703 - val_accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 1.0588 - accuracy: 0.4676 - val_loss: 1.0338 - val_accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.0313 - accuracy: 0.4759 - val_loss: 1.0351 - val_accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.0699 - accuracy: 0.3878 - val_loss: 1.0460 - val_accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 1.0409 - accuracy: 0.4197 - val_loss: 1.0230 - val_accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.0654 - accuracy: 0.4075 - val_loss: 1.0330 - val_accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 1.0190 - accuracy: 0.4523 - val_loss: 1.0238 - val_accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.0268 - accuracy: 0.4157 - val_loss: 1.0207 - val_accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 0.9991 - accuracy: 0.4715 - val_loss: 1.0222 - val_accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 0.9832 - accuracy: 0.5225 - val_loss: 1.0342 - val_accuracy: 0.4750\n",
            "Epoch 15/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.0111 - accuracy: 0.5149 - val_loss: 1.0069 - val_accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 0.9963 - accuracy: 0.4331 - val_loss: 1.0068 - val_accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 1.0017 - accuracy: 0.4618 - val_loss: 1.0158 - val_accuracy: 0.5250\n",
            "Epoch 18/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 0.9714 - accuracy: 0.4829 - val_loss: 1.0009 - val_accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 0.9827 - accuracy: 0.5740 - val_loss: 1.0141 - val_accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 0.9473 - accuracy: 0.4939 - val_loss: 1.0155 - val_accuracy: 0.4000\n",
            "Epoch 21/30\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 0.9361 - accuracy: 0.6420 - val_loss: 0.9911 - val_accuracy: 0.5250\n",
            "Epoch 22/30\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 0.9132 - accuracy: 0.4992 - val_loss: 0.9990 - val_accuracy: 0.4750\n",
            "Epoch 23/30\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 0.9150 - accuracy: 0.7392 - val_loss: 0.9947 - val_accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 0.9208 - accuracy: 0.5212 - val_loss: 0.9857 - val_accuracy: 0.4750\n",
            "Epoch 25/30\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 0.8890 - accuracy: 0.6291 - val_loss: 0.9948 - val_accuracy: 0.4750\n",
            "Epoch 26/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 0.8869 - accuracy: 0.6052 - val_loss: 0.9894 - val_accuracy: 0.4000\n",
            "Epoch 27/30\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 0.9051 - accuracy: 0.5895 - val_loss: 0.9988 - val_accuracy: 0.4500\n",
            "Epoch 28/30\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 0.8738 - accuracy: 0.7301 - val_loss: 0.9708 - val_accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.8809 - accuracy: 0.5170"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8pheYdBfhuR"
      },
      "source": [
        "# Define models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "__EGiOvl5IjU",
        "outputId": "c6f11459-ca2a-4345-988a-79055eb6148f"
      },
      "source": [
        "# CNN MODELS\n",
        "\n",
        "# Model configuration\n",
        "sample_shape = (79,95,68,1)\n",
        "no_classes = 3\n",
        "\n",
        "def get_basic_model(width=79, height=95, depth=68):\n",
        "    # 10-fold cross validation results:\n",
        "    #                   Accuracy                    Loss\n",
        "    # With dropout      0.4728947371244431          1.5121068477630615\n",
        "    # No dropout        0.4921052664518356          1.3477824926376343\n",
        "    # No drop. 64u      0.45210526287555697         1.35818772315979 -->\n",
        "    # Train 0.88acc vs 1.0 in previous models\n",
        "\n",
        "    inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "            #    kernel_regularizer = keras.regularizers.l2(0.0005),\n",
        "            #    bias_regularizer = keras.regularizers.l2(0.0005))(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)  \n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "            #    kernel_regularizer = keras.regularizers.l2(0.0005),\n",
        "            #    bias_regularizer = keras.regularizers.l2(0.0005))(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x) \n",
        "            #   kernel_regularizer = keras.regularizers.l2(0.0005),\n",
        "            #   bias_regularizer = keras.regularizers.l2(0.0005))(x)\n",
        "    # x = Dropout(rate=0.5)(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = k.Model(inputs, outputs, name=\"basic_3Dcnn\")\n",
        "    return model\n",
        "\n",
        "def get_second_model(width=79, height=95, depth=68):\n",
        "    inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "    x = Conv3D(filters=8, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = k.Model(inputs, outputs, name=\"model_2\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# def get_first_model(width=79, height=95, depth=68):\n",
        "#     '''\n",
        "#     Deeper model, \n",
        "#     '''\n",
        "#     inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "#     x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = GlobalAveragePooling3D()(x)\n",
        "\n",
        "#     x = Dense(units=512, activation=\"relu\")(x)\n",
        "\n",
        "#     outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "#     model = k.Model(inputs, outputs, name=\"baseline_3Dcnn\")\n",
        "#     return model\n",
        "\n",
        "# def get_first_model(width=79, height=95, depth=68):\n",
        "#     \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "#     inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "#     x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.GlobalAveragePooling3D()(x)\n",
        "#     x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "#     x = layers.Dropout(0.3)(x)\n",
        "\n",
        "#     outputs = layers.Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "#     # Define the model.\n",
        "#     model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "#     return model\n",
        "\n",
        "\n",
        "# def get_fourth_model(width=79, height=95, depth=68):\n",
        "\n",
        "#     inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "#     x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     x = Flatten()(x)\n",
        "#     x = Dense(units=256, activation=\"relu\")(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "#     x = Dropout(0.5)(x)\n",
        "\n",
        "#     model = k.Model(inputs, outputs, name=\"baseline_3Dcnn\")\n",
        "#     return model\n",
        "\n",
        "\n",
        "# Fit model to data\n",
        "# Batch size of 16 as a standard starting point: https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/ ?\n",
        "\n",
        "print('\\n --------------- MODELO BÁSICO ------------------\\n')\n",
        "model = get_basic_model()\n",
        "model.summary()\n",
        "results = cross_validate(model, X_train_n, y_train, 10, k.optimizers.Adam(lr=0.0001), 4, 50, \n",
        "               verbose = 1, show_history = True)\n",
        "print(results)\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# print('\\n --------------- MODELO 2 ------------------\\n')\n",
        "# model = get_second_model()\n",
        "# model.summary()\n",
        "\n",
        "# results = cross_validate(model, X_train_n, y_train, 10, k.optimizers.Adam(lr=0.0001), 4, 50, \n",
        "#                verbose = 1, show_history = True)\n",
        "\n",
        "# keras.backend.clear_session()\n",
        "\n",
        "\n",
        "# print('\\n --------------- MODELO 3 ------------------\\n')\n",
        "# model = get_model(3)\n",
        "# model.summary()\n",
        "\n",
        "# # opt = SGD(lr = 0.001, momentum = 0.9, nesterov = True)\n",
        "# results = cross_validate(model, X_train_n, y_train, 5,  k.optimizers.Adam(), 16, 30, \n",
        "#                verbose = 1, show_history = False)\n",
        "# print(results)\n",
        "# keras.backend.clear_session()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --------------- MODELO BÁSICO ------------------\n",
            "\n",
            "Model: \"basic_3Dcnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 79, 95, 68, 1)]   0         \n",
            "_________________________________________________________________\n",
            "conv3d_6 (Conv3D)            (None, 77, 93, 66, 32)    896       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_6 (MaxPooling3 (None, 38, 46, 33, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_7 (Conv3D)            (None, 36, 44, 31, 32)    27680     \n",
            "_________________________________________________________________\n",
            "max_pooling3d_7 (MaxPooling3 (None, 18, 22, 15, 32)    0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 190080)            0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               48660736  \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 48,690,083\n",
            "Trainable params: 48,690,083\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Entrenando para el fold 1 ...\n",
            "Epoch 1/50\n",
            "45/45 [==============================] - 3s 54ms/step - loss: 1.2793 - accuracy: 0.3980 - val_loss: 1.0665 - val_accuracy: 0.4500\n",
            "Epoch 2/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 1.0582 - accuracy: 0.4820 - val_loss: 1.0526 - val_accuracy: 0.4500\n",
            "Epoch 3/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 1.0689 - accuracy: 0.4291 - val_loss: 1.0651 - val_accuracy: 0.5500\n",
            "Epoch 4/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 1.0631 - accuracy: 0.4348 - val_loss: 1.0050 - val_accuracy: 0.5000\n",
            "Epoch 5/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 1.0138 - accuracy: 0.4865 - val_loss: 0.9361 - val_accuracy: 0.5500\n",
            "Epoch 6/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 0.9379 - accuracy: 0.4733 - val_loss: 0.9371 - val_accuracy: 0.5500\n",
            "Epoch 7/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 0.9155 - accuracy: 0.5491 - val_loss: 0.8461 - val_accuracy: 0.6000\n",
            "Epoch 8/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 0.8062 - accuracy: 0.6533 - val_loss: 0.8415 - val_accuracy: 0.5000\n",
            "Epoch 9/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 0.7546 - accuracy: 0.6612 - val_loss: 0.7887 - val_accuracy: 0.7000\n",
            "Epoch 10/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 0.6303 - accuracy: 0.7794 - val_loss: 0.7923 - val_accuracy: 0.5000\n",
            "Epoch 11/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 0.6067 - accuracy: 0.7444 - val_loss: 1.1395 - val_accuracy: 0.5000\n",
            "Epoch 12/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 0.4922 - accuracy: 0.8528 - val_loss: 0.9049 - val_accuracy: 0.7000\n",
            "Epoch 13/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 0.4846 - accuracy: 0.8343 - val_loss: 1.1204 - val_accuracy: 0.6000\n",
            "Epoch 14/50\n",
            "45/45 [==============================] - 2s 51ms/step - loss: 0.3696 - accuracy: 0.8541 - val_loss: 0.9607 - val_accuracy: 0.4500\n",
            "Epoch 15/50\n",
            " 9/45 [=====>........................] - ETA: 1s - loss: 0.2815 - accuracy: 0.9297"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-98db34012e57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m results = cross_validate(model, X_train_n, y_train, 10, k.optimizers.Adam(lr=0.0001), 4, 50, \n\u001b[0;32m--> 158\u001b[0;31m                verbose = 1, show_history = True)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-cb9e7dc3413a>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(model, x_train, y_train, num_folds, opt, batch_size, epochs, verbose, show_history, return_results)\u001b[0m\n\u001b[1;32m     34\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                             \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkfold_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkfold_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                             )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}