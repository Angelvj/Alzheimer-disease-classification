{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/TFG/blob/main/code/jupyter/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA36tehHryhd"
      },
      "source": [
        "# Imports\n",
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras as k\n",
        "import sklearn\n",
        "import nibabel as nib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from keras import models\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization, MaxPool3D, GlobalAveragePooling3D, AveragePooling3D\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import time\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier \n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import decomposition\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCMEYyQ-UYaS"
      },
      "source": [
        "# Load data and impute NaN values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZf1JmliAMNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021436bc-15d3-4194-8062-2f77bee20579"
      },
      "source": [
        "# Load data\n",
        "COLAB = True\n",
        "preprocessed = True\n",
        "\n",
        "if COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  DATA_PATH = '/content/drive/My Drive/Machine learning/data'\n",
        "  if preprocessed:\n",
        "      DATA_PATH += '/preprocessed'\n",
        "\n",
        "else: \n",
        "  DATA_PATH = '../../data'\n",
        "\n",
        "def load_image(filename):    \n",
        "    \"\"\"\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    filename : str\n",
        "        relative path to de image\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img : numpy ndarray\n",
        "        array containing the image\n",
        "        \n",
        "    \"\"\"\n",
        "    img = nib.load(filename)\n",
        "    img = np.asarray(img.dataobj)\n",
        "    img = np.expand_dims(img, axis=3)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_images_from_dir(dirname):\n",
        "    \"\"\"\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dirname : str\n",
        "        name of the directory containing images.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    imgs : numpy ndarray\n",
        "        array containing all of the images in the folder.\n",
        "\n",
        "    \"\"\"\n",
        "    imgs = []\n",
        "\n",
        "    for filename in tqdm(glob.glob(dirname + '/*.nii')):\n",
        "        imgs.append(load_image(filename))\n",
        "        \n",
        "    imgs = np.stack(imgs) # All images over the new first dimension\n",
        "    return imgs\n",
        "\n",
        "def load_data(dirs_dict, categorical = False):\n",
        "    \"\"\"\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dirs_dict : dictionary\n",
        "        dictionary containing data folders name, and the label for the images\n",
        "        on each forlder.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x : numpy ndarray\n",
        "        array containing the images.\n",
        "    y : numpy ndarray\n",
        "\n",
        "        array containig the label of each image.\n",
        "\n",
        "    \"\"\"\n",
        "    first = True\n",
        "    for key, value in dirs_dict.items():\n",
        "        if first:\n",
        "            X = load_images_from_dir(value)\n",
        "            # ¿necesario float32 o puedo usar uint8?\n",
        "            y = np.full((X.shape[0]), key, dtype=np.float32)\n",
        "            first = False\n",
        "        else:\n",
        "            X_current = load_images_from_dir(value)\n",
        "            X = np.concatenate((X, X_current), axis=0)\n",
        "            y = np.concatenate((y, np.full((X_current.shape[0]), key, dtype=np.float32)), axis=0)\n",
        "            \n",
        "    if categorical:\n",
        "        y = k.utils.to_categorical(y)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "def impute_nan_values(imgs, inplace=True):\n",
        "    # Replace nan values with 0\n",
        "    return np.nan_to_num(imgs, copy = not inplace)\n",
        "\n",
        "# Load PET images with labels\n",
        "print('\\n --- Loading PET data --- \\n')\n",
        "time.sleep(0.5)\n",
        "X, y = load_data({0: DATA_PATH + \"/ppNOR/PET\", \n",
        "                  1: DATA_PATH + \"/ppAD/PET\",\n",
        "                  2: DATA_PATH + \"/ppMCI/PET\"})\n",
        "\n",
        "# Separate into training and test sets (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size = 0.2, stratify = y, random_state = 1)\n",
        "\n",
        "impute_nan_values(X_train)\n",
        "impute_nan_values(X_test)\n",
        "\n",
        "print('\\n --- PET data loaded --- \\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            " --- Loading PET data --- \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:00<00:00, 283.97it/s]\n",
            "100%|██████████| 70/70 [00:00<00:00, 302.38it/s]\n",
            "100%|██████████| 111/111 [00:00<00:00, 309.97it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --- PET data loaded --- \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf_tgVdodQ6E"
      },
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm7JaiUnr7ZI"
      },
      "source": [
        "def max_intensity_normalization(X, percentage, inplace=True):\n",
        "\n",
        "    if not inplace:\n",
        "        X = X.copy()\n",
        "\n",
        "    volume_shape = X[0].shape\n",
        "    n_max_values = int((volume_shape[0]*volume_shape[1]*volume_shape[2]*percentage)/100)\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        n_max_idx = np.unravel_index((X[i]).argsort(axis=None)[-n_max_values:], X[i].shape)\n",
        "        mean = np.mean(X[i][n_max_idx])\n",
        "        X[i] /= mean\n",
        "\n",
        "    if not inplace:\n",
        "        return X\n",
        "\n",
        "X_train_n = max_intensity_normalization(X_train, 1, inplace=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G7pRWJVGpmK"
      },
      "source": [
        "# Testing SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLuS8qfUkHAe"
      },
      "source": [
        "# # Test SVM classifier first\n",
        "# x_tr, x_val, y_tr, y_val = train_test_split(X_train_n, y_train, test_size = 0.2, stratify = y_train)\n",
        "\n",
        "# x_tr_1D = np.empty((x_tr.shape[0], x_tr.shape[1]*x_tr.shape[2]*x_tr.shape[3]))\n",
        "\n",
        "# # Convert each image to a single dimension ndarray\n",
        "# for i in range(len(x_tr)):\n",
        "#     x_tr_1D[i] = x_tr[i].flatten()\n",
        "\n",
        "# x_val_1D = np.empty((x_val.shape[0], x_val.shape[1]*x_val.shape[2]*x_val.shape[3]))\n",
        "# for i in range(len(y_val)):\n",
        "#     x_val_1D[i] = x_val[i].flatten()\n",
        "\n",
        "# # clf = svm.SVC(kernel='linear', C=0.8)\n",
        "# # clf = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5, C=0.05)) # 0.48 en 5 tiradas\n",
        "# # clf.fit(x_tr_1D, y_tr)\n",
        "\n",
        "# print(clf.score(x_tr_1D, y_tr))\n",
        "# print(clf.score(x_val_1D, y_val))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gMXCMkfGymI"
      },
      "source": [
        "# Functions for training models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQFcE3TcwW3m"
      },
      "source": [
        "def learning_curve(hist):\n",
        "    history_dict = hist.history\n",
        "    loss = history_dict['loss']\n",
        "    val_loss = history_dict['val_loss']\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.show()\n",
        "\n",
        "    acc = hist.history['accuracy']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training','Validation'])\n",
        "    plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yCcZQKccaYm"
      },
      "source": [
        "def cross_validate(model, x_train, y_train, num_folds, opt, batch_size, epochs, verbose=0, show_history=False, return_results=True):\n",
        "\n",
        "    # Creamos un objeto kfold, especificando el número de segmentos que queremos utilizar,\n",
        "    # además utilizamos shuffle true, para que los num_folds conjuntos disjuntos se seleccionen\n",
        "    # de forma aleatoria, evitando de esta forma problemas que podría haber si los datos\n",
        "    # estuvieran ordenados siguiendo una cierta distribución\n",
        "\n",
        "    skfold = StratifiedKFold(n_splits = num_folds, shuffle=True)\n",
        "\n",
        "    model.compile(loss = k.losses.categorical_crossentropy, optimizer=opt,\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "    initial_weights = model.get_weights()\n",
        "\n",
        "    acc_per_fold = []\n",
        "    loss_per_fold = []\n",
        "    acc_per_fold_tr = []\n",
        "    loss_per_fold_tr = []\n",
        "\n",
        "    fold_no = 1\n",
        "\n",
        "    for kfold_train, kfold_test in skfold.split(x_train, y_train):\n",
        "\n",
        "        # En cada fold, comenzamos con los pesos iniciales. Tratamos de que las 5\n",
        "        # folds sean lo más independientes posible.\n",
        "        model.set_weights(initial_weights)\n",
        "\n",
        "        if verbose:\n",
        "            print('------------------------------------------------------------------------')\n",
        "            print(f'Entrenando para el fold {fold_no} ...')\n",
        "\n",
        "        history = model.fit(x_train[kfold_train], \n",
        "                            k.utils.to_categorical(y_train[kfold_train]), \n",
        "                            batch_size= batch_size,\n",
        "                            epochs=epochs, verbose = verbose, \n",
        "                            validation_data = (x_train[kfold_test], k.utils.to_categorical(y_train[kfold_test]))\n",
        "                            )\n",
        "\n",
        "        if show_history:\n",
        "            # Mostramos la evolución en cada fold\n",
        "            learning_curve(history)\n",
        "\n",
        "        # Calculamos la bondad del modelo para el fold reservado para testing\n",
        "        scores = model.evaluate(x_train[kfold_test], k.utils.to_categorical(y_train[kfold_test]), verbose=0)\n",
        "        scores_train = model.evaluate(x_train[kfold_train], k.utils.to_categorical(y_train[kfold_train]), verbose=0)\n",
        "\n",
        "        # Vamos guardando el accuracy y pérdida para cada fold\n",
        "        acc_per_fold.append(scores[1])\n",
        "        loss_per_fold.append(scores[0])\n",
        "\n",
        "        acc_per_fold_tr.append(scores_train[1])\n",
        "        loss_per_fold_tr.append(scores_train[0])\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Resultado para el fold {fold_no}: {model.metrics_names[0]} de {scores[0]}; {model.metrics_names[1]} de {scores[1]*100}%')\n",
        "\n",
        "        fold_no += 1\n",
        "\n",
        "    # ==  Mostramos los valores medios == \n",
        "    if verbose:\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print('Resultados por cada fold')\n",
        "        for i in range(0, len(acc_per_fold)):\n",
        "            print('------------------------------------------------------------------------')\n",
        "            print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "\n",
        "    mean_acc_cv = np.mean(acc_per_fold)\n",
        "    mean_loss_cv = np.mean(loss_per_fold)\n",
        "    mean_acc_tr = np.mean(acc_per_fold_tr)\n",
        "    mean_loss_tr = np.mean(loss_per_fold_tr)\n",
        "\n",
        "    if not return_results:\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print('Media de los resultados para todos los folds:')\n",
        "        print(f'> Accuracy: {np.mean(acc_per_fold)}')\n",
        "        print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "        print(f'> Train accuracy: {np.mean(acc_per_fold_tr)}')\n",
        "        print(f'> Train accuracy: {np.mean(loss_per_fold_tr)}')\n",
        "        print('------------------------------------------------------------------------')\n",
        "\n",
        "    else:\n",
        "        return mean_acc_tr, mean_loss_tr, mean_acc_cv, mean_loss_cv"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOExIqfKG39b"
      },
      "source": [
        "global num_llamadas\n",
        "num_llamadas = 0\n",
        "\n",
        "def cross_validate_new(model, x_train, y_train, num_folds, batch_size, epochs, verbose=0, show_history=False, return_results=True):\n",
        "\n",
        "    global num_llamadas\n",
        "    num_llamadas += 1\n",
        "\n",
        "    print('He sido llamada', num_llamadas, ' veces')\n",
        "\n",
        "    # Creamos un objeto kfold, especificando el número de segmentos que queremos utilizar,\n",
        "    # además utilizamos shuffle true, para que los num_folds conjuntos disjuntos se seleccionen\n",
        "    # de forma aleatoria, evitando de esta forma problemas que podría haber si los datos\n",
        "    # estuvieran ordenados siguiendo una cierta distribución\n",
        "\n",
        "    skfold = StratifiedKFold(n_splits = num_folds, shuffle=True)\n",
        "\n",
        "    initial_weights = model.get_weights()\n",
        "\n",
        "    acc_per_fold = []\n",
        "    loss_per_fold = []\n",
        "    acc_per_fold_tr = []\n",
        "    loss_per_fold_tr = []\n",
        "\n",
        "    fold_no = 1\n",
        "\n",
        "    for kfold_train, kfold_test in skfold.split(x_train, y_train):\n",
        "\n",
        "        # En cada fold, comenzamos con los pesos iniciales. Tratamos de que las 5\n",
        "        # folds sean lo más independientes posible.\n",
        "        model.set_weights(initial_weights)\n",
        "\n",
        "        if verbose:\n",
        "            print('------------------------------------------------------------------------')\n",
        "            print(f'Entrenando para el fold {fold_no} ...')\n",
        "\n",
        "        history = model.fit(x_train[kfold_train], \n",
        "                            k.utils.to_categorical(y_train[kfold_train]), \n",
        "                            batch_size= batch_size,\n",
        "                            epochs=epochs, verbose = verbose, \n",
        "                            validation_data = (x_train[kfold_test], k.utils.to_categorical(y_train[kfold_test]))\n",
        "                            )\n",
        "\n",
        "        if show_history:\n",
        "            # Mostramos la evolución en cada fold\n",
        "            learning_curve(history)\n",
        "\n",
        "        # Calculamos la bondad del modelo para el fold reservado para testing\n",
        "        scores = model.evaluate(x_train[kfold_test], k.utils.to_categorical(y_train[kfold_test]), verbose=0)\n",
        "        scores_train = model.evaluate(x_train[kfold_train], k.utils.to_categorical(y_train[kfold_train]), verbose=0)\n",
        "\n",
        "        # Vamos guardando el accuracy y pérdida para cada fold\n",
        "        acc_per_fold.append(scores[1])\n",
        "        loss_per_fold.append(scores[0])\n",
        "\n",
        "        acc_per_fold_tr.append(scores_train[1])\n",
        "        loss_per_fold_tr.append(scores_train[0])\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Resultado para el fold {fold_no}: {model.metrics_names[0]} de {scores[0]}; {model.metrics_names[1]} de {scores[1]*100}%')\n",
        "\n",
        "        fold_no += 1\n",
        "\n",
        "    # ==  Mostramos los valores medios == \n",
        "    if verbose:\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print('Resultados por cada fold')\n",
        "        for i in range(0, len(acc_per_fold)):\n",
        "            print('------------------------------------------------------------------------')\n",
        "            print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "\n",
        "    mean_acc_cv = np.mean(acc_per_fold)\n",
        "    mean_loss_cv = np.mean(loss_per_fold)\n",
        "    mean_acc_tr = np.mean(acc_per_fold_tr)\n",
        "    mean_loss_tr = np.mean(loss_per_fold_tr)\n",
        "\n",
        "    if not return_results:\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print('Media de los resultados para todos los folds:')\n",
        "        print(f'> Accuracy: {np.mean(acc_per_fold)}')\n",
        "        print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "        print(f'> Train accuracy: {np.mean(acc_per_fold_tr)}')\n",
        "        print(f'> Train accuracy: {np.mean(loss_per_fold_tr)}')\n",
        "        print('------------------------------------------------------------------------')\n",
        "\n",
        "    else:\n",
        "        return mean_acc_tr, mean_loss_tr, mean_acc_cv, mean_loss_cv"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFxaFXjCsE9n",
        "outputId": "d46111b5-1051-4934-fd06-3600045772df"
      },
      "source": [
        "!rm -r ./basic\\_model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove './basic_model': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlk5X7zs06eG"
      },
      "source": [
        "# Hypertuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9lmY8gvDklb"
      },
      "source": [
        "class CVTuner(kt.engine.tuner.Tuner):\n",
        "    def run_trial(self, trial, x, y, *args, **kwargs):\n",
        "        kwargs['batch_size'] = trial.hyperparameters.Choice('batch_size', values=[4, 8, 16])\n",
        "        model = self.hypermodel.build(trial.hyperparameters)\n",
        "        results = cross_validate_new(model, x, y, 5, batch_size=kwargs['batch_size'], epochs=1)\n",
        "        cv_loss = results[3]\n",
        "        self.oracle.update_trial(trial.trial_id, {'val_loss': cv_loss})\n",
        "        self.save_model(trial.trial_id, model)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGPY9CN30_Mp"
      },
      "source": [
        "# Best hyperparams for basic model: \n",
        "\n",
        "'''\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is 256, the optimal learning rate for the optimizer is 0.0001. The optimal batch size is \n",
        "4\n",
        "'''\n",
        "def basic_model_builder(hp, width=79, height=95, depth=68):\n",
        "     \n",
        "    model = keras.Sequential()\n",
        "\n",
        "    model.add(Conv3D(filters=32, kernel_size=3, activation=\"relu\", input_shape = (width, height, depth, 1)))\n",
        "    model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "    model.add(Conv3D(filters=32, kernel_size=3, activation=\"relu\"))\n",
        "    model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    hp_units = hp.Choice('units', values=[64, 128, 256])\n",
        "    model.add(Dense(units=hp_units, activation=\"relu\"))\n",
        "\n",
        "    # hp_dropout_rate = hp.Choice('drop_rate', values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "    # model.add(Dropout(rate=hp_dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=3, activation=\"softmax\"))\n",
        "\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5, 1e-6])\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                  loss = keras.losses.categorical_crossentropy,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# tuner = CVTuner(\n",
        "#     hypermodel=basic_model_builder,\n",
        "#     oracle=kt.oracles.BayesianOptimization(\n",
        "#     objective='val_loss'),\n",
        "#     project_name='basic_model')\n",
        "\n",
        "# tuner.search(X_train_n, y_train, epochs=30)\n",
        "\n",
        "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "\n",
        "# print(f\"\"\"\n",
        "# The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "# layer is {best_hps.get('units')}. The optimal learning rate for the optimizer\n",
        "# is {best_hps.get('learning_rate')}. The optimal batch size is {best_hps.get('batch_size')}\n",
        "# \"\"\")\n",
        "\n",
        "\n",
        "# tuner = kt.Hyperband(basic_model_builder, \n",
        "#                      objective='val_loss',\n",
        "#                      max_epochs=50,\n",
        "#                      project_name='basic_model'\n",
        "#                     )\n",
        "\n",
        "\n",
        "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 5)\n",
        "# tuner.search(X_train_n, k.utils.to_categorical(y_train), epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "# best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# print(f\"\"\"\n",
        "# The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "# layer is {best_hps.get('units')}. The optimal learning rate for the optimizer\n",
        "# is {best_hps.get('learning_rate')}. The optimal batch size is {best_hps.get('batch_size')}\n",
        "# \"\"\")\n",
        "\n",
        "# Best hyperparameters for second model\n",
        "# 32 32 128\n",
        "# 5 3 5\n",
        "def second_model_builder(hp, width=79, height=95, depth=68):\n",
        "    inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "    hp_filter_size_1 = hp.Choice('ks_1', values=[3,5])\n",
        "    hp_filters_1 = hp.Choice('filters_1', values=[8, 16, 32])\n",
        "    x = Conv3D(filters=hp_filters_1, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    hp_filter_size_2 = hp.Choice('ks_2', values=[3,5])\n",
        "    hp_filters_2 = hp.Choice('filters_2', values=[16, 32, 64])\n",
        "    x = Conv3D(filters=hp_filters_2, kernel_size=hp_filter_size_2, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    hp_filter_size_3 = hp.Choice('ks_3', values=[3,5])\n",
        "    hp_filters_3 = hp.Choice('filters_3', values=[16, 64, 128])\n",
        "    x = Conv3D(filters=hp_filters_3, kernel_size=hp_filter_size_3, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = k.Model(inputs, outputs, name=\"second_model_builder\")\n",
        "\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                  loss = keras.losses.categorical_crossentropy,\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# print(best_hps.get('filters_1'), best_hps.get('filters_2'), best_hps.get('filters_3'))\n",
        "# print(best_hps.get('ks_1'), best_hps.get('ks_2'), best_hps.get('ks_3'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8pheYdBfhuR"
      },
      "source": [
        "# Define models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__EGiOvl5IjU",
        "outputId": "24958b64-5b4f-48ea-f4af-585e3f5554b9"
      },
      "source": [
        "# CNN MODELS\n",
        "\n",
        "# Model configuration\n",
        "sample_shape = (79,95,68,1)\n",
        "no_classes = 3\n",
        "\n",
        "def get_basic_model(width=79, height=95, depth=68):\n",
        "    # 10-fold cross validation results:\n",
        "    #                   Accuracy                    Loss\n",
        "    # With dropout      0.4681578904390335          1.0665521562099456\n",
        "    # No dropout        0.4921052664518356          1.3477824926376343\n",
        "\n",
        "    inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)  \n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "    x = Dropout(rate=0.4)(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = k.Model(inputs, outputs, name=\"basic_3Dcnn\")\n",
        "    return model\n",
        "\n",
        "def get_second_model(width=79, height=95, depth=68):\n",
        "    inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "    x = Conv3D(filters=8, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = k.Model(inputs, outputs, name=\"model_2\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# def get_first_model(width=79, height=95, depth=68):\n",
        "#     '''\n",
        "#     Deeper model, \n",
        "#     '''\n",
        "#     inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "#     x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = GlobalAveragePooling3D()(x)\n",
        "\n",
        "#     x = Dense(units=512, activation=\"relu\")(x)\n",
        "\n",
        "#     outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "#     model = k.Model(inputs, outputs, name=\"baseline_3Dcnn\")\n",
        "#     return model\n",
        "\n",
        "# def get_first_model(width=79, height=95, depth=68):\n",
        "#     \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "#     inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "#     x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.GlobalAveragePooling3D()(x)\n",
        "#     x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "#     x = layers.Dropout(0.3)(x)\n",
        "\n",
        "#     outputs = layers.Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "#     # Define the model.\n",
        "#     model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "#     return model\n",
        "\n",
        "\n",
        "# def get_fourth_model(width=79, height=95, depth=68):\n",
        "\n",
        "#     inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "#     x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     x = Flatten()(x)\n",
        "#     x = Dense(units=256, activation=\"relu\")(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "#     x = Dropout(0.5)(x)\n",
        "\n",
        "#     model = k.Model(inputs, outputs, name=\"baseline_3Dcnn\")\n",
        "#     return model\n",
        "\n",
        "\n",
        "# Fit model to data\n",
        "# Batch size of 16 as a standard starting point: https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/ ?\n",
        "\n",
        "print('\\n --------------- MODELO BÁSICO ------------------\\n')\n",
        "model = get_basic_model()\n",
        "model.summary()\n",
        "results = cross_validate(model, X_train_n, y_train, 5, k.optimizers.Adam(lr=0.0001), 4, 50, \n",
        "               verbose = 1, show_history = True)\n",
        "print(results)\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# print('\\n --------------- MODELO 2 ------------------\\n')\n",
        "# model = get_second_model()\n",
        "# model.summary()\n",
        "\n",
        "# results = cross_validate(model, X_train_n, y_train, 10, k.optimizers.Adam(lr=0.0001), 4, 50, \n",
        "#                verbose = 1, show_history = True)\n",
        "\n",
        "# keras.backend.clear_session()\n",
        "\n",
        "\n",
        "# print('\\n --------------- MODELO 3 ------------------\\n')\n",
        "# model = get_model(3)\n",
        "# model.summary()\n",
        "\n",
        "# # opt = SGD(lr = 0.001, momentum = 0.9, nesterov = True)\n",
        "# results = cross_validate(model, X_train_n, y_train, 5,  k.optimizers.Adam(), 16, 30, \n",
        "#                verbose = 1, show_history = False)\n",
        "# print(results)\n",
        "# keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --------------- MODELO BÁSICO ------------------\n",
            "\n",
            "Model: \"basic_3Dcnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 79, 95, 68, 1)]   0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 77, 93, 66, 32)    896       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 38, 46, 33, 32)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1845888)           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               472547584 \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 472,549,251\n",
            "Trainable params: 472,549,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Entrenando para el fold 1 ...\n",
            "Epoch 1/50\n",
            "40/40 [==============================] - 9s 132ms/step - loss: 15.1215 - accuracy: 0.3844 - val_loss: 1.2894 - val_accuracy: 0.2750\n",
            "Epoch 2/50\n",
            "40/40 [==============================] - 5s 122ms/step - loss: 1.5623 - accuracy: 0.2583 - val_loss: 1.0985 - val_accuracy: 0.4500\n",
            "Epoch 3/50\n",
            "40/40 [==============================] - 5s 120ms/step - loss: 1.0984 - accuracy: 0.4357 - val_loss: 1.0983 - val_accuracy: 0.4500\n",
            "Epoch 4/50\n",
            "40/40 [==============================] - 5s 118ms/step - loss: 1.0983 - accuracy: 0.4266 - val_loss: 1.0980 - val_accuracy: 0.4500\n",
            "Epoch 5/50\n",
            "40/40 [==============================] - 5s 121ms/step - loss: 1.0979 - accuracy: 0.4686 - val_loss: 1.0978 - val_accuracy: 0.4500\n",
            "Epoch 6/50\n",
            "40/40 [==============================] - 5s 118ms/step - loss: 1.0979 - accuracy: 0.4292 - val_loss: 1.0976 - val_accuracy: 0.4500\n",
            "Epoch 7/50\n",
            "40/40 [==============================] - 5s 118ms/step - loss: 1.0974 - accuracy: 0.4629 - val_loss: 1.0973 - val_accuracy: 0.4500\n",
            "Epoch 8/50\n",
            "40/40 [==============================] - 5s 121ms/step - loss: 1.0966 - accuracy: 0.5060 - val_loss: 1.0970 - val_accuracy: 0.4500\n",
            "Epoch 9/50\n",
            "40/40 [==============================] - 5s 120ms/step - loss: 1.0967 - accuracy: 0.4700 - val_loss: 1.0968 - val_accuracy: 0.4500\n",
            "Epoch 10/50\n",
            "40/40 [==============================] - 5s 121ms/step - loss: 1.0961 - accuracy: 0.4880 - val_loss: 1.0965 - val_accuracy: 0.4500\n",
            "Epoch 11/50\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0960 - accuracy: 0.4739"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}