{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/TFG/blob/main/code/jupyter/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA36tehHryhd"
      },
      "source": [
        "# Imports\n",
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras as k\n",
        "import sklearn\n",
        "import nibabel as nib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from keras import models\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization, MaxPool3D, GlobalAveragePooling3D, AveragePooling3D\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import time\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier \n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import decomposition\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCMEYyQ-UYaS"
      },
      "source": [
        "# Load data and impute NaN values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZf1JmliAMNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c75f2f-b629-40f0-889f-da847549205c"
      },
      "source": [
        "# Load data\n",
        "COLAB = True\n",
        "\n",
        "if COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  DATA_PATH = '/content/drive/My Drive/Machine learning/data'\n",
        "\n",
        "else: \n",
        "  DATA_PATH = '../../data'\n",
        "\n",
        "def load_image(filename):    \n",
        "    \"\"\"\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    filename : str\n",
        "        relative path to de image\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img : numpy ndarray\n",
        "        array containing the image\n",
        "        \n",
        "    \"\"\"\n",
        "    img = nib.load(filename)\n",
        "    img = np.asarray(img.dataobj)\n",
        "    img = np.expand_dims(img, axis=3)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_images_from_dir(dirname):\n",
        "    \"\"\"\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dirname : str\n",
        "        name of the directory containing images.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    imgs : numpy ndarray\n",
        "        array containing all of the images in the folder.\n",
        "\n",
        "    \"\"\"\n",
        "    imgs = []\n",
        "\n",
        "    for filename in tqdm(glob.glob(dirname + '/*.nii')):\n",
        "        imgs.append(load_image(filename))\n",
        "        \n",
        "    imgs = np.stack(imgs) # All images over the new first dimension\n",
        "    return imgs\n",
        "\n",
        "def load_data(dirs_dict, categorical = False):\n",
        "    \"\"\"\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dirs_dict : dictionary\n",
        "        dictionary containing data folders name, and the label for the images\n",
        "        on each forlder.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x : numpy ndarray\n",
        "        array containing the images.\n",
        "    y : numpy ndarray\n",
        "\n",
        "        array containig the label of each image.\n",
        "\n",
        "    \"\"\"\n",
        "    first = True\n",
        "    for key, value in dirs_dict.items():\n",
        "        if first:\n",
        "            X = load_images_from_dir(value)\n",
        "            # ¿necesario float32 o puedo usar uint8?\n",
        "            y = np.full((X.shape[0]), key, dtype=np.float32)\n",
        "            first = False\n",
        "        else:\n",
        "            X_current = load_images_from_dir(value)\n",
        "            X = np.concatenate((X, X_current), axis=0)\n",
        "            y = np.concatenate((y, np.full((X_current.shape[0]), key, dtype=np.float32)), axis=0)\n",
        "            \n",
        "    if categorical:\n",
        "        y = k.utils.to_categorical(y)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "def impute_nan_values(imgs, inplace=True):\n",
        "    # Replace nan values with 0\n",
        "    return np.nan_to_num(imgs, copy = not inplace)\n",
        "\n",
        "# Load PET images with labels\n",
        "print('\\n --- Loading PET data --- \\n')\n",
        "time.sleep(0.5)\n",
        "X, y = load_data({0: DATA_PATH + \"/ppNOR/PET\", \n",
        "                  1: DATA_PATH + \"/ppAD/PET\",\n",
        "                  2: DATA_PATH + \"/ppMCI/PET\"})\n",
        "\n",
        "# Separate into training and test sets (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size = 0.2, stratify = y, random_state = 1)\n",
        "\n",
        "impute_nan_values(X_train)\n",
        "impute_nan_values(X_test)\n",
        "\n",
        "print('\\n --- PET data loaded --- \\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            " --- Loading PET data --- \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:00<00:00, 280.69it/s]\n",
            "100%|██████████| 70/70 [00:00<00:00, 318.48it/s]\n",
            "100%|██████████| 111/111 [00:00<00:00, 311.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --- PET data loaded --- \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf_tgVdodQ6E"
      },
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm7JaiUnr7ZI"
      },
      "source": [
        "def max_intensity_normalization(X, percentage, inplace=True):\n",
        "\n",
        "    if not inplace:\n",
        "        X = X.copy()\n",
        "\n",
        "    volume_shape = X[0].shape\n",
        "    n_max_values = int((volume_shape[0]*volume_shape[1]*volume_shape[2]*percentage)/100)\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        n_max_idx = np.unravel_index((X[i]).argsort(axis=None)[-n_max_values:], X[i].shape)\n",
        "        mean = np.mean(X[i][n_max_idx])\n",
        "        X[i] /= mean\n",
        "\n",
        "    if not inplace:\n",
        "        return X\n",
        "\n",
        "X_train_n = max_intensity_normalization(X_train, 1, inplace=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLuS8qfUkHAe"
      },
      "source": [
        "# # Test SVM classifier first\n",
        "# x_tr, x_val, y_tr, y_val = train_test_split(X_train_n, y_train, test_size = 0.2, stratify = y_train)\n",
        "\n",
        "# x_tr_1D = np.empty((x_tr.shape[0], x_tr.shape[1]*x_tr.shape[2]*x_tr.shape[3]))\n",
        "\n",
        "# # Convert each image to a single dimension ndarray\n",
        "# for i in range(len(x_tr)):\n",
        "#     x_tr_1D[i] = x_tr[i].flatten()\n",
        "\n",
        "# x_val_1D = np.empty((x_val.shape[0], x_val.shape[1]*x_val.shape[2]*x_val.shape[3]))\n",
        "# for i in range(len(y_val)):\n",
        "#     x_val_1D[i] = x_val[i].flatten()\n",
        "\n",
        "# # clf = svm.SVC(kernel='linear', C=0.8)\n",
        "# # clf = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5, C=0.05)) # 0.48 en 5 tiradas\n",
        "# # clf.fit(x_tr_1D, y_tr)\n",
        "\n",
        "# print(clf.score(x_tr_1D, y_tr))\n",
        "# print(clf.score(x_val_1D, y_val))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo5R0R8Q385b"
      },
      "source": [
        "# Loading MRI images, its a bit different process because they are separated into \n",
        "# gray and white matter\n",
        "\n",
        "# def load_mri_data(dict_dirs, categorical = False):\n",
        "#     pass\n",
        "\n",
        "# # Pruebo a cargar una imagen cualquiera\n",
        "# image = load_image(DATA_PATH + \"/ppNOR/MRI/greyMatter/m0wrp1ADNI_005_S_0223_MR_MPR-R__GradWarp__B1_Correction__N3__Scaled_Br_20070809183352346_S19963_I66520.nii\")\n",
        "\n",
        "# print(type(image))\n",
        "# print(image.shape)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQFcE3TcwW3m"
      },
      "source": [
        "def learning_curve(hist):\n",
        "    history_dict = hist.history\n",
        "    loss = history_dict['loss']\n",
        "    val_loss = history_dict['val_loss']\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.show()\n",
        "\n",
        "    acc = hist.history['accuracy']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training','Validation'])\n",
        "    plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yCcZQKccaYm"
      },
      "source": [
        "def cross_validate(model, x_train, y_train, num_folds, opt, batch_size, epochs, verbose=0, show_history=False, return_results=True):\n",
        "\n",
        "    # Creamos un objeto kfold, especificando el número de segmentos que queremos utilizar,\n",
        "    # además utilizamos shuffle true, para que los num_folds conjuntos disjuntos se seleccionen\n",
        "    # de forma aleatoria, evitando de esta forma problemas que podría haber si los datos\n",
        "    # estuvieran ordenados siguiendo una cierta distribución\n",
        "\n",
        "    skfold = StratifiedKFold(n_splits = num_folds, shuffle=True)\n",
        "\n",
        "    model.compile(loss = k.losses.categorical_crossentropy, optimizer=opt,\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "    initial_weights = model.get_weights()\n",
        "\n",
        "    acc_per_fold = []\n",
        "    loss_per_fold = []\n",
        "    acc_per_fold_tr = []\n",
        "    loss_per_fold_tr = []\n",
        "\n",
        "    fold_no = 1\n",
        "\n",
        "    for kfold_train, kfold_test in skfold.split(x_train, y_train):\n",
        "\n",
        "        # En cada fold, comenzamos con los pesos iniciales. Tratamos de que las 5\n",
        "        # folds sean lo más independientes posible.\n",
        "        model.set_weights(initial_weights)\n",
        "\n",
        "        if verbose:\n",
        "            print('------------------------------------------------------------------------')\n",
        "            print(f'Entrenando para el fold {fold_no} ...')\n",
        "\n",
        "        history = model.fit(x_train[kfold_train], \n",
        "                            k.utils.to_categorical(y_train[kfold_train]), \n",
        "                            batch_size= batch_size,\n",
        "                            epochs=epochs, verbose = verbose, \n",
        "                            validation_data = (x_train[kfold_test], k.utils.to_categorical(y_train[kfold_test]))\n",
        "                            )\n",
        "\n",
        "        if show_history:\n",
        "            # Mostramos la evolución en cada fold\n",
        "            learning_curve(history)\n",
        "\n",
        "        # Calculamos la bondad del modelo para el fold reservado para testing\n",
        "        scores = model.evaluate(x_train[kfold_test], k.utils.to_categorical(y_train[kfold_test]), verbose=0)\n",
        "        scores_train = model.evaluate(x_train[kfold_train], k.utils.to_categorical(y_train[kfold_train]), verbose=0)\n",
        "\n",
        "        # Vamos guardando el accuracy y pérdida para cada fold\n",
        "        acc_per_fold.append(scores[1])\n",
        "        loss_per_fold.append(scores[0])\n",
        "\n",
        "        acc_per_fold_tr.append(scores_train[1])\n",
        "        loss_per_fold_tr.append(scores_train[0])\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Resultado para el fold {fold_no}: {model.metrics_names[0]} de {scores[0]}; {model.metrics_names[1]} de {scores[1]*100}%')\n",
        "\n",
        "        fold_no += 1\n",
        "\n",
        "    # ==  Mostramos los valores medios == \n",
        "    if verbose:\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print('Resultados por cada fold')\n",
        "        for i in range(0, len(acc_per_fold)):\n",
        "            print('------------------------------------------------------------------------')\n",
        "            print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "\n",
        "    mean_acc_cv = np.mean(acc_per_fold)\n",
        "    mean_loss_cv = np.mean(loss_per_fold)\n",
        "    mean_acc_tr = np.mean(acc_per_fold_tr)\n",
        "    mean_loss_tr = np.mean(loss_per_fold_tr)\n",
        "\n",
        "    if not return_results:\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print('Media de los resultados para todos los folds:')\n",
        "        print(f'> Accuracy: {np.mean(acc_per_fold)}')\n",
        "        print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "        print(f'> Train accuracy: {np.mean(acc_per_fold_tr)}')\n",
        "        print(f'> Train accuracy: {np.mean(loss_per_fold_tr)}')\n",
        "        print('------------------------------------------------------------------------')\n",
        "\n",
        "    else:\n",
        "        return mean_acc_tr, mean_loss_tr, mean_acc_cv, mean_acc_cv"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8pheYdBfhuR"
      },
      "source": [
        "# Define models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFxaFXjCsE9n",
        "outputId": "e81397c3-5b2d-437d-df3e-eb6d1d4c9fe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 25.4 GB  |     Proc size: 2.2 GB\n",
            "GPU RAM Free: 16160MB | Used: 0MB | Util   0% | Total     16160MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "__EGiOvl5IjU",
        "outputId": "12601805-28fc-4407-b489-7a328f1d32dd"
      },
      "source": [
        "def model_builder(hp):\n",
        "     \n",
        "    model = keras.Sequential()\n",
        "\n",
        "    hp_filters = hp.Int('filters', min_value=8, max_value=64, step=)\n",
        "\n",
        "\n",
        "    model.add(Conv3D())\n",
        "\n",
        "     inputs = k.Input((width, height, depth))\n",
        "\n",
        "\n",
        "# CNN MODELS\n",
        "\n",
        "# Model configuration\n",
        "sample_shape = (79,95,68,1)\n",
        "no_classes = 3\n",
        "\n",
        "def get_basic_model(width=79, height=95, depth=68):\n",
        "    inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)  \n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = k.Model(inputs, outputs, name=\"basic_3Dcnn\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_first_model_r(width=79, height=95, depth=68):\n",
        "    inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)  \n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    model = k.Model(inputs, outputs, name=\"basic_3Dcnn_drop03\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_first_model_rr(width=79, height=95, depth=68):\n",
        "    inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)  \n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    model = k.Model(inputs, outputs, name=\"basic_3Dcnn_drop05\")\n",
        "    return model\n",
        "\n",
        "def get_second_model(width=79, height=95, depth=68):\n",
        "    ''' \n",
        "    Shallow basic model, has too many parameters, leads to enormous overfitting.\n",
        "    It also has some optimization problems (does not fully explain train dataset)\n",
        "    '''\n",
        "    inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "    x = Conv3D(filters=8, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = k.Model(inputs, outputs, name=\"model-1\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model(model_no):\n",
        "    \"\"\"\n",
        "    Return the selected model\n",
        "\n",
        "    \"\"\"\n",
        "    if model_no == 0:\n",
        "        return get_basic_model()\n",
        "    elif model_no == 1:\n",
        "        return get_first_model_r()\n",
        "    elif model_no == 2:\n",
        "        return get_first_model_rr()\n",
        "    elif model_no == 3:\n",
        "        return get_second_model()\n",
        "        \n",
        "\n",
        "\n",
        "# def get_first_model(width=79, height=95, depth=68):\n",
        "#     '''\n",
        "#     Deeper model, \n",
        "#     '''\n",
        "#     inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "#     x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     # x = BatchNormalization()(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = GlobalAveragePooling3D()(x)\n",
        "\n",
        "#     x = Dense(units=512, activation=\"relu\")(x)\n",
        "\n",
        "#     outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "#     model = k.Model(inputs, outputs, name=\"baseline_3Dcnn\")\n",
        "#     return model\n",
        "\n",
        "# def get_first_model(width=79, height=95, depth=68):\n",
        "#     \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "#     inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "#     x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = layers.MaxPool3D(pool_size=2)(x)\n",
        "#     x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "\n",
        "#     x = layers.GlobalAveragePooling3D()(x)\n",
        "#     x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "#     x = layers.Dropout(0.3)(x)\n",
        "\n",
        "#     outputs = layers.Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "#     # Define the model.\n",
        "#     model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "#     return model\n",
        "\n",
        "\n",
        "# def get_fourth_model(width=79, height=95, depth=68):\n",
        "\n",
        "#     inputs = k.Input((width, height, depth, 1))\n",
        "\n",
        "#     x = Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#     x = MaxPooling3D(pool_size=2)(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     x = Flatten()(x)\n",
        "#     x = Dense(units=256, activation=\"relu\")(x)\n",
        "#     x = Dropout(0.1)(x)\n",
        "\n",
        "#     outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "#     x = Dropout(0.5)(x)\n",
        "\n",
        "#     model = k.Model(inputs, outputs, name=\"baseline_3Dcnn\")\n",
        "#     return model\n",
        "\n",
        "\n",
        "# Fit model to data\n",
        "# Batch size of 16 as a standard starting point: https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/ ?\n",
        "\n",
        "# print('\\n --------------- MODELO BÁSICO ------------------\\n')\n",
        "# model = get_model(0)\n",
        "# model.summary()\n",
        "# results = cross_validate(model, X_train_n, y_train, 5, k.optimizers.Adam(), 16, 30, \n",
        "#                verbose = 1, show_history = False)\n",
        "# print(results)\n",
        "# keras.backend.clear_session()\n",
        "\n",
        "# Pruebas de distintos modelos\n",
        "# print('\\n --------------- MODELO 1 ------------------\\n')\n",
        "# model = get_model(1)\n",
        "# model.summary()\n",
        "# cross_validate(model, X_train_n, y_train, 5, k.optimizers.Adam(), 1, 30, \n",
        "#                verbose = 1, show_history = True)\n",
        "\n",
        "# print('\\n --------------- MODELO 2 ------------------\\n')\n",
        "# model = get_model(2)\n",
        "# cross_validate(model, X_train_n, y_train, 10, k.optimizers.Adam(lr=0.00001), 4, 40, \n",
        "#                verbose = 0, show_history = True)\n",
        "\n",
        "# print('\\n --------------- MODELO 3 ------------------\\n')\n",
        "# model = get_model(3)\n",
        "# model.summary()\n",
        "\n",
        "# # opt = SGD(lr = 0.001, momentum = 0.9, nesterov = True)\n",
        "# results = cross_validate(model, X_train_n, y_train, 5,  k.optimizers.Adam(), 16, 30, \n",
        "#                verbose = 1, show_history = False)\n",
        "# print(results)\n",
        "# keras.backend.clear_session()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --------------- MODELO 3 ------------------\n",
            "\n",
            "Model: \"model-1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 79, 95, 68, 1)]   0         \n",
            "_________________________________________________________________\n",
            "conv3d_26 (Conv3D)           (None, 77, 93, 66, 8)     224       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_26 (MaxPooling (None, 38, 46, 33, 8)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_27 (Conv3D)           (None, 36, 44, 31, 16)    3472      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_27 (MaxPooling (None, 18, 22, 15, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_28 (Conv3D)           (None, 16, 20, 13, 32)    13856     \n",
            "_________________________________________________________________\n",
            "max_pooling3d_28 (MaxPooling (None, 8, 10, 6, 32)      0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 15360)             0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 256)               3932416   \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 3,950,739\n",
            "Trainable params: 3,950,739\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Entrenando para el fold 1 ...\n",
            "Epoch 1/30\n",
            "10/10 [==============================] - 1s 85ms/step - loss: 1.3721 - accuracy: 0.3383 - val_loss: 1.0925 - val_accuracy: 0.4750\n",
            "Epoch 2/30\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 1.0854 - accuracy: 0.4594 - val_loss: 1.0721 - val_accuracy: 0.4500\n",
            "Epoch 3/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 1.0639 - accuracy: 0.4786 - val_loss: 1.0835 - val_accuracy: 0.4500\n",
            "Epoch 4/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 1.0937 - accuracy: 0.3990 - val_loss: 1.0894 - val_accuracy: 0.4500\n",
            "Epoch 5/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 1.0883 - accuracy: 0.4136 - val_loss: 1.0672 - val_accuracy: 0.4500\n",
            "Epoch 6/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 1.0542 - accuracy: 0.4941 - val_loss: 1.0617 - val_accuracy: 0.4500\n",
            "Epoch 7/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 1.0762 - accuracy: 0.4333 - val_loss: 1.0807 - val_accuracy: 0.4500\n",
            "Epoch 8/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 1.0796 - accuracy: 0.4444 - val_loss: 1.0690 - val_accuracy: 0.4500\n",
            "Epoch 9/30\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 1.0537 - accuracy: 0.5149 - val_loss: 1.0649 - val_accuracy: 0.4500\n",
            "Epoch 10/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 1.0616 - accuracy: 0.4601 - val_loss: 1.0663 - val_accuracy: 0.4500\n",
            "Epoch 11/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 1.0726 - accuracy: 0.4188 - val_loss: 1.0532 - val_accuracy: 0.4500\n",
            "Epoch 12/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 1.0537 - accuracy: 0.4365 - val_loss: 1.0364 - val_accuracy: 0.4500\n",
            "Epoch 13/30\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 1.0238 - accuracy: 0.4739 - val_loss: 1.0517 - val_accuracy: 0.3500\n",
            "Epoch 14/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 1.0168 - accuracy: 0.4822 - val_loss: 1.0285 - val_accuracy: 0.4500\n",
            "Epoch 15/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 1.0427 - accuracy: 0.4596 - val_loss: 1.0357 - val_accuracy: 0.4750\n",
            "Epoch 16/30\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 1.0156 - accuracy: 0.4923 - val_loss: 0.9884 - val_accuracy: 0.4250\n",
            "Epoch 17/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.9036 - accuracy: 0.5614 - val_loss: 0.9861 - val_accuracy: 0.4000\n",
            "Epoch 18/30\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.9423 - accuracy: 0.5057 - val_loss: 0.9640 - val_accuracy: 0.4750\n",
            "Epoch 19/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.9226 - accuracy: 0.5376 - val_loss: 0.8718 - val_accuracy: 0.5500\n",
            "Epoch 20/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.8436 - accuracy: 0.5790 - val_loss: 0.8598 - val_accuracy: 0.5250\n",
            "Epoch 21/30\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.7475 - accuracy: 0.6622 - val_loss: 1.0533 - val_accuracy: 0.5750\n",
            "Epoch 22/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.7181 - accuracy: 0.5973 - val_loss: 0.9195 - val_accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.7192 - accuracy: 0.7137 - val_loss: 0.8530 - val_accuracy: 0.6000\n",
            "Epoch 24/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.5851 - accuracy: 0.7779 - val_loss: 1.2615 - val_accuracy: 0.5750\n",
            "Epoch 25/30\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.5941 - accuracy: 0.7104 - val_loss: 0.9476 - val_accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.5833 - accuracy: 0.7878 - val_loss: 0.8904 - val_accuracy: 0.6000\n",
            "Epoch 27/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.4399 - accuracy: 0.8644 - val_loss: 1.1733 - val_accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.3937 - accuracy: 0.8484 - val_loss: 1.0775 - val_accuracy: 0.5750\n",
            "Epoch 29/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.4108 - accuracy: 0.8002 - val_loss: 1.0467 - val_accuracy: 0.5750\n",
            "Epoch 30/30\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.4942 - accuracy: 0.7366 - val_loss: 1.1052 - val_accuracy: 0.5750\n",
            "Resultado para el fold 1: loss de 1.1052219867706299; accuracy de 57.499998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Entrenando para el fold 2 ...\n",
            "Epoch 1/30\n",
            "10/10 [==============================] - 1s 73ms/step - loss: 1.1413 - accuracy: 0.3396 - val_loss: 1.0827 - val_accuracy: 0.4500\n",
            "Epoch 2/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 1.0890 - accuracy: 0.3899 - val_loss: 1.0798 - val_accuracy: 0.4500\n",
            "Epoch 3/30\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 1.0635 - accuracy: 0.4465 - val_loss: 1.0879 - val_accuracy: 0.4500\n",
            "Epoch 4/30\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 1.0887 - accuracy: 0.4465 - val_loss: 1.0796 - val_accuracy: 0.4500\n",
            "Epoch 5/30\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 1.0722 - accuracy: 0.4465 - val_loss: 1.0652 - val_accuracy: 0.4500\n",
            "Epoch 6/30\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 1.0679 - accuracy: 0.4465 - val_loss: 1.0618 - val_accuracy: 0.4500\n",
            "Epoch 7/30\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 1.0646 - accuracy: 0.4465 - val_loss: 1.0590 - val_accuracy: 0.4500\n",
            "Epoch 8/30\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 1.0693 - accuracy: 0.4196"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-87464284b0bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;31m# opt = SGD(lr = 0.001, decay = 1e-6, momentum = 0.9, nesterov = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m results = cross_validate(model, X_train_n, y_train, 5,  k.optimizers.Adam(), 16, 30, \n\u001b[0;32m--> 226\u001b[0;31m                verbose = 1, show_history = False)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;31m# keras.backend.clear_session()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b60297541baf>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(model, x_train, y_train, num_folds, opt, batch_size, epochs, verbose, show_history, return_results)\u001b[0m\n\u001b[1;32m     34\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                             \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkfold_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkfold_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                             )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}