{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "two_inputs_net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9HugudT6GTSHk7bqLnTve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/two_inputs_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52ti41AT1QNV"
      },
      "source": [
        "kaggle = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk40fZH21oKG"
      },
      "source": [
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from tensorflow import keras\n",
        "import os, shutil, re\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "if kaggle:\n",
        "    from kaggle_datasets import KaggleDatasets\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "else:\n",
        "    from google.colab import drive\n",
        "import nibabel as nib\n",
        "\n",
        "# Import the most used layers\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Input, BatchNormalization, Dropout"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqk53tuI1us3",
        "outputId": "e207f8fb-2ddb-42bd-a16e-5a09a863dfb0"
      },
      "source": [
        "DEVICE = 'TPU' # or TPU\n",
        "tpu = None\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU, setting default strategy')\n",
        "        tpu = None\n",
        "        STRATEGY = tf.distribute.get_strategy()\n",
        "elif DEVICE == 'GPU':\n",
        "    STRATEGY = tf.distribute.MirroredStrategy()\n",
        "    \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = STRATEGY.num_replicas_in_sync\n",
        "\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not connect to TPU, setting default strategy\n",
            "Number of accelerators: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo66gGUaACMD",
        "outputId": "3065fbaf-5674-4ad2-c131-8852cd5bc73c"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGtYhpoff5Gc"
      },
      "source": [
        "# Ensemble model creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPhKLGYGOzJT"
      },
      "source": [
        "# Based on https://stackoverflow.com/questions/49546922/keras-replacing-input-layer\n",
        "def change_input_shape(model, new_shape, name=None):\n",
        "    # Extract model's configuration\n",
        "    model_config = model.get_config()\n",
        "    # Change config\n",
        "    if name is not None:\n",
        "        input_layer_name = name\n",
        "    else:\n",
        "        input_layer_name = model_config['layers'][0]['name']\n",
        "    model_config['layers'][0] = {\n",
        "                        'name': input_layer_name,\n",
        "                        'class_name': 'InputLayer',\n",
        "                        'config': {\n",
        "                            'batch_input_shape': new_shape,\n",
        "                            'dtype': 'float32',\n",
        "                            'sparse': False,\n",
        "                            'name': input_layer_name\n",
        "                        },\n",
        "                        'inbound_nodes': []\n",
        "                    }\n",
        "    model_config['layers'][1]['inbound_nodes'] = [[[input_layer_name, 0, 0, {}]]]\n",
        "    model_config['input_layers'] = [[input_layer_name, 0, 0]] \n",
        "    # Create new model\n",
        "    new_model = model.__class__.from_config(model_config, custom_objects={})\n",
        "    # Copy weights\n",
        "    weights = [layer.get_weights() for layer in model.layers[1:]]\n",
        "    for layer, weight in zip(new_model.layers[1:], weights):\n",
        "        layer.set_weights(weight)\n",
        "\n",
        "    return new_model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k44Gb-ZRahi"
      },
      "source": [
        "mri_shape = (121, 145, 121, 1)\n",
        "pet_shape = (79, 95, 68, 1)\n",
        "\n",
        "def build_model_4(input_shape):\n",
        "    inputs = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation='relu')(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    # x = Dropout(rate=0.1)(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"model_4\")\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY5he2XT_IbT"
      },
      "source": [
        "# # Pretrained ResNet for MRI\n",
        "# pretrained_resnet = keras.models.load_model('/content/drive/MyDrive/pretrained_models/pretrained_3D_resnet18.h5')\n",
        "# # Remove classifier layers\n",
        "# pretrained_resnet = keras.Model(pretrained_resnet.input, pretrained_resnet.layers[-4].output)\n",
        "# # Change input shape\n",
        "# mri_model = change_input_shape(pretrained_resnet, mri_shape, 'mri_input')\n",
        "# mri_model.trainable = False\n",
        "\n",
        "# # Add a new classifier\n",
        "# # inputs = mri_model.input\n",
        "# mri_inputs = keras.Input(shape=(121, 145, 121, 1), name='mri')\n",
        "# x = mri_model(mri_inputs, training = False)\n",
        "# x = keras.layers.GlobalAveragePooling3D()(x)\n",
        "# outputs = keras.layers.Dense(4)(x)\n",
        "# mri_model = keras.Model(mri_inputs, outputs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36F8zE1PUE-r"
      },
      "source": [
        "# pet_model = build_model_4(pet_shape)\n",
        "# # Delete classifier layer\n",
        "# pet_model = keras.Model(pet_model.input, pet_model.layers[-2].output)\n",
        "# # inputs = pet_model.input\n",
        "# pet_inputs = keras.Input(shape=(79, 95, 68, 1), name='pet')\n",
        "# x = pet_model(pet_inputs)\n",
        "# outputs = keras.layers.Flatten()(x)\n",
        "# pet_model = keras.Model(pet_inputs, outputs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw4-4i5sKKzq"
      },
      "source": [
        "# mri_input = keras.Input(shape=(121, 145, 121, 1), name='mri_input')\n",
        "# pet_input = keras.Input(shape=(79, 95, 68, 1), name='pet_input')\n",
        "\n",
        "# pet_features = pet_model(pet_input)\n",
        "# mri_features = mri_model(mri_input)\n",
        "\n",
        "# x = keras.layers.concatenate([pet_features, mri_features])\n",
        "# x = keras.layers.Flatten()(x)\n",
        "# outputs = keras.layers.Dense(3, name='label')(x)\n",
        "\n",
        "# def build_model():\n",
        "#     return keras.Model(inputs=[pet_input, mri_input], outputs = [outputs])\n",
        "\n",
        "\n",
        "# modelo = build_model()\n",
        "\n",
        "# print(modelo.summary())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWxAbvqIeIRN"
      },
      "source": [
        "def build_model():\n",
        "    pet_input = keras.Input(shape=(79,95,68,1), name='pet_input')\n",
        "    mri_input = keras.Input(shape=(121, 145, 121, 1), name='mri_input')\n",
        "\n",
        "    pet_features = keras.layers.Conv3D(16, 5)(pet_input)\n",
        "    mri_features = keras.layers.Conv3D(16, 5)(mri_input)\n",
        "\n",
        "    pet_features = keras.layers.GlobalAveragePooling3D()(pet_features)\n",
        "    mri_features = keras.layers.GlobalAveragePooling3D()(mri_features)\n",
        "\n",
        "    x = keras.layers.concatenate([pet_features, mri_features])\n",
        "\n",
        "    pred = keras.layers.Dense(3, name='label')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[pet_input, mri_input], outputs = [pred])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIxLfy2olCwz"
      },
      "source": [
        "# from tensorflow.keras import layers\n",
        "\n",
        "# num_tags = 12  # Number of unique issue tags\n",
        "# num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
        "# num_departments = 4  # Number of departments for predictions\n",
        "\n",
        "# title_input = keras.Input(\n",
        "#     shape=(None,), name=\"title\"\n",
        "# )  # Variable-length sequence of ints\n",
        "# body_input = keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\n",
        "# tags_input = keras.Input(\n",
        "#     shape=(num_tags,), name=\"tags\"\n",
        "# )  # Binary vectors of size `num_tags`\n",
        "\n",
        "# # Embed each word in the title into a 64-dimensional vector\n",
        "# title_features = layers.Embedding(num_words, 64)(title_input)\n",
        "# # Embed each word in the text into a 64-dimensional vector\n",
        "# body_features = layers.Embedding(num_words, 64)(body_input)\n",
        "\n",
        "# # Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
        "# title_features = layers.LSTM(128)(title_features)\n",
        "# # Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
        "# body_features = layers.LSTM(32)(body_features)\n",
        "\n",
        "# # Merge all available features into a single large vector via concatenation\n",
        "# x = layers.concatenate([title_features, body_features, tags_input])\n",
        "\n",
        "# # Stick a logistic regression for priority prediction on top of the features\n",
        "# priority_pred = layers.Dense(1, name=\"priority\")(x)\n",
        "# # Stick a department classifier on top of the features\n",
        "# department_pred = layers.Dense(num_departments, name=\"department\")(x)\n",
        "\n",
        "# # Instantiate an end-to-end model predicting both priority and department\n",
        "# model = keras.Model(\n",
        "#     inputs=[title_input, body_input, tags_input],\n",
        "#     outputs=[priority_pred, department_pred],\n",
        "# )\n",
        "\n",
        "# print(model.summary())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFfJiHkf-i5"
      },
      "source": [
        "# Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od9zbOTFgVnx"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "    \n",
        "    tfrec_format = {\n",
        "        \"image_pet\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image_mri\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"one_hot_label\": tf.io.VarLenFeature(tf.float32)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, tfrec_format)\n",
        "    one_hot_label = tf.sparse.to_dense(example['one_hot_label'])\n",
        "    one_hot_label = tf.reshape(one_hot_label, [NUM_CLASSES])\n",
        "    image_pet = tf.reshape(tf.sparse.to_dense(example['image_pet']), PET_SHAPE)\n",
        "    image_mri = tf.reshape(tf.sparse.to_dense(example['image_mri']), MRI_SHAPE)\n",
        "\n",
        "    # TPU needs size to be known statically, so this doesn't work\n",
        "    #     image  = tf.reshape(example['image'], example['shape']) \n",
        "    return {'pet_input':image_pet, 'mri_input':image_mri}, {'label':one_hot_label}\n",
        "\n",
        "def load_dataset(filenames, labels, use_tfrec, no_order=True):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        # Allow order-altering optimizations\n",
        "        option_no_order = tf.data.Options()\n",
        "        option_no_order.experimental_deterministic = False\n",
        "        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "        if no_order:\n",
        "            dataset = dataset.with_options(option_no_order)\n",
        "        dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO)\n",
        "\n",
        "    else:\n",
        "        dataset = tf.data.Dataset.from_generator(generator_fn(filenames, labels),\n",
        "            output_signature=(\n",
        "                 tf.TensorSpec(shape=IMG_SHAPE, dtype=tf.float32),\n",
        "                 tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames, use_tfrec):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
        "            for filename in filenames]\n",
        "        return np.sum(n)\n",
        "    else:\n",
        "        return len(filenames)\n",
        "\n",
        "def get_dataset(filenames, labels=None, use_tfrec=True, batch_size = 4, train=False, augment=False, cache=True, no_order=True):\n",
        "    \n",
        "    dataset =  load_dataset(filenames, labels, use_tfrec, no_order)\n",
        "    \n",
        "    if cache:\n",
        "        dataset = dataset.cache() # Do it only if dataset fits in ram\n",
        "    if train:\n",
        "        dataset = dataset.repeat()\n",
        "        if augment:\n",
        "            raise NotImplementedError\n",
        "#             dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "        dataset = dataset.shuffle(count_data_items(filenames, use_tfrec))\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr5PIICRghda"
      },
      "source": [
        "# Train schedule\n",
        "def get_lr_decay_callback(batch_size=4, verbose=False):\n",
        "    lr_max = 0.00001\n",
        "    lr_exp_decay = 0.9\n",
        "    lr_min = 0.000001\n",
        "    \n",
        "    def lrfn(epoch):\n",
        "        lr = (lr_max - lr_min) * lr_exp_decay**(epoch) + lr_min\n",
        "        return lr\n",
        "        \n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = verbose)\n",
        "    return lr_callback"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt6tAFvrgqaC"
      },
      "source": [
        "# Visualization\n",
        "\n",
        "def plot_epochs_history(num_epochs, history):\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(np.arange(num_epochs), history['accuracy'], '-o', label='Train acc',\n",
        "            color = '#ff7f0e')\n",
        "    plt.plot(np.arange(num_epochs), history['val_accuracy'], '-o', label='Val acc',\n",
        "            color = '#1f77b4')\n",
        "    x = np.argmax(history['val_accuracy']); y = np.max(history['val_accuracy'])\n",
        "    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x, y, s=150, color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max acc\\n%.2f'%y,size=14)\n",
        "    plt.ylabel('ACC', size=14); plt.xlabel('Epoch', size=14)\n",
        "    plt.legend(loc=2)\n",
        "    plt2 = plt.gca().twinx()\n",
        "    plt2.plot(np.arange(num_epochs),history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
        "    plt2.plot(np.arange(num_epochs),history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
        "    x = np.argmin(history['val_loss'] ); y = np.min(history['val_loss'])\n",
        "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x,y,s=150,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
        "    plt.ylabel('Loss',size=14)\n",
        "    plt.legend(loc=3)\n",
        "    plt.show()  \n",
        "    \n",
        "def plot_cm(labels, predictions):\n",
        "    \n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-3jsaWLgvZE"
      },
      "source": [
        "# Training and evaluation\n",
        "\n",
        "def evaluate_model_kfold(model_builder, train_filenames, n_folds, batch_size, epochs, \n",
        "                         plot_fold_results = True, plot_avg_results = True, train_labels=None, \n",
        "                         stratify=False, shuffle=True, random_state=None, use_tfrec=True):\n",
        "    \n",
        "    # np_rs = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(random_state)))\n",
        "    folds_histories = []\n",
        "\n",
        "    if stratify:\n",
        "        skf = StratifiedKFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "    else:\n",
        "        skf = KFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "\n",
        "    for fold, (idx_train, idx_val) in enumerate(skf.split(train_filenames, train_labels)):\n",
        "        if tpu != None:\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "        # np_rs.shuffle(idx_train)\n",
        "        X_train = train_filenames[idx_train]\n",
        "        X_val = train_filenames[idx_val]\n",
        "        y_train = None if use_tfrec is None else train_labels[idx_train]\n",
        "        y_val = None if use_tfrec is None else train_labels[idx_val]\n",
        "\n",
        "        # Build model\n",
        "        tf.keras.backend.clear_session()\n",
        "        with STRATEGY.scope():\n",
        "            model = model_builder()\n",
        "            # Optimizers and Losses create TF variables --> should always be initialized in the scope\n",
        "            OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "            LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "            model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS) #steps_per_execution=8)\n",
        "\n",
        "        # Train\n",
        "        print(f'Training for fold {fold + 1} of {n_folds}...')\n",
        "        cbks = [get_lr_decay_callback(batch_size)] # TODO: poner bien\n",
        "        history = model.fit(\n",
        "            get_dataset(X_train, y_train,  use_tfrec, train=True, augment=False, batch_size=batch_size), \n",
        "            epochs = EPOCHS, callbacks = cbks,\n",
        "            steps_per_epoch = max(1, int(np.rint(count_data_items(X_train, use_tfrec)/batch_size))))\n",
        "            # ,\n",
        "            # validation_data = get_dataset(X_val, y_val, use_tfrec, batch_size = batch_size, train=False) ,\n",
        "            # validation_steps= max(1, int(np.rint(count_data_items(X_val,use_tfrec)/batch_size))))\n",
        "    \n",
        "        if tf.__version__ == \"2.4.1\": # TODO: delete when tensorflow fixes the bug\n",
        "            scores = model.evaluate(get_dataset(X_train, y_train, use_tfrec, batch_size = batch_size, train=False), \n",
        "                                    batch_size = batch_size, steps = max(1, int(np.rint(count_data_items(X_train, use_tfrec)/batch_size))))\n",
        "            for i in range(len(model.metrics_names)):\n",
        "                history.history[model.metrics_names[i]][-1] = scores[i]\n",
        "            \n",
        "        folds_histories.append(history.history)\n",
        "        \n",
        "        if plot_fold_results:\n",
        "            plot_epochs_history(epochs, history.history)\n",
        "        \n",
        "    avg_history = avg_results_per_epoch(folds_histories)\n",
        "            \n",
        "    if plot_avg_results:\n",
        "        \n",
        "        plot_epochs_history(epochs, avg_history)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Results per fold')\n",
        "        for i in range(n_folds):\n",
        "            print('-'*80)\n",
        "            out = f\"> Fold {i + 1} - loss: {folds_histories[i]['loss'][-1]} - accuracy: {folds_histories[i]['accuracy'][-1]}\"\n",
        "            out += f\" - val_loss.: {folds_histories[i]['val_loss'][-1]} - val_accuracy: {folds_histories[i]['val_accuracy'][-1]}\"\n",
        "            print(out)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Average results over folds (on last epoch):')\n",
        "        print(f\"> loss: {avg_history['loss'][-1]}\")\n",
        "        print(f\"> accuracy: {avg_history['accuracy'][-1]}\")\n",
        "        print(f\"> cval_loss: {avg_history['val_loss'][-1]}\")\n",
        "        print(f\"> cval_accuracy: {avg_history['val_accuracy'][-1]}\")\n",
        "        print('-'*80)\n",
        "\n",
        "    return folds_histories\n",
        "\n",
        "def repeated_kfold(model_builder, train_filenames, n_folds, batch_size, epochs, reps=5, train_labels=None,\n",
        "                   stratify=True, shuffle=True, random_state=None, use_tfrec=True):\n",
        "    \n",
        "    reps_histories = []\n",
        "    \n",
        "    for i in range(reps):\n",
        "        print(f'Repetition {i + 1}')\n",
        "        folds_histories = evaluate_model_kfold(model_builder, train_filenames, n_folds,\n",
        "                                             batch_size, epochs, train_labels=train_labels, stratify=stratify,\n",
        "                                             shuffle=shuffle, random_state=random_state, use_tfrec=use_tfrec)\n",
        "\n",
        "        reps_histories.append(folds_histories)\n",
        "\n",
        "    return reps_histories\n",
        "\n",
        "def test_model_rkfold(model_builder, results_filename):\n",
        "    # Evaluate model with repeated k-fold (because of the high variance)\n",
        "    reps_results = repeated_kfold(model_builder, X_train, FOLDS, BATCH_SIZE, EPOCHS, reps=REPS, train_labels=y_train,\n",
        "                   random_state=SEED)\n",
        "    \n",
        "    # Save results to disk\n",
        "    f = open(results_filename, 'w' )\n",
        "    f.write(repr(reps_results))\n",
        "    f.close()\n",
        "    \n",
        "def train_model(model_builder):\n",
        "    # Test model\n",
        "    with STRATEGY.scope():\n",
        "        OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "        LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "        model = model_builder(IMG_SHAPE)\n",
        "        model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
        "\n",
        "\n",
        "    cbks = [get_lr_decay_callback(BATCH_SIZE)] # TODO: poner bien\n",
        "\n",
        "    history = model.fit(\n",
        "        get_dataset(X_train, train=True, batch_size=BATCH_SIZE), \n",
        "        epochs = EPOCHS, callbacks = cbks,\n",
        "        steps_per_epoch = int(np.rint(count_data_items(X_train, use_tfrec=True)/BATCH_SIZE))\n",
        "        )\n",
        "    \n",
        "    return model\n",
        "    \n",
        "def avg_results_per_epoch(histories):\n",
        "    \n",
        "    keys = list(histories[0].keys())\n",
        "    epochs = len(histories[0][keys[0]])\n",
        "    \n",
        "    avg_histories = dict()\n",
        "    for key in keys:\n",
        "        avg_histories[key] = [np.mean([x[key][i] for x in histories]) for i in range(epochs)]\n",
        "        \n",
        "    return avg_histories\n",
        "\n",
        "def avg_reps_results(reps_histories):\n",
        "    return avg_results_per_epoch([avg_results_per_epoch(history) for history in reps_histories])\n",
        "    \n",
        "def show_rkfold_results(results_file):\n",
        "    # Load results from disk\n",
        "    f = open(results_file, 'r')\n",
        "    reps_results = eval(f.read())\n",
        "    \n",
        "    reps_avgd_per_kfold = [avg_results_per_epoch(history) for history in reps_results]\n",
        "    reps_avg = avg_results_per_epoch(reps_avgd_per_kfold)\n",
        "    \n",
        "    # Plot final result over epochs\n",
        "    plot_epochs_history(EPOCHS, reps_avg)\n",
        "    \n",
        "    print('-'*80)\n",
        "    print('Results per repetition (on last epoch)')\n",
        "    for i in range(REPS):\n",
        "        print('-'*80)\n",
        "        print(f\"> Repetition {i + 1} - Loss: {reps_avgd_per_kfold[i]['val_loss'][-1]} - Accuracy : {reps_avgd_per_kfold[i]['val_accuracy'][-1]}\")\n",
        "\n",
        "    print('-'*80)\n",
        "    print('Average results over repetitions (on last epoch):')\n",
        "    print(f\"> Train Accuracy: {reps_avg['accuracy'][-1]}\")\n",
        "    print(f\"> Train Loss: {reps_avg['loss'][-1]}\")\n",
        "    print(f\"> CV accuracy: {reps_avg['val_accuracy'][-1]}\")\n",
        "    print(f\"> CV Loss: {reps_avg['val_loss'][-1]}\")\n",
        "    print('-'*80)\n",
        "    \n",
        "def show_test_results(model):\n",
        "    results = model.evaluate(get_dataset(X_test))\n",
        "\n",
        "    predictions = model.predict(get_dataset(X_test, no_order=False))\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "    plot_cm(y_test, y_pred)\n",
        "    print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "6gadxiFghj32",
        "outputId": "b788b4c2-8b3f-4a7f-bfac-5f0b91f4dd0f"
      },
      "source": [
        "SEED = 268\n",
        "\n",
        "# Datasets\n",
        "TFREC_DATASETS = ['tfrec-ensemble_mri_pet']\n",
        "# TFREC_DATASETS = ['tfrec-pet-spatialnorm-elastic']\n",
        "\n",
        "SHAPES = [(79, 95, 68, 1), (121, 145, 121, 1)]\n",
        "\n",
        "PET_SHAPE = SHAPES[0]\n",
        "MRI_SHAPE = SHAPES[1]\n",
        "\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 4\n",
        "REPS = 5\n",
        "FOLDS = 10\n",
        "LR = 0.00001\n",
        "\n",
        "CLASSES = ['NOR', 'AD', 'MCI']\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "METRICS = ['accuracy']\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "INPUT_DATAPATH = '/content/drive/MyDrive/data/'\n",
        "METADATA_PATH = '/content/drive/MyDrive/data/'\n",
        "\n",
        "\n",
        "def select_dataset(ds_id):\n",
        "    global DS, PET_SHAPE, DS_PATH, INPUT_DATAPATH\n",
        "    DS = TFREC_DATASETS[ds_id]\n",
        "    # IMG_SHAPE = SHAPES[ds_id]\n",
        "    if INPUT_DATAPATH == None:\n",
        "        user_secrets = UserSecretsClient()\n",
        "        user_credential = user_secrets.get_gcloud_credential()\n",
        "        user_secrets.set_tensorflow_credential(user_credential)\n",
        "        DS_PATH = KaggleDatasets().get_gcs_path(DS)\n",
        "    else:\n",
        "        DS_PATH = INPUT_DATAPATH + DS\n",
        "\n",
        "\n",
        "select_dataset(0)\n",
        "\n",
        "metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "\n",
        "X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "test_model_rkfold(build_model, 'ensemble_results.txt')\n",
        "# show_rkfold_results('ensemble_results.txt')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Repetition 1\n",
            "Training for fold 1 of 10...\n",
            "Epoch 1/50\n",
            "16/45 [=========>....................] - ETA: 9:36 - loss: 5.7840 - accuracy: 0.3281"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-cfaab97a23a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtest_model_rkfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ensemble_results.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m# show_rkfold_results('ensemble_results.txt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-cbc61f093bca>\u001b[0m in \u001b[0;36mtest_model_rkfold\u001b[0;34m(model_builder, results_filename)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Evaluate model with repeated k-fold (because of the high variance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     reps_results = repeated_kfold(model_builder, X_train, FOLDS, BATCH_SIZE, EPOCHS, reps=REPS, train_labels=y_train,\n\u001b[0;32m---> 98\u001b[0;31m                    random_state=SEED)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Save results to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-cbc61f093bca>\u001b[0m in \u001b[0;36mrepeated_kfold\u001b[0;34m(model_builder, train_filenames, n_folds, batch_size, epochs, reps, train_labels, stratify, shuffle, random_state, use_tfrec)\u001b[0m\n\u001b[1;32m     87\u001b[0m         folds_histories = evaluate_model_kfold(model_builder, train_filenames, n_folds,\n\u001b[1;32m     88\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                                              shuffle=shuffle, random_state=random_state, use_tfrec=use_tfrec)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mreps_histories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds_histories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-cbc61f093bca>\u001b[0m in \u001b[0;36mevaluate_model_kfold\u001b[0;34m(model_builder, train_filenames, n_folds, batch_size, epochs, plot_fold_results, plot_avg_results, train_labels, stratify, shuffle, random_state, use_tfrec)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0muse_tfrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             steps_per_epoch = max(1, int(np.rint(count_data_items(X_train, use_tfrec)/batch_size))))\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# validation_data = get_dataset(X_val, y_val, use_tfrec, batch_size = batch_size, train=False) ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}