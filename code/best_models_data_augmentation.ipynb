{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "best_models_data_augmentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAuQ2HB1QV7ifML9CKJe25",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/best_models_data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf4v7wj7Hro8"
      },
      "source": [
        "kaggle = False"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_ogIhEDJlhL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw1iuQ6LJYeJ"
      },
      "source": [
        "import os, shutil, re\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import scipy\n",
        "import skimage.transform as transform\n",
        "import seaborn as sns\n",
        "if kaggle:\n",
        "    from kaggle_datasets import KaggleDatasets\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "else:\n",
        "    from google.colab import drive\n",
        "import nibabel as nib\n",
        "\n",
        "# Import the most used layers\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Input, BatchNormalization, Dropout, ReLU"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4cCXmS8Jp3U"
      },
      "source": [
        "# Hardware config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMhnmx8tJZM3",
        "outputId": "be097a48-9305-4b1a-8548-71d4d4a77216"
      },
      "source": [
        "DEVICE = 'TPU' # or TPU\n",
        "tpu = None\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU, setting default strategy')\n",
        "        tpu = None\n",
        "        STRATEGY = tf.distribute.get_strategy()\n",
        "elif DEVICE == 'GPU':\n",
        "    STRATEGY = tf.distribute.MirroredStrategy()\n",
        "    \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = STRATEGY.num_replicas_in_sync\n",
        "\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not connect to TPU, setting default strategy\n",
            "Number of accelerators: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0xZepLOJxLM"
      },
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkQDZnLKJ1Ag"
      },
      "source": [
        "def augment_image(img):\n",
        "\n",
        "    img = img.squeeze()\n",
        "    original_shape = img.shape\n",
        "    img = random_rotations(img, -5, 5)\n",
        "    img = random_zoom(img, min=0.9, max=1.1)\n",
        "    img = random_shift(img, max=0.2)\n",
        "    # img = random_flip(img)\n",
        "    img = downscale(img, original_shape)\n",
        "    img = np.expand_dims(img, axis=3) # Restore channel axis\n",
        "    return img\n",
        "\n",
        "def downscale(image, shape):\n",
        "    'For upscale, anti_aliasing should be false'\n",
        "    return transform.resize(image, shape, mode='constant', anti_aliasing=True)\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "def tf_augment_image(input):\n",
        "    \"\"\" Tensorflow can't manage numpy functions, we have to wrap our augmentation function \"\"\"\n",
        "    img = tf.numpy_function(augment_image, [input], tf.float32)\n",
        "    return img\n",
        "\n",
        "def random_rotations(img, min_angle, max_angle):\n",
        "    \"\"\"\n",
        "    Rotate 3D image randomly\n",
        "    \"\"\"\n",
        "    assert img.ndim == 3, \"Image must be 3D\"\n",
        "    rotation_axes = [(1, 0), (1, 2), (0, 2)]\n",
        "    angle = np.random.randint(low=min_angle, high=max_angle+1)\n",
        "    axes_random_id = np.random.randint(low=0, high=len(rotation_axes))\n",
        "    axis = rotation_axes[axes_random_id] # Select a random rotation axis\n",
        "    return scipy.ndimage.rotate(img, angle, axes=axis)\n",
        "\n",
        "def random_zoom(img,min=0.7, max=1.2):\n",
        "    \"\"\"\n",
        "    Generate random zoom of a 3D image\n",
        "    \"\"\"\n",
        "    zoom = np.random.sample()*(max - min) + min # Generate random zoom between min and max\n",
        "    zoom_matrix = np.array([[zoom, 0, 0, 0],\n",
        "                            [0, zoom, 0, 0],\n",
        "                            [0, 0, zoom, 0],\n",
        "                            [0, 0, 0, 1]])\n",
        "    \n",
        "    return scipy.ndimage.interpolation.affine_transform(img, zoom_matrix)\n",
        "\n",
        "def random_flip(img):\n",
        "    \"\"\"\n",
        "    Flip image over a random axis\n",
        "    \"\"\"\n",
        "    axes = [0, 1, 2]\n",
        "    rand_axis = np.random.randint(len(axes))\n",
        "    img = img.swapaxes(rand_axis, 0)\n",
        "    img = img[::-1, ...]\n",
        "    img = img.swapaxes(0, rand_axis)\n",
        "    img = np.squeeze(img)\n",
        "    return img\n",
        "\n",
        "def random_shift(img, max=0.4):\n",
        "    \"\"\"\n",
        "    Random shift over a random axis\n",
        "    \"\"\"\n",
        "    (x, y, z) = img.shape\n",
        "    (max_shift_x, max_shift_y, max_shift_z) = int(x*max/2),int(y*max/2), int(z*max/2)\n",
        "    shift_x = np.random.randint(-max_shift_x, max_shift_x)\n",
        "    shift_y = np.random.randint(-max_shift_y,max_shift_y)\n",
        "    shift_z = np.random.randint(-max_shift_z,max_shift_z)\n",
        "\n",
        "    translation_matrix = np.array([[1, 0, 0, shift_x],\n",
        "                                   [0, 1, 0, shift_y],\n",
        "                                   [0, 0, 1, shift_z],\n",
        "                                   [0, 0, 0, 1]\n",
        "                                   ])\n",
        "\n",
        "    return scipy.ndimage.interpolation.affine_transform(img, translation_matrix)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYVIuCAUJuJy"
      },
      "source": [
        "# Tfrecords loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lYpggG7JbEE"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "    \n",
        "    tfrec_format = {\n",
        "        \"image\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"one_hot_label\": tf.io.VarLenFeature(tf.float32)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, tfrec_format)\n",
        "    one_hot_label = tf.sparse.to_dense(example['one_hot_label'])\n",
        "    one_hot_label = tf.reshape(one_hot_label, [NUM_CLASSES])\n",
        "    image = tf.reshape(tf.sparse.to_dense(example['image']), IMG_SHAPE)\n",
        "    # TPU needs size to be known statically, so this doesn't work\n",
        "    #     image  = tf.reshape(example['image'], example['shape']) \n",
        "    return image, one_hot_label\n",
        "\n",
        "\n",
        "def load_dataset(filenames, labels, use_tfrec, no_order=True):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        # Allow order-altering optimizations\n",
        "        option_no_order = tf.data.Options()\n",
        "        option_no_order.experimental_deterministic = False\n",
        "        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "        if no_order:\n",
        "            dataset = dataset.with_options(option_no_order)\n",
        "        dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO)\n",
        "\n",
        "    else:\n",
        "        dataset = tf.data.Dataset.from_generator(generator_fn(filenames, labels),\n",
        "            output_signature=(\n",
        "                 tf.TensorSpec(shape=IMG_SHAPE, dtype=tf.float32),\n",
        "                 tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames, use_tfrec):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
        "            for filename in filenames]\n",
        "        return np.sum(n)\n",
        "    else:\n",
        "        return len(filenames)\n",
        "\n",
        "def get_dataset(filenames, labels=None, use_tfrec=True, batch_size = 4, train=False, augment=False, cache=True, no_order=True):\n",
        "    \n",
        "    dataset =  load_dataset(filenames, labels, use_tfrec, no_order)\n",
        "    \n",
        "    if cache:\n",
        "        dataset = dataset.cache() # Do it only if dataset fits in ram\n",
        "    if train:\n",
        "        dataset = dataset.repeat()\n",
        "        if augment:\n",
        "            # raise NotImplementedError\n",
        "\n",
        "            dataset = dataset.map(lambda img, label: (tf_augment_image(img), label), num_parallel_calls=AUTO)\n",
        "\n",
        "            # dataset = dataset.map(tf_augment_image, num_parallel_calls=AUTO)\n",
        "        dataset = dataset.shuffle(count_data_items(filenames, use_tfrec))\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OZV2vu8KJUR"
      },
      "source": [
        "# Train schedule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z15mykxvKLx-"
      },
      "source": [
        "def get_lr_decay_callback(batch_size=4, verbose=False):\n",
        "    lr_max = 0.00001\n",
        "    lr_exp_decay = 0.9\n",
        "    lr_min = 0.000001\n",
        "    \n",
        "    def lrfn(epoch):\n",
        "        lr = (lr_max - lr_min) * lr_exp_decay**(epoch) + lr_min\n",
        "        return lr\n",
        "        \n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = verbose)\n",
        "    return lr_callback"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaqDbK9nKOt_"
      },
      "source": [
        "# Build models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW6FgdurKRRV"
      },
      "source": [
        "def build_model_4(input_shape):\n",
        "    inputs = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation='relu')(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    # x = Dropout(rate=0.1)(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"model_4\")\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVQnYIAJKaeq"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-xLwGfUKcqS"
      },
      "source": [
        "def plot_epochs_history(num_epochs, history):\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(np.arange(num_epochs), history['accuracy'], '-o', label='Train acc',\n",
        "            color = '#ff7f0e')\n",
        "    plt.plot(np.arange(num_epochs), history['val_accuracy'], '-o', label='Val acc',\n",
        "            color = '#1f77b4')\n",
        "    x = np.argmax(history['val_accuracy']); y = np.max(history['val_accuracy'])\n",
        "    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x, y, s=150, color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max acc\\n%.2f'%y,size=14)\n",
        "    plt.ylabel('ACC', size=14); plt.xlabel('Epoch', size=14)\n",
        "    plt.legend(loc=2)\n",
        "    plt2 = plt.gca().twinx()\n",
        "    plt2.plot(np.arange(num_epochs),history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
        "    plt2.plot(np.arange(num_epochs),history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
        "    x = np.argmin(history['val_loss'] ); y = np.min(history['val_loss'])\n",
        "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x,y,s=150,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
        "    plt.ylabel('Loss',size=14)\n",
        "    plt.legend(loc=3)\n",
        "    plt.show()  \n",
        "    \n",
        "def plot_cm(labels, predictions):\n",
        "    \n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fga8ZFz9KjfU"
      },
      "source": [
        "# Functions for training and evaluating models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZoalXK6Kn1e"
      },
      "source": [
        "def evaluate_model_kfold(model_builder, train_filenames, n_folds, batch_size, epochs, \n",
        "                         plot_fold_results = True, plot_avg_results = True, train_labels=None, \n",
        "                         stratify=False, shuffle=True, random_state=None, use_tfrec=True, augment=True):\n",
        "    \n",
        "    # np_rs = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(random_state)))\n",
        "    folds_histories = []\n",
        "\n",
        "    if stratify:\n",
        "        skf = StratifiedKFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "    else:\n",
        "        skf = KFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "\n",
        "    for fold, (idx_train, idx_val) in enumerate(skf.split(train_filenames, train_labels)):\n",
        "        if tpu != None:\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "        # np_rs.shuffle(idx_train)\n",
        "        X_train = train_filenames[idx_train]\n",
        "        X_val = train_filenames[idx_val]\n",
        "        y_train = None if use_tfrec is None else train_labels[idx_train]\n",
        "        y_val = None if use_tfrec is None else train_labels[idx_val]\n",
        "\n",
        "        # Build model\n",
        "        tf.keras.backend.clear_session()\n",
        "        with STRATEGY.scope():\n",
        "            model = model_builder(input_shape=IMG_SHAPE)\n",
        "            # Optimizers and Losses create TF variables --> should always be initialized in the scope\n",
        "            OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "            LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "            model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS, steps_per_execution=8)\n",
        "\n",
        "        # Train\n",
        "        print(f'Training for fold {fold + 1} of {n_folds}...')\n",
        "        cbks = [get_lr_decay_callback(batch_size)] # TODO: poner bien\n",
        "        history = model.fit(\n",
        "            get_dataset(X_train, y_train,  use_tfrec, train=True, augment=augment, batch_size=batch_size), \n",
        "            epochs = EPOCHS, callbacks = cbks,\n",
        "            steps_per_epoch = max(1, int(np.rint(count_data_items(X_train, use_tfrec)/batch_size))),\n",
        "            validation_data = get_dataset(X_val, y_val, use_tfrec, batch_size = batch_size, train=False), \n",
        "            validation_steps= max(1, int(np.rint(count_data_items(X_val,use_tfrec)/batch_size))))\n",
        "    \n",
        "        if tf.__version__ == \"2.4.1\": # TODO: delete when tensorflow fixes the bug\n",
        "            scores = model.evaluate(get_dataset(X_train, y_train, use_tfrec, batch_size = batch_size, train=False), \n",
        "                                    batch_size = batch_size, steps = max(1, int(np.rint(count_data_items(X_train, use_tfrec)/batch_size))))\n",
        "            for i in range(len(model.metrics_names)):\n",
        "                history.history[model.metrics_names[i]][-1] = scores[i]\n",
        "            \n",
        "        folds_histories.append(history.history)\n",
        "        \n",
        "        if plot_fold_results:\n",
        "            plot_epochs_history(epochs, history.history)\n",
        "        \n",
        "    avg_history = avg_results_per_epoch(folds_histories)\n",
        "            \n",
        "    if plot_avg_results:\n",
        "        \n",
        "        plot_epochs_history(epochs, avg_history)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Results per fold')\n",
        "        for i in range(n_folds):\n",
        "            print('-'*80)\n",
        "            out = f\"> Fold {i + 1} - loss: {folds_histories[i]['loss'][-1]} - accuracy: {folds_histories[i]['accuracy'][-1]}\"\n",
        "            out += f\" - val_loss.: {folds_histories[i]['val_loss'][-1]} - val_accuracy: {folds_histories[i]['val_accuracy'][-1]}\"\n",
        "            print(out)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Average results over folds (on last epoch):')\n",
        "        print(f\"> loss: {avg_history['loss'][-1]}\")\n",
        "        print(f\"> accuracy: {avg_history['accuracy'][-1]}\")\n",
        "        print(f\"> cval_loss: {avg_history['val_loss'][-1]}\")\n",
        "        print(f\"> cval_accuracy: {avg_history['val_accuracy'][-1]}\")\n",
        "        print('-'*80)\n",
        "\n",
        "    return folds_histories\n",
        "\n",
        "def repeated_kfold(model_builder, train_filenames, n_folds, batch_size, epochs, reps=5, train_labels=None,\n",
        "                   stratify=True, shuffle=True, random_state=None, use_tfrec=True):\n",
        "    \n",
        "    reps_histories = []\n",
        "    \n",
        "    for i in range(reps):\n",
        "        print(f'Repetition {i + 1}')\n",
        "        folds_histories = evaluate_model_kfold(model_builder, train_filenames, n_folds,\n",
        "                                             batch_size, epochs, train_labels=train_labels, stratify=stratify,\n",
        "                                             shuffle=shuffle, random_state=random_state, use_tfrec=use_tfrec)\n",
        "\n",
        "        reps_histories.append(folds_histories)\n",
        "\n",
        "    return reps_histories\n",
        "\n",
        "def test_model_rkfold(model_builder, results_filename):\n",
        "    # Evaluate model with repeated k-fold (because of the high variance)\n",
        "    reps_results = repeated_kfold(model_builder, X_train, FOLDS, BATCH_SIZE, EPOCHS, reps=REPS, train_labels=y_train,\n",
        "                   random_state=SEED)\n",
        "    \n",
        "    # Save results to disk\n",
        "    f = open(results_filename, 'w' )\n",
        "    f.write(repr(reps_results))\n",
        "    f.close()\n",
        "    \n",
        "def train_model(model_builder):\n",
        "    # Test model\n",
        "    with STRATEGY.scope():\n",
        "        OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "        LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "        model = model_builder(IMG_SHAPE)\n",
        "        model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
        "\n",
        "\n",
        "    cbks = [get_lr_decay_callback(BATCH_SIZE)] # TODO: poner bien\n",
        "\n",
        "    history = model.fit(\n",
        "        get_dataset(X_train, train=True, batch_size=BATCH_SIZE), \n",
        "        epochs = EPOCHS, callbacks = cbks,\n",
        "        steps_per_epoch = int(np.rint(count_data_items(X_train, use_tfrec=True)/BATCH_SIZE))\n",
        "        )\n",
        "    \n",
        "    return model\n",
        "    \n",
        "def avg_results_per_epoch(histories):\n",
        "    \n",
        "    keys = list(histories[0].keys())\n",
        "    epochs = len(histories[0][keys[0]])\n",
        "    \n",
        "    avg_histories = dict()\n",
        "    for key in keys:\n",
        "        avg_histories[key] = [np.mean([x[key][i] for x in histories]) for i in range(epochs)]\n",
        "        \n",
        "    return avg_histories\n",
        "\n",
        "def avg_reps_results(reps_histories):\n",
        "    return avg_results_per_epoch([avg_results_per_epoch(history) for history in reps_histories])\n",
        "    \n",
        "def show_rkfold_results(results_file):\n",
        "    # Load results from disk\n",
        "    f = open(results_file, 'r')\n",
        "    reps_results = eval(f.read())\n",
        "    \n",
        "    reps_avgd_per_kfold = [avg_results_per_epoch(history) for history in reps_results]\n",
        "    reps_avg = avg_results_per_epoch(reps_avgd_per_kfold)\n",
        "    \n",
        "    # Plot final result over epochs\n",
        "    plot_epochs_history(EPOCHS, reps_avg)\n",
        "    \n",
        "    print('-'*80)\n",
        "    print('Results per repetition (on last epoch)')\n",
        "    for i in range(REPS):\n",
        "        print('-'*80)\n",
        "        print(f\"> Repetition {i + 1} - Loss: {reps_avgd_per_kfold[i]['val_loss'][-1]} - Accuracy : {reps_avgd_per_kfold[i]['val_accuracy'][-1]}\")\n",
        "\n",
        "    print('-'*80)\n",
        "    print('Average results over repetitions (on last epoch):')\n",
        "    print(f\"> Train Accuracy: {reps_avg['accuracy'][-1]}\")\n",
        "    print(f\"> Train Loss: {reps_avg['loss'][-1]}\")\n",
        "    print(f\"> CV accuracy: {reps_avg['val_accuracy'][-1]}\")\n",
        "    print(f\"> CV Loss: {reps_avg['val_loss'][-1]}\")\n",
        "    print('-'*80)\n",
        "    \n",
        "def show_test_results(model):\n",
        "    results = model.evaluate(get_dataset(X_test))\n",
        "\n",
        "    predictions = model.predict(get_dataset(X_test, no_order=False))\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "    plot_cm(y_test, y_pred)\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySm58mdzKsy2"
      },
      "source": [
        "# Prepare datasets and evaluation constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LFBy9NsKsXJ",
        "outputId": "1ee2f83f-ee3c-4a1b-cc66-877d4e8c2bb1"
      },
      "source": [
        "SEED = 268\n",
        "\n",
        "TFREC_DATASETS = ['tfrec-pet-spatialnorm-elastic-standarized', \n",
        "                  'tfrec-mri-grey-minmax']\n",
        "SHAPES = [(79, 95, 68, 1),\n",
        "          (160, 160, 96, 1)]\n",
        "\n",
        "REPS = 5\n",
        "FOLDS = 10\n",
        "\n",
        "CLASSES = ['NOR', 'AD', 'MCI']\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "METRICS = ['accuracy']\n",
        "\n",
        "if kaggle:\n",
        "    INPUT_DATAPATH = '/kaggle/input/' if tpu is None else None\n",
        "    METADATA_PATH = '/kaggle/input/'\n",
        "else:\n",
        "    drive.mount('/content/drive') \n",
        "    INPUT_DATAPATH = '/content/drive/MyDrive/data/'\n",
        "    METADATA_PATH = '/content/drive/MyDrive/data/'"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5VLQbxtK-jt"
      },
      "source": [
        "# Training/testing with PET images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imu8_5EqLniv",
        "outputId": "0a306610-f12f-4dbe-951f-44b584150f43"
      },
      "source": [
        "def select_dataset(ds_id):\n",
        "    global DS, IMG_SHAPE, DS_PATH, INPUT_DATAPATH\n",
        "    DS = TFREC_DATASETS[ds_id]\n",
        "    IMG_SHAPE = SHAPES[ds_id]\n",
        "    if INPUT_DATAPATH == None:\n",
        "        user_secrets = UserSecretsClient()\n",
        "        user_credential = user_secrets.get_gcloud_credential()\n",
        "        user_secrets.set_tensorflow_credential(user_credential)\n",
        "        DS_PATH = KaggleDatasets().get_gcs_path(DS)\n",
        "    else:\n",
        "        DS_PATH = INPUT_DATAPATH + DS\n",
        "\n",
        "\n",
        "select_dataset(0) # Not intensity normalized\n",
        "\n",
        "metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "\n",
        "X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "\n",
        "LR = 0.00001\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "\n",
        "test_model_rkfold(build_model_4, 'pet-spatialnorm-elastic-standarized_model-4_augmentation_results.txt')\n",
        "show_rkfold_results('pet-spatialnorm-elastic-standarized_model-4_augmentation_results.txt')\n",
        "model = train_model(build_model_4)\n",
        "show_test_results(model)\n",
        "model.save('pet-spatialnorm-elastic-standarized_augmentation_model-4.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Repetition 1\n",
            "Training for fold 1 of 10...\n",
            "Epoch 1/50\n",
            "45/45 [==============================] - 184s 4s/step - loss: 1.1209 - accuracy: 0.4056 - val_loss: 1.0942 - val_accuracy: 0.4500\n",
            "Epoch 2/50\n",
            "45/45 [==============================] - 90s 2s/step - loss: 1.0688 - accuracy: 0.4889 - val_loss: 1.0885 - val_accuracy: 0.4500\n",
            "Epoch 3/50\n",
            "45/45 [==============================] - 92s 2s/step - loss: 1.0702 - accuracy: 0.4778 - val_loss: 1.0907 - val_accuracy: 0.4500\n",
            "Epoch 4/50\n",
            "45/45 [==============================] - 91s 2s/step - loss: 1.0518 - accuracy: 0.4500 - val_loss: 1.0883 - val_accuracy: 0.4500\n",
            "Epoch 5/50\n",
            "45/45 [==============================] - 90s 2s/step - loss: 1.0386 - accuracy: 0.4500 - val_loss: 1.0824 - val_accuracy: 0.4500\n",
            "Epoch 6/50\n",
            "45/45 [==============================] - 91s 2s/step - loss: 1.0129 - accuracy: 0.5167 - val_loss: 1.0695 - val_accuracy: 0.4500\n",
            "Epoch 7/50\n",
            "45/45 [==============================] - 90s 2s/step - loss: 1.0387 - accuracy: 0.4778 - val_loss: 1.0756 - val_accuracy: 0.4500\n",
            "Epoch 8/50\n",
            "45/45 [==============================] - 91s 2s/step - loss: 1.0328 - accuracy: 0.4556 - val_loss: 1.0600 - val_accuracy: 0.5000\n",
            "Epoch 9/50\n",
            "45/45 [==============================] - 91s 2s/step - loss: 1.0118 - accuracy: 0.5111 - val_loss: 1.0486 - val_accuracy: 0.5000\n",
            "Epoch 10/50\n",
            "45/45 [==============================] - 90s 2s/step - loss: 0.9781 - accuracy: 0.5333 - val_loss: 1.0290 - val_accuracy: 0.5000\n",
            "Epoch 11/50\n",
            "45/45 [==============================] - 90s 2s/step - loss: 0.9632 - accuracy: 0.5389 - val_loss: 1.0138 - val_accuracy: 0.5000\n",
            "Epoch 12/50\n",
            "45/45 [==============================] - 90s 2s/step - loss: 0.9507 - accuracy: 0.5500 - val_loss: 0.9912 - val_accuracy: 0.5000\n",
            "Epoch 13/50\n",
            "45/45 [==============================] - 91s 2s/step - loss: 0.9596 - accuracy: 0.5222 - val_loss: 0.9737 - val_accuracy: 0.5500\n",
            "Epoch 14/50\n",
            "45/45 [==============================] - 90s 2s/step - loss: 0.9799 - accuracy: 0.5056 - val_loss: 0.9583 - val_accuracy: 0.5500\n",
            "Epoch 15/50\n",
            "45/45 [==============================] - 90s 2s/step - loss: 0.9534 - accuracy: 0.5444 - val_loss: 0.9505 - val_accuracy: 0.5000\n",
            "Epoch 16/50\n",
            "45/45 [==============================] - 90s 2s/step - loss: 0.9704 - accuracy: 0.5278 - val_loss: 0.9394 - val_accuracy: 0.4500\n",
            "Epoch 17/50\n",
            "45/45 [==============================] - 91s 2s/step - loss: 0.9511 - accuracy: 0.5167 - val_loss: 0.9209 - val_accuracy: 0.5000\n",
            "Epoch 18/50\n",
            "45/45 [==============================] - 90s 2s/step - loss: 0.9345 - accuracy: 0.5000 - val_loss: 0.9118 - val_accuracy: 0.4500\n",
            "Epoch 19/50\n",
            "16/45 [=========>....................] - ETA: 58s - loss: 0.9258 - accuracy: 0.5938 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY-SHWj4X3Ww"
      },
      "source": [
        "# Training/testing with MRI images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ujNJDzrZgVW"
      },
      "source": [
        "select_dataset(1) # Not intensity normalized\n",
        "\n",
        "metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "\n",
        "X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "\n",
        "LR = 0.00001\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "\n",
        "test_model_rkfold(build_model_4, 'mri-grey-minmax_model-4_augmentation_results.txt')\n",
        "show_rkfold_results('mri-grey-minmax_model-4_augmentation_results.txt')\n",
        "model = train_model(build_model_4)\n",
        "show_test_results(model)\n",
        "model.save('mri-grey-minmax_augmentation_model-4.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}