{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "experiments1-4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMr0eB/KdqUTMa3vBp7hrc0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/experiments1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqYngRbOdQ14"
      },
      "source": [
        "kaggle = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4cD0E6qF4Ev"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-3d75vmFdIZ"
      },
      "source": [
        "import shutil\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "if tf.io.gfile.exists('Alzheimer-disease-classification'):\n",
        "    shutil.rmtree('Alzheimer-disease-classification')\n",
        "! git clone https://github.com/Angelvj/Alzheimer-disease-classification.git\n",
        "\n",
        "if not kaggle:\n",
        "    sys.path.insert(0,'/content/Alzheimer-disease-classification/code')\n",
        "else:\n",
        "    sys.path.insert(0, './Alzheimer-disease-classification/code')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9BT-NVON7ui"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "if not kaggle:\n",
        "    from google.colab import drive\n",
        "else:\n",
        "    from kaggle_datasets import KaggleDatasets\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "import functions.lr_schedules as lr_schedules\n",
        "import functions.io_utils as io\n",
        "import functions.data_augmentation as augmentation\n",
        "from functions.tfrec_loading import get_dataset\n",
        "from functions.model_evaluation import repeated_kfold, plot_epochs_history, get_rkf_history, get_predictions, calculate_results, train_model, present_results\n",
        "from functions.general import change_input_shape\n",
        "from models import feedforward_models_pet, feedforward_models_mri"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SongU7emF9U8"
      },
      "source": [
        "# Hardware config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frYJqzP7GBeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc341f5-e217-4879-e916-3411b3097adc"
      },
      "source": [
        "DEVICE = 'TPU' # or GPU\n",
        "tpu = None\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU, setting default strategy')\n",
        "        tpu = None\n",
        "        STRATEGY = tf.distribute.get_strategy()\n",
        "elif DEVICE == 'GPU':\n",
        "    STRATEGY = tf.distribute.MirroredStrategy()\n",
        "    \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = STRATEGY.num_replicas_in_sync\n",
        "\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not connect to TPU, setting default strategy\n",
            "Number of accelerators: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br-Gf3QtG0N3"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgVlJERZHFN3"
      },
      "source": [
        "SEED = 268 # Arbitrary seed\n",
        "\n",
        "# Name of different datasets used\n",
        "TFREC_DATASETS = ['tfrec-pet-spatialnorm-elastic-maxintensitynorm',\n",
        "                  'tfrec-mri_grey-standarized-downscale',\n",
        "                  'tfrec-mri_grey-standarized',\n",
        "                  'tfrec-pet-minmax',\n",
        "                  'tfrec-pet-standarized',\n",
        "                  'tfrec-pet-minmax-standarized',\n",
        "                  'tfrec-pet-maxintensitynorm',\n",
        "                  'tfrec-pet'\n",
        "                 ]\n",
        "\n",
        "# Shape of images on each dataset\n",
        "SHAPES = [(79, 95, 68, 1), (75, 90, 75, 1), (121, 145, 121, 1), (79, 95, 68, 1),\n",
        "          (79, 95, 68, 1), (79, 95, 68, 1), (79, 95, 68, 1), (79, 95, 68, 1)]\n",
        "\n",
        "# Number of repetitions and folds for repeated k-fold\n",
        "REPS = 5\n",
        "FOLDS = 10\n",
        "\n",
        "# Different classes on the dataset\n",
        "CLASSES = ['NOR', 'AD', 'MCI']\n",
        "\n",
        "# Path to data\n",
        "if kaggle:\n",
        "    INPUT_DATAPATH = '/kaggle/input/' if tpu is None else None\n",
        "    METADATA_PATH = '/kaggle/input/'\n",
        "else:\n",
        "    drive.mount('/content/drive') \n",
        "    INPUT_DATAPATH = '/content/drive/MyDrive/data/'\n",
        "    METADATA_PATH = '/content/drive/MyDrive/data/'\n",
        "\n",
        "\n",
        "# Function for setting up constant for a given dataset\n",
        "def select_dataset(ds_id):\n",
        "    global DS, IMG_SHAPE, DS_PATH, INPUT_DATAPATH, X_train, y_train, X_test, y_test\n",
        "    DS = TFREC_DATASETS[ds_id]\n",
        "    IMG_SHAPE = SHAPES[ds_id]\n",
        "    if INPUT_DATAPATH == None:\n",
        "        user_secrets = UserSecretsClient()\n",
        "        user_credential = user_secrets.get_gcloud_credential()\n",
        "        user_secrets.set_tensorflow_credential(user_credential)\n",
        "        DS_PATH = KaggleDatasets().get_gcs_path(DS)\n",
        "    else:\n",
        "        DS_PATH = INPUT_DATAPATH + DS\n",
        "\n",
        "    # Load filenames and labels\n",
        "    metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "    metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "    X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "    y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "    X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "    y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1k0ITb7HNIS"
      },
      "source": [
        "#  Phase 1: depth study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB1MzXUMHSyK"
      },
      "source": [
        "## PET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab1NrdBTlurg"
      },
      "source": [
        "select_dataset(0) # Preprocessed PET dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTxD5Z_omUfn"
      },
      "source": [
        "### Experiment 1 --> 1 convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8KbMdLBc0rI"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 50\n",
        "LR = 0.00001\n",
        "DECAY = 0.1\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_0, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "# save_dict(detailed_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-0_results.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-0.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_rGKpbxOfV2"
      },
      "source": [
        "### Experiment 2 --> 2 concolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtnGZqqiOkIh"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "LR = 0.00001\n",
        "DECAY = 0.1\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_1, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "# save_dict(detailed_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-1_results.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-1.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM1byuFBOsZS"
      },
      "source": [
        "### Experiment 3 --> 3 convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Ss3dl2O4km"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 50\n",
        "LR = 0.0001\n",
        "DECAY = 0.01\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_2, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "# save_dict(detailed_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-2_results.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-2.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adr9xkE6PQUB"
      },
      "source": [
        "### Experiment 4 --> 6 convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f0S3447PaJN"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 40\n",
        "LR = 0.000001\n",
        "DECAY = 0.1\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_3, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "# save_dict(detailed_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-3_results.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-3.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le3ZxeSQPupc"
      },
      "source": [
        "### Experiment 5 --> 8 convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMuQE0_JP4ba"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "LR = 0.00001\n",
        "DECAY = 0.1\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_4, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "# save_dict(detailed_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-4_results.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-4.txt')\n",
        "\n",
        "# Train and get results on test data\n",
        "model_4 = train_model(feedforward_models_pet.model_4, X_train, IMG_SHAPE, STRATEGY,\n",
        "                      AUTO, BATCH_SIZE, EPOCHS, cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "y_pred = get_predictions(model_4, X_test, IMG_SHAPE, 3, AUTO, BATCH_SIZE)\n",
        "results = calculate_results(y_test, y_pred)\n",
        "present_results(results, {0:'NOR', 1:'AD', 2:'MCI'})\n",
        "io.save_dict(results, 'model_4-pet-results.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3fGyE9CHWrU"
      },
      "source": [
        "## MRI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF5acLhFTFcg"
      },
      "source": [
        "select_dataset(1) # Preprocessed MRI dataset, downscaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dgbbS75Wd1L"
      },
      "source": [
        "### Experiment 1 --> 1 convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LyRD4_UXQys"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "LR = 0.000001\n",
        "DECAY = 0.1\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_mri.model_0, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "# save_dict(detailed_history, 'mri-grey-standarized-downscale_model-0.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'mri-grey-standarized-downscale_model-0.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZbZSUrQWlhg"
      },
      "source": [
        "### Experiment 2 --> 3 convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc-gJEFVXReM"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 70\n",
        "LR = 0.00001\n",
        "DECAY = 0.1\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_mri.model_1, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "# save_dict(detailed_history, 'mri-grey-standarized-downscale_model-1.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'mri-grey-standarized-downscale_model-1.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzuVPvUKWriE"
      },
      "source": [
        "### Experiment 3 --> 4 convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seDWgwZhXSnM"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "LR = 0.000001\n",
        "DECAY = 0.1\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_mri.model_2, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "# save_dict(detailed_history, 'mri-grey-standarized-downscale_model-2.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'mri-grey-standarized-downscale_model-2.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzWF-YTtWz3t"
      },
      "source": [
        "### Experiment 4 --> 6 convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl8KE7KzXTFp"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "LR = 0.000001\n",
        "DECAY = 0.1\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_mri.model_3, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "# save_dict(detailed_history, 'mri-grey-standarized-downscale_model-3.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'mri-grey-standarized-downscale_model-3.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5zbTXReW9UQ"
      },
      "source": [
        "### Experiment 5 --> 9 convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NWfcdIkXTpi"
      },
      "source": [
        "select_dataset(2) # Preprocessed MRI dataset, not downscaled\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "LR = 0.000001\n",
        "DECAY = 0.1\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_mri.model_4, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "# save_dict(detailed_history, 'mri-grey-standarized-downscale_model-4.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'mri-grey-standarized_model-4.txt')\n",
        "\n",
        "# Train and get results on test data\n",
        "model_4 = train_model(feedforward_models_mri.model_4, X_train, IMG_SHAPE, STRATEGY,\n",
        "                      AUTO, BATCH_SIZE, EPOCHS, cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "y_pred = get_predictions(model_4, X_test, IMG_SHAPE, 3, AUTO, BATCH_SIZE)\n",
        "results = calculate_results(y_test, y_pred)\n",
        "present_results(results, {0:'NOR', 1:'AD', 2:'MCI'})\n",
        "io.save_dict(results, 'model_4-mri-results.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S28QwoSawrN_"
      },
      "source": [
        "# Phase 2: Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Jo08lPxb_2"
      },
      "source": [
        "## PET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSwZGZ57xMZz"
      },
      "source": [
        "# Define augmentation for pet images\n",
        "def augment_image(img):\n",
        "    img = img.squeeze()\n",
        "    original_shape = img.shape\n",
        "    img = augmentation.random_rotations(img, -0.5, 0.5)\n",
        "    img = augmentation.random_shift(img, 0.2)\n",
        "    img = augmentation.downscale(img, original_shape)\n",
        "    img = np.expand_dims(img, axis=3) \n",
        "    return img\n",
        "\n",
        "# This function allow us to use numpy (augmentation) functions in training time with tensorflow\n",
        "@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "def tf_augment_image(input):\n",
        "    img = tf.numpy_function(augment_image, [input], tf.float32)\n",
        "    return img\n",
        "\n",
        "select_dataset(0) # Preprocessed PET dataset\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 50\n",
        "LR = 0.0001\n",
        "DECAY = 0.1 \n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_4_augmentation, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)],\n",
        "                         augment=tf_augment_image)\n",
        "# save_dict(detailed_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-4_results.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'pet-spatialnorm-elastic-maxintensitynorm_model-4_augmentation.txt')\n",
        "\n",
        "# Train and get results on test data\n",
        "model_4 = train_model(feedforward_models_pet.model_4_augmentation, X_train, IMG_SHAPE, STRATEGY,\n",
        "                      AUTO, BATCH_SIZE, EPOCHS, cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)],\n",
        "                      augment=tf_augment_image)\n",
        "y_pred = get_predictions(model_4, X_test, IMG_SHAPE, 3, AUTO, BATCH_SIZE)\n",
        "results = calculate_results(y_test, y_pred)\n",
        "present_results(results, {0:'NOR', 1:'AD', 2:'MCI'})\n",
        "io.save_dict(results, 'model-4_augmentation_pet_results.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE016Rs2xeOE"
      },
      "source": [
        "## MRI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOLaRWBryUsr"
      },
      "source": [
        "# Define augmentation for pet images\n",
        "def augment_image(img):\n",
        "    img = img.squeeze()\n",
        "    original_shape = img.shape\n",
        "    img = augmentation.random_rotations(img, -0.5, 0.5)\n",
        "    img = augmentation.random_zoom(img, 0.95, 1.05)\n",
        "    img = augmentation.random_shift(img, 0.2)\n",
        "    img = augmentation.downscale(img, original_shape)\n",
        "    img = np.expand_dims(img, axis=3) \n",
        "    return img\n",
        "\n",
        "# This function allow us to use numpy (augmentation) functions in training time with tensorflow\n",
        "@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "def tf_augment_image(input):\n",
        "    img = tf.numpy_function(augment_image, [input], tf.float32)\n",
        "    return img\n",
        "\n",
        "select_dataset(2) # Preprocessed MRI dataset, not downscaled\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "LR = 0.000001\n",
        "DECAY = 0.1\n",
        "\n",
        "# Data augmentation hyperparams\n",
        "MAX_ROTATION = 0.5\n",
        "ZOOM = (0.95, 1.05)\n",
        "SHIFT = 0.2\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_mri.model_4, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)],\n",
        "                         augment=tf_augment_image)\n",
        "# save_dict(detailed_history, 'mri-grey-standarized_model-4_augmentation.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'mri-grey-standarized_model-4_augmentation.txt')\n",
        "\n",
        "# Train and get results on test data\n",
        "model_4 = train_model(feedforward_models_mri.model_4, X_train, IMG_SHAPE, STRATEGY,\n",
        "                      AUTO, BATCH_SIZE, EPOCHS, cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)],\n",
        "                      augment=tf_augment_image)\n",
        "y_pred = get_predictions(model_4, X_test, IMG_SHAPE, 3, AUTO, BATCH_SIZE)\n",
        "results = calculate_results(y_test, y_pred)\n",
        "present_results(results, {0:'NOR', 1:'AD', 2:'MCI'})\n",
        "io.save_dict(results, 'model-4_augmentation_mri_results.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxzBjT-68K6n"
      },
      "source": [
        "# Phase 3: Raw images (only PET)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctQ9RBth8fVO"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "LR = 0.00001\n",
        "DECAY = 0.1\n",
        "\n",
        "select_dataset(0) # Really bad results\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_4, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "\n",
        "select_dataset(4) # Better but equally bad\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_4, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "\n",
        "select_dataset(5) # Again, bad results\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_4, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "\n",
        "select_dataset(6) # Bad\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_4, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "\n",
        "select_dataset(7) # The worst, as expected\n",
        "\n",
        "detailed_history = repeated_kfold(feedforward_models_pet.model_4, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)])\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26qjXA4I8r9m"
      },
      "source": [
        "# Phase 4: Transfer learning\n",
        "\n",
        "Note: for running this experiment, a pretrained resnet should be located at /content/drive/MyDrive/pretrained_models/pretrained_3D_resnet18.h5 (run the notebook pretrain_resnet.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD_V1_hKEL0Q"
      },
      "source": [
        "def build_pretrained_resnet18(input_shape):\n",
        "    # Load pretrained resnet \n",
        "    pretrained_resnet18 = tf.keras.models.load_model('/content/drive/MyDrive/pretrained_models/pretrained_3D_resnet18.h5')\n",
        "    # Delete classifier\n",
        "    pretrained_resnet18 = tf.keras.Model(pretrained_resnet18.input, pretrained_resnet18.layers[-2].output)\n",
        "    # Change input shape to match our dataset\n",
        "    pretrained_resnet18 = change_input_shape(pretrained_resnet18, IMG_SHAPE, name='new_input')\n",
        "\n",
        "    pretrained_resnet18.trainable = False\n",
        "    inputs = tf.keras.layers.Input(shape=IMG_SHAPE, name='pet_input')\n",
        "    features = pretrained_resnet18(inputs, training=False)\n",
        "    # Add new classifier at the end\n",
        "    outputs = tf.keras.layers.Dense(3)(features)\n",
        "    pretrained_resnet18 = tf.keras.Model(inputs, outputs)\n",
        "    return pretrained_resnet18"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBIQ0uG8GD-s"
      },
      "source": [
        "## PET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO6H8yA48y8E"
      },
      "source": [
        "select_dataset(0) # Preprocessed PET dataset\n",
        "\n",
        "# Configure augmentation\n",
        "def augment_image(img):\n",
        "    img = img.squeeze()\n",
        "    original_shape = img.shape\n",
        "    img = augmentation.random_rotations(img, -0.5, 0.5)\n",
        "    img = augmentation.random_shift(img, 0.2)\n",
        "    img = augmentation.downscale(img, original_shape)\n",
        "    img = np.expand_dims(img, axis=3) \n",
        "    return img\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "def tf_augment_image(input):\n",
        "    img = tf.numpy_function(augment_image, [input], tf.float32)\n",
        "    return img\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 30\n",
        "LR = 0.0001\n",
        "DECAY = 1 # Not decay\n",
        "\n",
        "detailed_history = repeated_kfold(build_pretrained_resnet18, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)],\n",
        "                         augment=tf_augment_image)\n",
        "# save_dict(detailed_history, 'pet-spatialnorm-elastic-maxintensitynorm_resnet18_results.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "io.save_dict(rkf_history, 'pet-spatialnorm-elastic-maxintensitynorm_resnet18.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r85T1EiZGGT2"
      },
      "source": [
        "## MRI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Oyu9UVHJTz"
      },
      "source": [
        "select_dataset(2) # Preprocessed MRI dataset, not downscaled\n",
        "\n",
        "# Configure augmentation\n",
        "def augment_image(img):\n",
        "    img = img.squeeze()\n",
        "    original_shape = img.shape\n",
        "    img = augmentation.random_rotations(img, -0.5, 0.5)\n",
        "    img = augmentation.random_zoom(img, 0.95, 1.05)\n",
        "    img = augmentation.random_shift(img, 0.2)\n",
        "    img = augmentation.downscale(img, original_shape)\n",
        "    img = np.expand_dims(img, axis=3) \n",
        "    return img\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "def tf_augment_image(input):\n",
        "    img = tf.numpy_function(augment_image, [input], tf.float32)\n",
        "    return img\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 30\n",
        "LR = 0.0001\n",
        "DECAY = 1 # Not decay\n",
        "\n",
        "detailed_history = repeated_kfold(build_pretrained_resnet18, X_train, y_train, \n",
        "                         IMG_SHAPE, STRATEGY, tpu, AUTO, FOLDS, BATCH_SIZE, EPOCHS, REPS, \n",
        "                         cbks=[lr_schedules.get_exp_lr_decay_callback(LR, DECAY, EPOCHS)],\n",
        "                         augment=tf_augment_image)\n",
        "# save_dict(detailed_history, 'mri-grey-standarized_resnet18_results.txt')\n",
        "rkf_history = get_rkf_history(detailed_history)\n",
        "plot_epochs_history(EPOCHS, rkf_history)\n",
        "mri-grey-standarized_resnet_augmentation.txt\n",
        "io.save_dict(rkf_history, 'mri-grey-standarized_resnet18.txt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}