{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "ad-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3hOtmEwd7UGR",
        "ZQXvbqc17UGS"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/ad_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFGbGzyF7bcP"
      },
      "source": [
        "kaggle = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knFeHc_V7UGH"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T07:18:34.519821Z",
          "iopub.execute_input": "2021-07-14T07:18:34.520567Z",
          "iopub.status.idle": "2021-07-14T07:18:34.528267Z",
          "shell.execute_reply.started": "2021-07-14T07:18:34.520519Z",
          "shell.execute_reply": "2021-07-14T07:18:34.527008Z"
        },
        "trusted": true,
        "id": "71DFtTWm7UGK"
      },
      "source": [
        "import os, shutil, re\n",
        "import pickle\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "if kaggle:\n",
        "    from kaggle_datasets import KaggleDatasets\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "else:\n",
        "    from google.colab import drive\n",
        "import nibabel as nib\n",
        "\n",
        "# Import the most used layers\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Input, BatchNormalization, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkQ9DiIo7UGL"
      },
      "source": [
        "# Hardware config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T07:18:36.982855Z",
          "iopub.execute_input": "2021-07-14T07:18:36.983371Z",
          "iopub.status.idle": "2021-07-14T07:18:42.729134Z",
          "shell.execute_reply.started": "2021-07-14T07:18:36.983324Z",
          "shell.execute_reply": "2021-07-14T07:18:42.727666Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6640VE-n7UGM",
        "outputId": "81d4ad20-cd95-482a-dbfe-143c5107966a"
      },
      "source": [
        "DEVICE = 'TPU' # or TPU\n",
        "tpu = None\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU, setting default strategy')\n",
        "        tpu = None\n",
        "        STRATEGY = tf.distribute.get_strategy()\n",
        "elif DEVICE == 'GPU':\n",
        "    STRATEGY = tf.distribute.MirroredStrategy()\n",
        "    \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = STRATEGY.num_replicas_in_sync\n",
        "\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not connect to TPU, setting default strategy\n",
            "Number of accelerators: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXtHPBOm7UGN"
      },
      "source": [
        "# Functions for loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2021-07-14T07:18:54.799124Z",
          "iopub.execute_input": "2021-07-14T07:18:54.799730Z",
          "iopub.status.idle": "2021-07-14T07:18:54.816920Z",
          "shell.execute_reply.started": "2021-07-14T07:18:54.799693Z",
          "shell.execute_reply": "2021-07-14T07:18:54.816057Z"
        },
        "trusted": true,
        "id": "Vbp2phM67UGN"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "    \n",
        "    tfrec_format = {\n",
        "        \"image\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"one_hot_label\": tf.io.VarLenFeature(tf.float32)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, tfrec_format)\n",
        "    one_hot_label = tf.sparse.to_dense(example['one_hot_label'])\n",
        "    one_hot_label = tf.reshape(one_hot_label, [NUM_CLASSES])\n",
        "    image = tf.reshape(tf.sparse.to_dense(example['image']), IMG_SHAPE)\n",
        "    # TPU needs size to be known statically, so this doesn't work\n",
        "    #     image  = tf.reshape(example['image'], example['shape']) \n",
        "    return image, one_hot_label\n",
        "\n",
        "# =========================================================================\n",
        "# The three functions below are used for generating a dataset directly from\n",
        "# the filenames of the data (via a callable generator).\n",
        "# ==========================================================================\n",
        "\n",
        "def load_image(path):    \n",
        "\n",
        "    img = nib.load(path)\n",
        "    img = np.asarray(img.dataobj, dtype=np.float32)\n",
        "    img = np.expand_dims(img, axis=3) # Add channel axis\n",
        "    return img\n",
        "\n",
        "def parse_file(filename, label):\n",
        "\n",
        "    image = load_image(filename)\n",
        "    image = np.nan_to_num(image, copy=False)\n",
        "    label = np.eye(3, dtype=np.float32)[label]\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def generator_fn(filenames, labels):\n",
        "\n",
        "    def images_generator():\n",
        "\n",
        "        for X, y in zip(filenames, labels):\n",
        "            X, y = parse_file(X, y)\n",
        "            yield X, y\n",
        "            \n",
        "    return images_generator\n",
        "\n",
        "def load_dataset(filenames, labels, use_tfrec, no_order=True):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        # Allow order-altering optimizations\n",
        "        option_no_order = tf.data.Options()\n",
        "        option_no_order.experimental_deterministic = False\n",
        "        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "        if no_order:\n",
        "            dataset = dataset.with_options(option_no_order)\n",
        "        dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO)\n",
        "\n",
        "    else:\n",
        "        dataset = tf.data.Dataset.from_generator(generator_fn(filenames, labels),\n",
        "            output_signature=(\n",
        "                 tf.TensorSpec(shape=IMG_SHAPE, dtype=tf.float32),\n",
        "                 tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames, use_tfrec):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
        "            for filename in filenames]\n",
        "        return np.sum(n)\n",
        "    else:\n",
        "        return len(filenames)\n",
        "\n",
        "def get_dataset(filenames, labels=None, use_tfrec=True, batch_size = 4, train=False, augment=False, cache=True, no_order=True):\n",
        "    \n",
        "    dataset =  load_dataset(filenames, labels, use_tfrec, no_order)\n",
        "    \n",
        "    if cache:\n",
        "        dataset = dataset.cache() # Do it only if dataset fits in ram\n",
        "    if train:\n",
        "        dataset = dataset.repeat()\n",
        "        if augment:\n",
        "            raise NotImplementedError\n",
        "#             dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "        dataset = dataset.shuffle(count_data_items(filenames, use_tfrec))\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IvxTPNk7UGP"
      },
      "source": [
        "# Train schedule\n",
        "\n",
        "**Note**: not used yet (will be used when pretrain)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T07:19:01.360315Z",
          "iopub.execute_input": "2021-07-14T07:19:01.360873Z",
          "iopub.status.idle": "2021-07-14T07:19:01.369023Z",
          "shell.execute_reply.started": "2021-07-14T07:19:01.360827Z",
          "shell.execute_reply": "2021-07-14T07:19:01.368305Z"
        },
        "trusted": true,
        "id": "nAq1ZLrr7UGP"
      },
      "source": [
        "# Only for pretraining\n",
        "def get_lr_callback(batch_size=4, verbose=False):\n",
        "    lr_start = 0.00001\n",
        "    lr_max = 0.00004 * batch_size\n",
        "    lr_min = 0.00001\n",
        "    lr_rampup_epochs = 3\n",
        "    lr_sustain_epochs = 0\n",
        "    lr_exp_decay = 0.7\n",
        "\n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_rampup_epochs:\n",
        "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
        "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
        "            lr = lr_max\n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
        "        return lr\n",
        "    \n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = verbose)\n",
        "    return lr_callback\n",
        "\n",
        "# For training from scratch\n",
        "def get_lr_decay_callback(batch_size=4, verbose=False):\n",
        "    lr_max = 0.00001\n",
        "    lr_exp_decay = 0.9\n",
        "    lr_min = 0.000001\n",
        "    \n",
        "    def lrfn(epoch):\n",
        "        lr = (lr_max - lr_min) * lr_exp_decay**(epoch) + lr_min\n",
        "        return lr\n",
        "        \n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = verbose)\n",
        "    return lr_callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK6evO2s7UGQ"
      },
      "source": [
        "# Build models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T07:19:04.824208Z",
          "iopub.execute_input": "2021-07-14T07:19:04.824652Z",
          "iopub.status.idle": "2021-07-14T07:19:04.840357Z",
          "shell.execute_reply.started": "2021-07-14T07:19:04.824612Z",
          "shell.execute_reply": "2021-07-14T07:19:04.839448Z"
        },
        "trusted": true,
        "id": "pDHqKyxZ7UGQ"
      },
      "source": [
        "# TODO (not important): define models in another file\n",
        "def build_model_0(input_shape):\n",
        "    \n",
        "    inputs = tf.keras.layers.Input(input_shape)\n",
        "    \n",
        "    x = tf.keras.layers.Conv3D(filters=32, kernel_size=5, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "    \n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "   \n",
        "    model = tf.keras.Model(inputs, outputs, name=\"model_0\")\n",
        "    return model\n",
        "\n",
        "def build_model_1(input_shape):\n",
        "    \n",
        "    inputs = tf.keras.layers.Input(input_shape)\n",
        "    \n",
        "    x = Conv3D(filters=32, kernel_size=5, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "    \n",
        "    x = Conv3D(filters=32, kernel_size=5, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "    \n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs, outputs, name=\"model_1\")\n",
        "    return model\n",
        "\n",
        "def build_model_2(input_shape):\n",
        "    \n",
        "    inputs = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "    x = Conv3D(filters=16, kernel_size=5, activation=\"relu\")(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=128, kernel_size=5, activation=\"relu\")(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"model_2\")\n",
        "    return model\n",
        "\n",
        "def build_model_3(input_shape):\n",
        "    \n",
        "    inputs = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    # x = Dropout(rate=0.1)(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"model_3\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_model_4(input_shape):\n",
        "    inputs = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = Conv3D(filters=16, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = Conv3D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation='relu')(x)\n",
        "    x = Conv3D(filters=128, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    # x = Dropout(rate=0.1)(x)\n",
        "    x = Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = Dense(units=3, activation=\"softmax\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"model_4\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hOtmEwd7UGR"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T07:19:08.368016Z",
          "iopub.execute_input": "2021-07-14T07:19:08.368655Z",
          "iopub.status.idle": "2021-07-14T07:19:08.384736Z",
          "shell.execute_reply.started": "2021-07-14T07:19:08.368595Z",
          "shell.execute_reply": "2021-07-14T07:19:08.383561Z"
        },
        "trusted": true,
        "id": "4jFoQTH37UGR"
      },
      "source": [
        "def plot_epochs_history(num_epochs, history):\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(np.arange(num_epochs), history['accuracy'], '-o', label='Train acc',\n",
        "            color = '#ff7f0e')\n",
        "    plt.plot(np.arange(num_epochs), history['val_accuracy'], '-o', label='Val acc',\n",
        "            color = '#1f77b4')\n",
        "    x = np.argmax(history['val_accuracy']); y = np.max(history['val_accuracy'])\n",
        "    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x, y, s=150, color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max acc\\n%.2f'%y,size=14)\n",
        "    plt.ylabel('ACC', size=14); plt.xlabel('Epoch', size=14)\n",
        "    plt.legend(loc=2)\n",
        "    plt2 = plt.gca().twinx()\n",
        "    plt2.plot(np.arange(num_epochs),history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
        "    plt2.plot(np.arange(num_epochs),history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
        "    x = np.argmin(history['val_loss'] ); y = np.min(history['val_loss'])\n",
        "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x,y,s=150,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
        "    plt.ylabel('Loss',size=14)\n",
        "    plt.legend(loc=3)\n",
        "    plt.show()  \n",
        "    \n",
        "def plot_cm(labels, predictions):\n",
        "    \n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQXvbqc17UGS"
      },
      "source": [
        "# Functions for training and evaluating models\n",
        "\n",
        "**Note**: bug in tf 2.4.1 (accuracies showed while training is different than the ones returned in history). [See issue here](https://github.com/tensorflow/tensorflow/issues/48033)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T07:23:52.798697Z",
          "iopub.execute_input": "2021-07-14T07:23:52.799129Z",
          "iopub.status.idle": "2021-07-14T07:23:52.836372Z",
          "shell.execute_reply.started": "2021-07-14T07:23:52.799093Z",
          "shell.execute_reply": "2021-07-14T07:23:52.835021Z"
        },
        "trusted": true,
        "id": "puELknqJ7UGS"
      },
      "source": [
        "def evaluate_model_kfold(model_builder, train_filenames, n_folds, batch_size, epochs, \n",
        "                         plot_fold_results = True, plot_avg_results = True, train_labels=None, \n",
        "                         stratify=False, shuffle=True, random_state=None, use_tfrec=True):\n",
        "    \n",
        "    # np_rs = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(random_state)))\n",
        "    folds_histories = []\n",
        "\n",
        "    if stratify:\n",
        "        skf = StratifiedKFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "    else:\n",
        "        skf = KFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "\n",
        "    for fold, (idx_train, idx_val) in enumerate(skf.split(train_filenames, train_labels)):\n",
        "        if tpu != None:\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "        # np_rs.shuffle(idx_train)\n",
        "        X_train = train_filenames[idx_train]\n",
        "        X_val = train_filenames[idx_val]\n",
        "        y_train = None if use_tfrec is None else train_labels[idx_train]\n",
        "        y_val = None if use_tfrec is None else train_labels[idx_val]\n",
        "\n",
        "        # Build model\n",
        "        tf.keras.backend.clear_session()\n",
        "        with STRATEGY.scope():\n",
        "            model = model_builder(input_shape=IMG_SHAPE)\n",
        "            # Optimizers and Losses create TF variables --> should always be initialized in the scope\n",
        "            OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "            LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "            model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS, steps_per_execution=8)\n",
        "\n",
        "        # Train\n",
        "        print(f'Training for fold {fold + 1} of {n_folds}...')\n",
        "        cbks = [get_lr_decay_callback(batch_size)] # TODO: poner bien\n",
        "        history = model.fit(\n",
        "            get_dataset(X_train, y_train,  use_tfrec, train=True, augment=False, batch_size=batch_size), \n",
        "            epochs = EPOCHS, callbacks = cbks,\n",
        "            steps_per_epoch = max(1, int(np.rint(count_data_items(X_train, use_tfrec)/batch_size))),\n",
        "            validation_data = get_dataset(X_val, y_val, use_tfrec, batch_size = batch_size, train=False), \n",
        "            validation_steps= max(1, int(np.rint(count_data_items(X_val,use_tfrec)/batch_size))))\n",
        "    \n",
        "        if tf.__version__ == \"2.4.1\": # TODO: delete when tensorflow fixes the bug\n",
        "            scores = model.evaluate(get_dataset(X_train, y_train, use_tfrec, batch_size = batch_size, train=False), \n",
        "                                    batch_size = batch_size, steps = max(1, int(np.rint(count_data_items(X_train, use_tfrec)/batch_size))))\n",
        "            for i in range(len(model.metrics_names)):\n",
        "                history.history[model.metrics_names[i]][-1] = scores[i]\n",
        "            \n",
        "        folds_histories.append(history.history)\n",
        "        \n",
        "        if plot_fold_results:\n",
        "            plot_epochs_history(epochs, history.history)\n",
        "        \n",
        "    avg_history = avg_results_per_epoch(folds_histories)\n",
        "            \n",
        "    if plot_avg_results:\n",
        "        \n",
        "        plot_epochs_history(epochs, avg_history)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Results per fold')\n",
        "        for i in range(n_folds):\n",
        "            print('-'*80)\n",
        "            out = f\"> Fold {i + 1} - loss: {folds_histories[i]['loss'][-1]} - accuracy: {folds_histories[i]['accuracy'][-1]}\"\n",
        "            out += f\" - val_loss.: {folds_histories[i]['val_loss'][-1]} - val_accuracy: {folds_histories[i]['val_accuracy'][-1]}\"\n",
        "            print(out)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Average results over folds (on last epoch):')\n",
        "        print(f\"> loss: {avg_history['loss'][-1]}\")\n",
        "        print(f\"> accuracy: {avg_history['accuracy'][-1]}\")\n",
        "        print(f\"> cval_loss: {avg_history['val_loss'][-1]}\")\n",
        "        print(f\"> cval_accuracy: {avg_history['val_accuracy'][-1]}\")\n",
        "        print('-'*80)\n",
        "\n",
        "    return folds_histories\n",
        "\n",
        "def repeated_kfold(model_builder, train_filenames, n_folds, batch_size, epochs, reps=5, train_labels=None,\n",
        "                   stratify=True, shuffle=True, random_state=None, use_tfrec=True):\n",
        "    \n",
        "    reps_histories = []\n",
        "    \n",
        "    for i in range(reps):\n",
        "        print(f'Repetition {i + 1}')\n",
        "        folds_histories = evaluate_model_kfold(model_builder, train_filenames, n_folds,\n",
        "                                             batch_size, epochs, train_labels=train_labels, stratify=stratify,\n",
        "                                             shuffle=shuffle, random_state=random_state, use_tfrec=use_tfrec)\n",
        "\n",
        "        reps_histories.append(folds_histories)\n",
        "\n",
        "    return reps_histories\n",
        "\n",
        "def test_model_rkfold(model_builder, results_filename):\n",
        "    # Evaluate model with repeated k-fold (because of the high variance)\n",
        "    reps_results = repeated_kfold(model_builder, X_train, FOLDS, BATCH_SIZE, EPOCHS, reps=REPS, train_labels=y_train,\n",
        "                   random_state=SEED)\n",
        "    \n",
        "    # Save results to disk\n",
        "    f = open(results_filename, 'w' )\n",
        "    f.write(repr(reps_results))\n",
        "    f.close()\n",
        "    \n",
        "def train_model(model_builder):\n",
        "    # Test model\n",
        "    with STRATEGY.scope():\n",
        "        OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "        LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "        model = model_builder(IMG_SHAPE)\n",
        "        model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
        "\n",
        "\n",
        "    cbks = [get_lr_decay_callback(BATCH_SIZE)] # TODO: poner bien\n",
        "\n",
        "    history = model.fit(\n",
        "        get_dataset(X_train, train=True, batch_size=BATCH_SIZE), \n",
        "        epochs = EPOCHS, callbacks = cbks,\n",
        "        steps_per_epoch = int(np.rint(count_data_items(X_train, use_tfrec=True)/BATCH_SIZE))\n",
        "        )\n",
        "    \n",
        "    return model\n",
        "    \n",
        "def avg_results_per_epoch(histories):\n",
        "    \n",
        "    keys = list(histories[0].keys())\n",
        "    epochs = len(histories[0][keys[0]])\n",
        "    \n",
        "    avg_histories = dict()\n",
        "    for key in keys:\n",
        "        avg_histories[key] = [np.mean([x[key][i] for x in histories]) for i in range(epochs)]\n",
        "        \n",
        "    return avg_histories\n",
        "\n",
        "def avg_reps_results(reps_histories):\n",
        "    return avg_results_per_epoch([avg_results_per_epoch(history) for history in reps_histories])\n",
        "    \n",
        "def show_rkfold_results(results_file):\n",
        "    # Load results from disk\n",
        "    f = open(results_file, 'r')\n",
        "    reps_results = eval(f.read())\n",
        "    \n",
        "    reps_avgd_per_kfold = [avg_results_per_epoch(history) for history in reps_results]\n",
        "    reps_avg = avg_results_per_epoch(reps_avgd_per_kfold)\n",
        "    \n",
        "    # Plot final result over epochs\n",
        "    plot_epochs_history(EPOCHS, reps_avg)\n",
        "    \n",
        "    print('-'*80)\n",
        "    print('Results per repetition (on last epoch)')\n",
        "    for i in range(REPS):\n",
        "        print('-'*80)\n",
        "        print(f\"> Repetition {i + 1} - Loss: {reps_avgd_per_kfold[i]['val_loss'][-1]} - Accuracy : {reps_avgd_per_kfold[i]['val_accuracy'][-1]}\")\n",
        "\n",
        "    print('-'*80)\n",
        "    print('Average results over repetitions (on last epoch):')\n",
        "    print(f\"> Train Accuracy: {reps_avg['accuracy'][-1]}\")\n",
        "    print(f\"> Train Loss: {reps_avg['loss'][-1]}\")\n",
        "    print(f\"> CV accuracy: {reps_avg['val_accuracy'][-1]}\")\n",
        "    print(f\"> CV Loss: {reps_avg['val_loss'][-1]}\")\n",
        "    print('-'*80)\n",
        "    \n",
        "def show_test_results(model):\n",
        "\n",
        "    predictions = model.predict(get_dataset(X_test, no_order=False))\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "    plot_cm(y_test, y_pred)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "def get_predictions(model, X):\n",
        "    predict_proba = model.predict(get_dataset(X, no_order=False))\n",
        "    y_pred = np.argmax(predict_proba, axis=1)\n",
        "    return y_pred\n",
        "\n",
        "def calculate_results(y_true, y_pred, labels = [0, 1, 2]):\n",
        "\n",
        "    results = dict()\n",
        "    results['sensitivity'] = dict()\n",
        "    results['specificity'] = dict()\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred, labels)\n",
        "\n",
        "    for i in range(len(cm)):\n",
        "\n",
        "        true_positives = cm[i, i]\n",
        "        false_positives_idx = [j for j in range(len(cm)) if j != i]\n",
        "        false_positives = np.sum(cm[false_positives_idx, i])\n",
        "\n",
        "        true_negatives = 0\n",
        "        for j in range(len(cm)):\n",
        "            for k in range(len(cm)):\n",
        "                if j != i and k != i:\n",
        "                    true_negatives += cm[j,k]\n",
        "\n",
        "        false_negatives_idx = [j for j in range(len(cm)) if j != i]\n",
        "        false_negatives = np.sum(cm[i, false_negatives_idx])\n",
        "\n",
        "        sensitivity = true_positives/(true_positives + false_negatives)\n",
        "        specificity = true_negatives/(true_negatives + false_positives)\n",
        "\n",
        "        results['sensitivity'][labels[i]] = sensitivity\n",
        "        results['specificity'][labels[i]] = specificity\n",
        "\n",
        "    results['cm'] = cm\n",
        "    results['accuracy'] = (true_positives + true_negatives)/np.sum(cm)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def present_results(results_dict, class_names):\n",
        "\n",
        "    print('Accuracy:', results_dict['accuracy'], '\\n')\n",
        "    for c in class_names:\n",
        "        print(f'---------- Results for class {class_names[c]} ----------')\n",
        "        print(f\" - Sensitivity: {results_dict['sensitivity'][c]}\")\n",
        "        print(f\" - Specificity: {results_dict['specificity'][c]}\\n\")\n",
        "\n",
        "    print(\" --------- Confusion matrix --------- \\n\")\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(results_dict['cm'], ['NOR', 'AD', 'MCI'])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "\n",
        "def save_dict(dic, filename):\n",
        "    f = open(filename,'w')\n",
        "    f.write(str(dic))\n",
        "    f.close()\n",
        "\n",
        "def load_dict(filename):\n",
        "    f = open(filename,'r')\n",
        "    data=f.read()\n",
        "    data = data.replace('array', 'np.array')\n",
        "    f.close()\n",
        "    return eval(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfSwVGZw7UGV"
      },
      "source": [
        "# Prepare datasets and evaluation constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T07:24:00.753943Z",
          "iopub.execute_input": "2021-07-14T07:24:00.754379Z",
          "iopub.status.idle": "2021-07-14T07:24:00.762130Z",
          "shell.execute_reply.started": "2021-07-14T07:24:00.754332Z",
          "shell.execute_reply": "2021-07-14T07:24:00.760816Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J239o6X7UGX",
        "outputId": "015382e5-a250-4bf1-dec1-1f1c662169cb"
      },
      "source": [
        "SEED = 268 # Arbitrary seed\n",
        "\n",
        "# Name of different datasets used\n",
        "TFREC_DATASETS = ['tfrec-pet-spatialnorm-elastic',\n",
        "                  'tfrec-pet-spatialnorm-elastic-maxintensitynorm',\n",
        "                  'tfrec-pet-spatialnorm-elastic-standarized',\n",
        "                  'tfrec-pet-spatialnorm-rigid',\n",
        "                  'tfrec-pet-spatialnorm-rigid-standarized',\n",
        "                  'tfrec-pet-spatialnorm-regid-maxintensitynorm-standarized'\n",
        "                 ]\n",
        "\n",
        "# IMG_DATASETS = ['ad-preprocessed', None, None]\n",
        "\n",
        "# Shape of images on each dataset\n",
        "SHAPES = [(79, 95, 68, 1), (79, 95, 68, 1), (79, 95, 68, 1), (160, 160, 96, 1),\n",
        "          (160, 160, 96, 1), (160, 160, 96, 1)]\n",
        "\n",
        "# Number of repetitions and folds for repeated k-fold\n",
        "REPS = 5\n",
        "FOLDS = 10\n",
        "\n",
        "# Different classes on the dataset\n",
        "CLASSES = ['NOR', 'AD', 'MCI']\n",
        "\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "METRICS = ['accuracy']\n",
        "\n",
        "if kaggle:\n",
        "    INPUT_DATAPATH = '/kaggle/input/' if tpu is None else None\n",
        "    METADATA_PATH = '/kaggle/input/'\n",
        "else:\n",
        "    drive.mount('/content/drive') \n",
        "    INPUT_DATAPATH = '/content/drive/MyDrive/data/'\n",
        "    METADATA_PATH = '/content/drive/MyDrive/data/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETL7vqE37UGX"
      },
      "source": [
        "# Training/testing with PET images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8bxgXUW7UGY"
      },
      "source": [
        "## Spatially normalized PET - elastic transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BFCXfQ9Z7UGY",
        "cellView": "code"
      },
      "source": [
        "# # ========================  NOT USED =========================\n",
        "# # Training model from images directly. Note: USE ONLY WITH GPU\n",
        "\n",
        "# DS = IMG_DATASETS[0] # Select dataset\n",
        "# IMG_SHAPE = SHAPES[0]\n",
        "# DS_PATH =  INPUT_DATAPATH + DS\n",
        "\n",
        "# # Path to images\n",
        "# pet_paths = np.empty((0,), dtype=str)\n",
        "# pet_labels = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "# for label, c in enumerate(CLASSES):\n",
        "#     pattern = os.path.join(DS_PATH, c, 'PET') + '/*.nii'\n",
        "#     pet_paths = np.concatenate((pet_paths, np.array(tf.io.gfile.glob(pattern))))\n",
        "#     pet_labels = np.concatenate((pet_labels, np.full(len(pet_paths) - len(pet_labels), label, dtype=np.int64)))\n",
        "    \n",
        "# X_train, X_test, y_train, y_test = train_test_split(pet_paths, pet_labels, \n",
        "#                                                     stratify = pet_labels,\n",
        "#                                                     test_size = 0.2,\n",
        "#                                                     shuffle=True,\n",
        "#                                                     random_state=SEED)\n",
        "\n",
        "# LR = 0.00001\n",
        "# BATCH_SIZE = 4 \n",
        "# EPOCHS = 50\n",
        "\n",
        "# reps_results = repeated_kfold(build_model_2, X_train, FOLDS, BATCH_SIZE, EPOCHS, reps=REPS, train_labels=y_train,\n",
        "#                random_state=SEED, use_tfrec=False)\n",
        "\n",
        "# reps_avgd_per_kfold = [avg_results_per_epoch(history) for history in reps_results]\n",
        "# reps_avg = avg_results_per_epoch(reps_avgd_per_kfold)\n",
        "\n",
        "# # Plot final result over epochs\n",
        "# plot_epochs_history(EPOCHS, reps_avg)\n",
        "\n",
        "# print('-'*80)\n",
        "# print('Results per repetition (on last epoch)')\n",
        "# for i in range(REPS):\n",
        "#     print('-'*80)\n",
        "#     print(f\"> Repetition {i + 1} - Loss: {reps_avgd_per_kfold[i]['val_loss'][-1]} - Accuracy : {reps_avgd_per_kfold[i]['val_accuracy'][-1]}\")\n",
        "\n",
        "# print('-'*80)\n",
        "# print('Average results over repetitions (on last epoch):')\n",
        "# print(f\"> Train Accuracy: {reps_avg['accuracy'][-1]}\")\n",
        "# print(f\"> Train Loss: {reps_avg['loss'][-1]}\")\n",
        "# print(f\"> CV accuracy: {reps_avg['val_accuracy'][-1]}\")\n",
        "# print(f\"> CV Loss: {reps_avg['val_loss'][-1]}\")\n",
        "# print('-'*80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW3Jz_Qc7UGY"
      },
      "source": [
        "In this case we have too little data and we need to reduce the variance of models evaluation, so we're going to use TFRecords \"wrong\" so that we can implement repeated-k-fold cross-validation (we have only one tfrecord per image, so we are not getting a good performance from TPU).\n",
        "\n",
        "When we have more data, each tfrecord will have more than one example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T07:25:39.973317Z",
          "iopub.execute_input": "2021-07-14T07:25:39.973719Z",
          "iopub.status.idle": "2021-07-14T07:25:39.979939Z",
          "shell.execute_reply.started": "2021-07-14T07:25:39.973675Z",
          "shell.execute_reply": "2021-07-14T07:25:39.978746Z"
        },
        "trusted": true,
        "id": "Pyc2CA5h7UGY"
      },
      "source": [
        "def select_dataset(ds_id):\n",
        "    global DS, IMG_SHAPE, DS_PATH, INPUT_DATAPATH\n",
        "    DS = TFREC_DATASETS[ds_id]\n",
        "    IMG_SHAPE = SHAPES[ds_id]\n",
        "    if INPUT_DATAPATH == None:\n",
        "        user_secrets = UserSecretsClient()\n",
        "        user_credential = user_secrets.get_gcloud_credential()\n",
        "        user_secrets.set_tensorflow_credential(user_credential)\n",
        "        DS_PATH = KaggleDatasets().get_gcs_path(DS)\n",
        "    else:\n",
        "        DS_PATH = INPUT_DATAPATH + DS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-07-14T07:25:45.042644Z",
          "iopub.execute_input": "2021-07-14T07:25:45.043014Z",
          "iopub.status.idle": "2021-07-14T07:25:45.910853Z",
          "shell.execute_reply.started": "2021-07-14T07:25:45.042979Z",
          "shell.execute_reply": "2021-07-14T07:25:45.909712Z"
        },
        "trusted": true,
        "id": "-YUhfz0R7UGZ"
      },
      "source": [
        "# Train from tfrecords: can be used with GPU or TPU\n",
        "\n",
        "# =============================================================================\n",
        "# \"Load\" data (get paths to train and test data)\n",
        "# =============================================================================\n",
        "\n",
        "select_dataset(0) # Not intensity normalized\n",
        "\n",
        "metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "\n",
        "X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "# # =============================================================================\n",
        "# # Train and test model 2 (and save results)\n",
        "# # =============================================================================\n",
        "# # Select hyperparameters\n",
        "# LR = 0.00001 #, 0.00008\n",
        "# BATCH_SIZE = 4 #, 64\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_2, 'pet-spatialnorm-elastic_model-2_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-elastic_model-2_results.txt')\n",
        "# model = train_model(build_model_2)\n",
        "# # model = tf.keras.models.load_model('pet-spatialnorm-elastic_model-2.h5') # Not used\n",
        "# show_test_results(model)\n",
        "# model.save('pet-spatialnorm-elastic_model-2.h5')\n",
        "\n",
        "# # =============================================================================\n",
        "# # Train and test model 0 (and save results)\n",
        "# # =============================================================================\n",
        "# LR = 0.00001\n",
        "# BATCH_SIZE = 4\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_0, 'pet-spatialnorm-elastic_model-0_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-elastic_model-0_results.txt')\n",
        "# model = train_model(build_model_0)\n",
        "# show_test_results(model)\n",
        "# model.save('pet-spatialnorm-elastic_model-0.h5')\n",
        "\n",
        "# # =============================================================================\n",
        "# # Train and test model 1 (and save results)\n",
        "# # =============================================================================\n",
        "# LR = 0.00001\n",
        "# BATCH_SIZE = 4\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_1, 'pet-spatialnorm-elastic_model-1_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-elastic_model-1_results.txt')\n",
        "# model = train_model(build_model_1)\n",
        "# show_test_results(model)\n",
        "# model.save('pet-spatialnorm-elastic_model-1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T07:26:00.927831Z",
          "iopub.execute_input": "2021-07-14T07:26:00.928204Z"
        },
        "trusted": true,
        "id": "URKSoiOe7UGZ"
      },
      "source": [
        "select_dataset(1) # Max-intensity normalized\n",
        "\n",
        "metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "\n",
        "X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "# # =============================================================================\n",
        "# # Train and test model 2 (and save results)\n",
        "# # =============================================================================\n",
        "# # Select hyperparameters\n",
        "# LR = 0.00001 #, 0.00008\n",
        "# BATCH_SIZE = 4 #, 64\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_2, 'pet-spatialnorm-elastic-maxintensitynorm_model-2_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-elastic-maxintensitynorm_model-2_results.txt')\n",
        "# model = train_model(build_model_2)\n",
        "# # model = tf.keras.models.load_model('pet-spatialnorm-elastic-maxintensitynorm_model-2.h5') # Not used\n",
        "# show_test_results(model)\n",
        "# model.save('pet-spatialnorm-elastic-maxintensitynorm_model-2.h5')\n",
        "\n",
        "# # =============================================================================\n",
        "# # Train and test model 0 (and save results)\n",
        "# # =============================================================================\n",
        "# LR = 0.00001\n",
        "# BATCH_SIZE = 4\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_0, 'pet-spatialnorm-elastic-maxintensitynorm_model-0_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-elastic-maxintensitynorm_model-0_results.txt')\n",
        "# model = train_model(build_model_0)\n",
        "# show_test_results(model)\n",
        "# model.save('pet-spatialnorm-elastic-maxintensitynorm_model-0.h5')\n",
        "\n",
        "# # =============================================================================\n",
        "# # Train and test model 1 (and save results)\n",
        "# # =============================================================================\n",
        "# LR = 0.00001\n",
        "# BATCH_SIZE = 4\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_1, 'pet-spatialnorm-elastic-maxintensitynorm_model-1_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-elastic-maxintensitynorm_model-1_results.txt')\n",
        "# model = train_model(build_model_1)\n",
        "# show_test_results(model)\n",
        "# model.save('pet-spatialnorm-elastic-maxintensitynorm_model-1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuhOejstC9pr"
      },
      "source": [
        "select_dataset(2) # Standarized\n",
        "\n",
        "metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "\n",
        "X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "# # =============================================================================\n",
        "# # Train and test model 2 (and save results)\n",
        "# # =============================================================================\n",
        "# Select hyperparameters\n",
        "# LR = 0.00001 #, 0.00008\n",
        "# BATCH_SIZE = 4 #, 64\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_2, 'pet-spatialnorm-elastic-standarized_model-2_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-elastic-standarized_model-2_results.txt')\n",
        "# model = train_model(build_model_2)\n",
        "# # model = tf.keras.models.load_model('pet-spatialnorm-elastic-standarized_model-2.h5') # Not used\n",
        "# show_test_results(model)\n",
        "# model.save('pet-spatialnorm-elastic-standarized_model-2.h5')\n",
        "\n",
        "# # =============================================================================\n",
        "# # Train and test model 0 (and save results)\n",
        "# # =============================================================================\n",
        "# LR = 0.00001\n",
        "# BATCH_SIZE = 4\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_0, 'pet-spatialnorm-elastic-standarized_model-0_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-elastic-standarized_model-0_results.txt')\n",
        "# model = train_model(build_model_0)\n",
        "# show_test_results(model)\n",
        "# model.save('pet-spatialnorm-elastic-standarized_model-0.h5')\n",
        "\n",
        "# # =============================================================================\n",
        "# # Train and test model 1 (and save results)\n",
        "# # =============================================================================\n",
        "# LR = 0.00001\n",
        "# BATCH_SIZE = 4\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_1, 'pet-spatialnorm-elastic-standarized_model-1_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-elastic-standarized_model-1_results.txt')\n",
        "# model = train_model(build_model_1)\n",
        "# show_test_results(model)\n",
        "# model.save('pet-spatialnorm-elastic-standarized_model-1.h5')\n",
        "\n",
        "\n",
        "# # ====================== Deeper model ======================\n",
        "\n",
        "# LR = 0.00001\n",
        "# BATCH_SIZE = 4\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_3, 'pet-spatialnorm-elastic-standarized_model-3_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-elastic-standarized_model-3_results.txt')\n",
        "# # model = train_model(build_model_3)\n",
        "# # show_test_results(model)\n",
        "# # model.save('pet-spatialnorm-elastic-standarized_model-3.h5')\n",
        "\n",
        "\n",
        "# ====================== Deeper model ======================\n",
        "\n",
        "LR = 0.00001\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "\n",
        "test_model_rkfold(build_model_4, 'pet-spatialnorm-elastic-standarized_model-4_results.txt')\n",
        "show_rkfold_results('pet-spatialnorm-elastic-standarized_model-4_results.txt')\n",
        "model_4 = train_model(build_model_4)\n",
        "\n",
        "y_pred = get_predictions(model_4, X_test)\n",
        "results = calculate_results(y_test, y_pred)\n",
        "save_dict(results, 'pet-model_4-results.txt')\n",
        "present_results(results, {0:'NOR', 1:'AD', 2:'MCI'})\n",
        "model.save('pet-spatialnorm-elastic-standarized_model-4.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUji7RQAqL0s"
      },
      "source": [
        "if tf.io.gfile.exists('Alzheimer-disease-classification'):\n",
        "    shutil.rmtree('Alzheimer-disease-classification')\n",
        "! git clone https://github.com/Angelvj/Alzheimer-disease-classification.git\n",
        "\n",
        "results_path = 'Alzheimer-disease-classification/test_results/'\n",
        "\n",
        "\n",
        "results = 0\n",
        "results = load_dict(results_path + 'ensemble-results.txt')\n",
        "present_results(results, {0:'NOR', 1:'AD', 2:'MCI'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F6WrwK1ohIF"
      },
      "source": [
        "# select_dataset(4)\n",
        "\n",
        "# metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "# metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "\n",
        "# X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "# y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "# X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "# y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "# LR = 0.00001\n",
        "# BATCH_SIZE = 4\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_model_4, 'pet-spatialnorm-rigid-standarized_model-4_results.txt')\n",
        "# show_rkfold_results('pet-spatialnorm-rigid-standarized_model-4_results.txt')\n",
        "# # model = train_model(build_model_3)\n",
        "# # show_test_results(model)\n",
        "# # model.save('pet-spatialnorm-elastic-standarized_model-3.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK5SIrscVHqk"
      },
      "source": [
        "# K-fold results visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3qLSCTuVO5k"
      },
      "source": [
        "if tf.io.gfile.exists('Alzheimer-disease-classification'):\n",
        "    shutil.rmtree('Alzheimer-disease-classification')\n",
        "! git clone https://github.com/Angelvj/Alzheimer-disease-classification.git\n",
        "\n",
        "results_path = 'Alzheimer-disease-classification/kfold_results/'\n",
        "\n",
        "EPOCHS = 50\n",
        "REPS = 5\n",
        "\n",
        "# # Model 0 results\n",
        "# print(' ===================== Results for model 0 =====================')\n",
        "# print(' - Spatially normalized PET images')\n",
        "# show_rkfold_results(results_path + 'pet-spatialnorm-elastic_model-0_results.txt')\n",
        "# print(' - Spatially normalized and max intensity normalized PET images')\n",
        "# show_rkfold_results(results_path + 'pet-spatialnorm-elastic-maxintensitynorm_model-0_results.txt')\n",
        "# print(' - Spatially normalized and standarized PET images')\n",
        "# show_rkfold_results(results_path + 'pet-spatialnorm-elastic-standarized_model-0_results.txt')\n",
        "\n",
        "# # Model 1 results\n",
        "# print(' ===================== Results for model 1 =====================')\n",
        "# print(' - Spatially normalized PET images')\n",
        "# show_rkfold_results(results_path + 'pet-spatialnorm-elastic_model-1_results.txt')\n",
        "# print(' - Spatially normalized and max intensity normalized PET images')\n",
        "# show_rkfold_results(results_path + 'pet-spatialnorm-elastic-maxintensitynorm_model-1_results.txt')\n",
        "# print(' - Spatially normalized and standarized PET images')\n",
        "# show_rkfold_results(results_path + 'pet-spatialnorm-elastic-standarized_model-1_results.txt')\n",
        "\n",
        "# # Model 2 results\n",
        "# print(' ===================== Results for model 2 =====================')\n",
        "# print(' - Spatially normalized PET images')\n",
        "# show_rkfold_results(results_path + 'pet-spatialnorm-elastic_model-2_results.txt')\n",
        "# print(' - Spatially normalized and max intensity normalized PET images')\n",
        "# show_rkfold_results(results_path + 'pet-spatialnorm-elastic-maxintensitynorm_model-2_results.txt')\n",
        "# print(' - Spatially normalized and standarized PET images')\n",
        "# show_rkfold_results(results_path + 'pet-spatialnorm-elastic-standarized_model-2_results.txt')\n",
        "\n",
        "\n",
        "# # Model 3 results\n",
        "# print(' ===================== Results for model 3 =====================')\n",
        "# print(' - Spatially normalized and standarized PET images')\n",
        "# show_rkfold_results(results_path + 'pet-spatialnorm-elastic-standarized_model-3_results.txt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}