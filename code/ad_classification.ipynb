{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "ad-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/ad_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--MHZhKpcC1b"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-14T16:50:54.169182Z",
          "iopub.execute_input": "2021-06-14T16:50:54.169713Z",
          "iopub.status.idle": "2021-06-14T16:50:59.441110Z",
          "shell.execute_reply.started": "2021-06-14T16:50:54.169607Z",
          "shell.execute_reply": "2021-06-14T16:50:59.440289Z"
        },
        "trusted": true,
        "id": "tdFpcAtpcC1e"
      },
      "source": [
        "import os, shutil\n",
        "import pandas as pd, numpy as np, random\n",
        "import nibabel as nib\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n2OzIaJcC1f"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-14T16:51:01.561176Z",
          "iopub.execute_input": "2021-06-14T16:51:01.561504Z",
          "iopub.status.idle": "2021-06-14T16:51:01.574496Z",
          "shell.execute_reply.started": "2021-06-14T16:51:01.561474Z",
          "shell.execute_reply": "2021-06-14T16:51:01.573596Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osHOtbhHcC1g",
        "outputId": "90d5231c-6c1e-443e-a162-d7cdf5627af0"
      },
      "source": [
        "DEVICE = 'TPU'\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU, setting default strategy')\n",
        "        tpu = None\n",
        "        STRATEGY = tf.distribute.get_strategy()\n",
        "elif DEVICE == 'GPU':\n",
        "    tpu = None\n",
        "    STRATEGY = tf.distribute.MirroredStrategy()\n",
        "    \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = STRATEGY.num_replicas_in_sync\n",
        "\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not connect to TPU, setting default strategy\n",
            "Number of accelerators: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq3H3R7rcC1h"
      },
      "source": [
        "# Functions for loading/creating dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-14T16:51:06.757087Z",
          "iopub.execute_input": "2021-06-14T16:51:06.757432Z",
          "iopub.status.idle": "2021-06-14T16:51:06.766179Z",
          "shell.execute_reply.started": "2021-06-14T16:51:06.757398Z",
          "shell.execute_reply": "2021-06-14T16:51:06.765141Z"
        },
        "trusted": true,
        "id": "XGBF_VhzcC1i"
      },
      "source": [
        "def load_image(path):    \n",
        "\n",
        "    img = nib.load(path)\n",
        "    img = np.asarray(img.dataobj, dtype=np.float32)\n",
        "    img = np.expand_dims(img, axis=3) # Add dummy axis for channel\n",
        "    return img\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    tfrec_format = {\n",
        "        \"image\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"one_hot_label\": tf.io.VarLenFeature(tf.float32)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, tfrec_format)\n",
        "    one_hot_label = tf.sparse.to_dense(example['one_hot_label'])\n",
        "    one_hot_label = tf.reshape(one_hot_label, [NUM_CLASSES])\n",
        "    image = tf.reshape(tf.sparse.to_dense(example['image']), IMG_SHAPE)\n",
        "    # TPU needs size to be known, so this doesn't work\n",
        "    #     image  = tf.reshape(example['image'], example['shape']) \n",
        "    return image, one_hot_label\n",
        "\n",
        "\n",
        "def parse_file(filename, label):\n",
        "\n",
        "    image = load_image(filename)\n",
        "    image = np.nan_to_num(image, copy=False)\n",
        "    label = np.eye(3, dtype=np.float32)[label]\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def generator_fn(filenames, labels):\n",
        "\n",
        "    def images_generator():\n",
        "\n",
        "        for X, y in zip(filenames, labels):\n",
        "            X, y = parse_file(X, y)\n",
        "            yield X, y\n",
        "    return images_generator\n",
        "\n",
        "def load_dataset(filenames, labels, use_tfrec):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        # Allow order-altering optimizations\n",
        "        option_no_order = tf.data.Options()\n",
        "        option_no_order.experimental_deterministic = False\n",
        "        \n",
        "        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "        dataset = dataset.with_options(option_no_order)\n",
        "        dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO)\n",
        "\n",
        "    else:\n",
        "        dataset = tf.data.Dataset.from_generator(generator_fn(filenames, labels),\n",
        "            output_signature=(\n",
        "                 tf.TensorSpec(shape=IMG_SHAPE, dtype=tf.float32),\n",
        "                 tf.TensorSpec(shape=(3,), dtype=tf.float32)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def get_dataset(filenames, labels=None, use_tfrec=True, batch_size = 8, train=False, augment=False, cache=False):\n",
        "    dataset =  load_dataset(filenames, labels, use_tfrec=use_tfrec)\n",
        "    if cache:\n",
        "        dataset = dataset.cache() # Do it only if dataset fits in ram\n",
        "    if train:\n",
        "        dataset = dataset.repeat()\n",
        "        if augment:\n",
        "            raise NotImplementedError\n",
        "#             dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "        dataset = dataset.shuffle(count_data_items(filenames, use_tfrec))\n",
        "\n",
        "    dataset = dataset.batch(batch_size * REPLICAS)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames, use_tfrec):\n",
        "    if use_tfrec:\n",
        "        n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
        "            for filename in filenames]\n",
        "        return np.sum(n)\n",
        "    else:\n",
        "        return len(filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWzZXlTqcC1j"
      },
      "source": [
        "# Train schedule\n",
        "\n",
        "**Note**: not used yet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NLTU5iHTcC1j"
      },
      "source": [
        "def get_lr_callback(batch_size=1, verbose=False):\n",
        "    lr_start = 0.00001\n",
        "#     lr_max = 0.00004 * REPLICAS\n",
        "    lr_max = 0.00004 * REPLICAS * batch_size\n",
        "    lr_min = 0.00001\n",
        "    lr_rampup_epochs = 3\n",
        "    lr_sustain_epochs = 0\n",
        "    lr_exp_decay = 0.7\n",
        "\n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_rampup_epochs:\n",
        "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
        "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
        "            lr = lr_max\n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
        "        return lr\n",
        "    \n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = verbose)\n",
        "    return lr_callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PocO0DJ1cC1k"
      },
      "source": [
        "# Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-14T16:51:12.615323Z",
          "iopub.execute_input": "2021-06-14T16:51:12.615660Z",
          "iopub.status.idle": "2021-06-14T16:51:12.623500Z",
          "shell.execute_reply.started": "2021-06-14T16:51:12.615630Z",
          "shell.execute_reply": "2021-06-14T16:51:12.622414Z"
        },
        "trusted": true,
        "id": "tnpR-yKhcC1k"
      },
      "source": [
        "def build_model(input_shape):\n",
        "\n",
        "    inputs = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "    x = tf.keras.layers.Conv3D(filters=16, kernel_size=5, activation=\"relu\")(inputs)\n",
        "    x = tf.keras.layers.MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv3D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv3D(filters=128, kernel_size=5, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.MaxPooling3D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(units=256, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=3, activation=\"softmax\", )(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"model_2\")\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENLI3jgGv9wr"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orUIymZ3wB0y"
      },
      "source": [
        "def plot_epochs_history(num_epochs, history):\n",
        "    # TODO: make a more general function\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(np.arange(num_epochs), history['accuracy'], '-o', label='Train acc',\n",
        "            color = '#ff7f0e')\n",
        "    plt.plot(np.arange(num_epochs), history['val_accuracy'], '-o', label='Val acc',\n",
        "            color = '#1f77b4')\n",
        "    x = np.argmax(history['val_accuracy']); y = np.max(history['val_accuracy'])\n",
        "    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x, y, s=200, color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max acc\\n%.2f'%y,size=14)\n",
        "    plt.ylabel('ACC', size=14); plt.xlabel('Epoch', size=14)\n",
        "    plt.legend(loc=2)\n",
        "    plt2 = plt.gca().twinx()\n",
        "    plt2.plot(np.arange(num_epochs),history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
        "    plt2.plot(np.arange(num_epochs),history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
        "    x = np.argmin(history['val_loss'] ); y = np.min(history['val_loss'])\n",
        "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
        "    plt.ylabel('Loss',size=14)\n",
        "    plt.legend(loc=3)\n",
        "    plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG_aGjWHcC1m"
      },
      "source": [
        "# Models evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA-6d55F12k3"
      },
      "source": [
        "def evaluate_model_kfold(model_builder, train_filenames, metrics, n_folds, batch_size, epochs, \n",
        "                         plot_fold_results = True, plot_avg_results = True, train_labels=None, \n",
        "                         stratify=False, shuffle=True, random_state=None, use_tfrec=True):\n",
        "    \n",
        "    # np_rs = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(random_state)))\n",
        "    folds_history = {metric: [] for metric in metrics}\n",
        "\n",
        "    if stratify:\n",
        "        skf = StratifiedKFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "    else:\n",
        "        skf = KFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "\n",
        "    for fold, (idx_train, idx_val) in enumerate(skf.split(train_filenames, train_labels)):\n",
        "        if tpu != None:\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "        # np_rs.shuffle(idx_train)\n",
        "        X_train = train_filenames[idx_train]\n",
        "        X_val = train_filenames[idx_val]\n",
        "        y_train = None if train_labels is None else train_labels[idx_train]\n",
        "        y_val = None if train_labels is None else train_labels[idx_val]\n",
        "\n",
        "        # Build model\n",
        "        K.clear_session()\n",
        "        with STRATEGY.scope():\n",
        "            model = model_builder(input_shape=IMG_SHAPE)\n",
        "            # Optimizers and Losses create TF variables --> should always be initialized in the scope\n",
        "            OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "            LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "            model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
        "\n",
        "        # Train\n",
        "        print(f'Training for fold {fold + 1} of {n_folds}...')\n",
        "\n",
        "        history = model.fit(\n",
        "            get_dataset(X_train, y_train,  use_tfrec, train=True, augment=False, batch_size=batch_size), \n",
        "            epochs = EPOCHS, \n",
        "            steps_per_epoch = int(np.rint(count_data_items(X_train, use_tfrec)/batch_size/REPLICAS)),\n",
        "            validation_data = get_dataset(X_val, y_val, use_tfrec, batch_size = batch_size, train=False), \n",
        "            validation_steps= int(np.rint(count_data_items(X_val,use_tfrec)/batch_size/REPLICAS)))\n",
        "    \n",
        "        for m in metrics:\n",
        "            folds_history[m].append(history.history[m])\n",
        "\n",
        "        if plot_fold_results:\n",
        "            # TODO: make more general\n",
        "            plot_epochs_history(epochs, history.history)\n",
        "\n",
        "    avg_history = dict()\n",
        "    for m in metrics:\n",
        "        avg_history[m] = [np.mean([x[i] for x in folds_history[m]]) for i in range(epochs)]\n",
        "\n",
        "    if plot_avg_results:\n",
        "        # TODO: make also more general\n",
        "        plot_epochs_history(epochs, avg_history)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Results per fold')\n",
        "        for i in range(n_folds):\n",
        "            print('-'*80)\n",
        "            print(f\"> Fold {i + 1} - Loss: {folds_history['val_loss'][i][-1]} - Accuracy : {folds_history['val_accuracy'][i][-1]}\")\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Average results over folds:')\n",
        "        print(f\"> Train Accuracy: {avg_history['accuracy'][-1]}\")\n",
        "        print(f\"> Train Loss: {avg_history['loss'][-1]}\")\n",
        "        print(f\"> CV accuracy: {avg_history['val_accuracy'][-1]}\")\n",
        "        print(f\"> CV Loss: {avg_history['val_loss'][-1]}\")\n",
        "        print('-'*80)\n",
        "\n",
        "    return folds_history\n",
        "\n",
        "def repeated_kfold(model_builder, train_filenames, metrics, n_folds, batch_size, epochs, reps=5, train_labels=None,\n",
        "                   stratify=True, shuffle=True, random_state=None, use_tfrec=True):\n",
        "    \n",
        "    reps_history = {metric: [] for metric in metrics}\n",
        "    \n",
        "    for i in range(reps):\n",
        "        print(f'Repetition {i + 1}')\n",
        "        folds_history = evaluate_model_kfold(model_builder, train_filenames, metrics, n_folds,\n",
        "                                             batch_size, epochs, train_labels=train_labels, stratify=stratify,\n",
        "                                             shuffle=shuffle, random_state=random_state, use_tfrec=use_tfrec)\n",
        "        random_state += 1\n",
        "        avg_folds_history = dict()\n",
        "        for m in metrics:\n",
        "            avg_folds_history[m] = [np.mean([x[i] for x in folds_history[m]]) for i in range(epochs)]\n",
        "        for m in metrics:\n",
        "            reps_history[m].append(avg_folds_history[m])\n",
        "\n",
        "    return reps_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-14T16:51:19.064528Z",
          "iopub.execute_input": "2021-06-14T16:51:19.064958Z",
          "iopub.status.idle": "2021-06-14T16:51:22.124434Z",
          "shell.execute_reply.started": "2021-06-14T16:51:19.064916Z",
          "shell.execute_reply": "2021-06-14T16:51:22.123585Z"
        },
        "trusted": true,
        "id": "4ez6XjMFcC1l"
      },
      "source": [
        "SEED = 268 # Arbitrary seed\n",
        "DATASETS = ['tfrec-PET-spatialnorm-elastic',\n",
        "            'tfrec-PET-spatialnorm-elastic-maxintensitynorm',\n",
        "            'tfrec-PET-spatialnorm-elastic-standarized',\n",
        "            'tfrec-PET-spatialnorm-elastic-maxintensitynorm-standarized']\n",
        "SHAPES = [(79, 95, 68, 1), (79, 95, 68, 1), (79, 95, 68, 1), (79, 95, 68, 1)]\n",
        "DS_NUM_CLASSES = [3, 3, 3, 3]\n",
        "REPS = 2\n",
        "FOLDS = 10\n",
        "CLASSES = ['NOR', 'AD', 'MCI']\n",
        "METRICS = ['accuracy']\n",
        "CV_METRICS = ['accuracy', 'val_accuracy', 'loss', 'val_loss'] # TODO: maybe I could use the model metrics directly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-06-14T16:51:22.186919Z",
          "iopub.execute_input": "2021-06-14T16:51:22.187196Z",
          "iopub.status.idle": "2021-06-14T16:51:23.375294Z",
          "shell.execute_reply.started": "2021-06-14T16:51:22.187162Z",
          "shell.execute_reply": "2021-06-14T16:51:23.374510Z"
        },
        "trusted": true,
        "id": "DyeAfxUhcC1l"
      },
      "source": [
        "# Training model from images directly (only for testing some stuff)\n",
        "DATASET = 'ad-preprocessed'\n",
        "IMG_SHAPE = SHAPES[0]\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/drive/My Drive/data/'\n",
        "\n",
        "DS = 'ad-preprocessed'\n",
        "DS_PATH =  DATA_PATH + DS\n",
        "\n",
        "# Path to images\n",
        "pet_paths = np.empty((0,), dtype=str)\n",
        "pet_labels = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "for label, c in enumerate(CLASSES):\n",
        "    pattern = os.path.join(DS_PATH, c, 'PET') + '/*.nii'\n",
        "    pet_paths = np.concatenate((pet_paths, np.array(tf.io.gfile.glob(pattern))))\n",
        "    pet_labels = np.concatenate((pet_labels, np.full(len(pet_paths) - len(pet_labels), label, dtype=np.int64)))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(pet_paths, pet_labels, \n",
        "                                                    stratify = pet_labels,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    shuffle=True,\n",
        "                                                    random_state=SEED)\n",
        "\n",
        "LR = 0.00001 #0.00008\n",
        "BATCH_SIZE = 4 #64\n",
        "\n",
        "OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.0)\n",
        "EPOCHS = 50\n",
        "\n",
        "# evaluate_model_kfold(build_model, X_train, CV_METRICS, FOLDS, BATCH_SIZE, EPOCHS, train_labels=y_train,\n",
        "#                      stratify=True, random_state=SEED, use_tfrec=False)\n",
        "reps_results = repeated_kfold(build_model, X_train, CV_METRICS, FOLDS, BATCH_SIZE, EPOCHS, reps=REPS, train_labels=y_train,\n",
        "               random_state=SEED, use_tfrec=False)\n",
        "\n",
        "# Average results in the different k-fold executions\n",
        "reps_avg = dict()\n",
        "for m in CV_METRICS:\n",
        "    reps_avg[m] = [np.mean([x[i] for x in reps_results[m]]) for i in range(EPOCHS)]\n",
        "\n",
        "# Plot results over epochs\n",
        "plot_epochs_history(EPOCHS, reps_avg)\n",
        "\n",
        "print('-'*80)\n",
        "print('Results per repetition')\n",
        "for i in range(REPS):\n",
        "    print('-'*80)\n",
        "    print(f\"> Repetition {i + 1} - Loss: {reps_results['val_loss'][i][-1]} - Accuracy : {reps_results['val_accuracy'][i][-1]}\")\n",
        "\n",
        "print('-'*80)\n",
        "print('Average results over repetitions (on last epoch):')\n",
        "print(f\"> Train Accuracy: {reps_avg['accuracy'][-1]}\")\n",
        "print(f\"> Train Loss: {reps_avg['loss'][-1]}\")\n",
        "print(f\"> CV accuracy: {reps_avg['val_accuracy'][-1]}\")\n",
        "print(f\"> CV Loss: {reps_avg['val_loss'][-1]}\")\n",
        "print('-'*80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAigMjWOfpyB",
        "outputId": "4074a7af-6378-46ef-d3b6-1f6aabeeb493"
      },
      "source": [
        "# Training model form tfrecords\n",
        "DATASET = DATASETS[0]\n",
        "IMG_SHAPE = SHAPES[0]\n",
        "NUM_CLASSES = DS_NUM_CLASSES[0]\n",
        "\n",
        "# Access to data on google Drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/drive/My Drive/data/'\n",
        "DS_PATH = DATA_PATH + DATASET\n",
        "\n",
        "# Read data filenames\n",
        "metadata_train = pd.read_csv(DS_PATH + '/train/train_summary.csv', encoding='utf-8')\n",
        "X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "X_test = np.array(tf.io.gfile.glob(DS_PATH + '/test/*.tfrec'))\n",
        "\n",
        "# Select hyperparameters (some of them)\n",
        "LR = 0.00001 # 0.00008\n",
        "BATCH_SIZE = 4 # 64\n",
        "EPOCHS = 50\n",
        "\n",
        "# # Evaluate model with repeated k-fold (because of the high variance)\n",
        "# # evaluate_model_kfold(build_model, X_train, CV_METRICS, FOLDS, BATCH_SIZE, EPOCHS, train_labels=y_train,\n",
        "# #                      stratify=True, random_state=SEED, use_tfrec=True)\n",
        "# reps_results = repeated_kfold(build_model, X_train, CV_METRICS, FOLDS, BATCH_SIZE, EPOCHS, reps=REPS, train_labels=y_train,\n",
        "#                random_state=SEED)\n",
        "\n",
        "# # Average results in the different k-fold executions\n",
        "# reps_avg = dict()\n",
        "# for m in CV_METRICS:\n",
        "#     reps_avg[m] = [np.mean([x[i] for x in reps_results[m]]) for i in range(EPOCHS)]\n",
        "\n",
        "# # Plot results over epochs\n",
        "# plot_epochs_history(EPOCHS, reps_avg)\n",
        "\n",
        "# print('-'*80)\n",
        "# print('Results per repetition')\n",
        "# for i in range(REPS):\n",
        "#     print('-'*80)\n",
        "#     print(f\"> Repetition {i + 1} - Loss: {reps_results['val_loss'][i][-1]} - Accuracy : {reps_results['val_accuracy'][i][-1]}\")\n",
        "\n",
        "# print('-'*80)\n",
        "# print('Average results over repetitions (on last epoch):')\n",
        "# print(f\"> Train Accuracy: {reps_avg['accuracy'][-1]}\")\n",
        "# print(f\"> Train Loss: {reps_avg['loss'][-1]}\")\n",
        "# print(f\"> CV accuracy: {reps_avg['val_accuracy'][-1]}\")\n",
        "# print(f\"> CV Loss: {reps_avg['val_loss'][-1]}\")\n",
        "# print('-'*80)\n",
        "\n",
        "# # Test model\n",
        "# with strategy.scope():\n",
        "#     OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "#     LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "#     model = build_model(IMG_SHAPE)\n",
        "#     model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
        "\n",
        "# history = model.fit(\n",
        "#     get_dataset(X_train, train=True, batch_size=BATCH_SIZE), \n",
        "#     epochs = EPOCHS, \n",
        "#     steps_per_epoch = int(np.rint(count_data_items(X_train, use_tfrec=True)/BATCH_SIZE/REPLICAS))\n",
        "#     )\n",
        "\n",
        "# results = model.evaluate(\n",
        "#     get_dataset(X_test)\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}