{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFau7tCN50T65Nt9ErwQd7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/experiment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52ti41AT1QNV"
      },
      "source": [
        "kaggle = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkbeH8YHgC7X"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk40fZH21oKG"
      },
      "source": [
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from tensorflow import keras\n",
        "import os, shutil, re\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "if kaggle:\n",
        "    from kaggle_datasets import KaggleDatasets\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "else:\n",
        "    from google.colab import drive\n",
        "import nibabel as nib\n",
        "\n",
        "# Import the most used layers\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Input, BatchNormalization, Dropout"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy0bXUdWgOW8",
        "outputId": "9cf90329-2279-4308-b77c-9753e4537975"
      },
      "source": [
        "import shutil\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "if tf.io.gfile.exists('Alzheimer-disease-classification'):\n",
        "    shutil.rmtree('Alzheimer-disease-classification')\n",
        "! git clone https://github.com/Angelvj/Alzheimer-disease-classification.git\n",
        "\n",
        "if not kaggle:\n",
        "    sys.path.insert(0,'/content/Alzheimer-disease-classification/code')\n",
        "else:\n",
        "    sys.path.insert(0, './Alzheimer-disease-classification/code')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Alzheimer-disease-classification'...\n",
            "remote: Enumerating objects: 1010, done.\u001b[K\n",
            "remote: Counting objects: 100% (259/259), done.\u001b[K\n",
            "remote: Compressing objects: 100% (207/207), done.\u001b[K\n",
            "remote: Total 1010 (delta 150), reused 137 (delta 52), pack-reused 751\u001b[K\n",
            "Receiving objects: 100% (1010/1010), 20.23 MiB | 22.18 MiB/s, done.\n",
            "Resolving deltas: 100% (508/508), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD05JVDtgS5N"
      },
      "source": [
        "import functions.lr_schedules as lr_schedules\n",
        "import functions.io_utils as io\n",
        "from functions.model_evaluation import repeated_kfold, plot_epochs_history, get_rkf_history\n",
        "from functions.tfrec_loading import count_data_items\n",
        "from functions.general import change_input_shape\n",
        "from models import feedforward_models_pet, ResNet"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgwkSm3AgF-2"
      },
      "source": [
        "# Hardware configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqk53tuI1us3",
        "outputId": "3692f7ae-a38f-4dcd-ba54-71ae3a5998ce"
      },
      "source": [
        "DEVICE = 'TPU' # or TPU\n",
        "tpu = None\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU, setting default strategy')\n",
        "        tpu = None\n",
        "        STRATEGY = tf.distribute.get_strategy()\n",
        "elif DEVICE == 'GPU':\n",
        "    STRATEGY = tf.distribute.MirroredStrategy()\n",
        "    \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = STRATEGY.num_replicas_in_sync\n",
        "\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not connect to TPU, setting default strategy\n",
            "Number of accelerators: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo66gGUaACMD",
        "outputId": "8960a7df-3ac3-4e15-a56f-ab4e3552328e"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGtYhpoff5Gc"
      },
      "source": [
        "# Create model with two inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWxAbvqIeIRN"
      },
      "source": [
        "# Model with two inputs\n",
        "def build_model():\n",
        "\n",
        "    # Base pet model\n",
        "    pet_model = feedforward_models_pet.model_4(input_shape=(79, 95, 68, 1))\n",
        "    # Delete classifier\n",
        "    pet_model = keras.Model(pet_model.input, pet_model.layers[-3].output)\n",
        "\n",
        "    # Input for pet images\n",
        "    pet_input = keras.Input(shape=(79, 95, 68, 1), name='pet_input')\n",
        "    pet_features = pet_model(pet_input)\n",
        "\n",
        "    # Base mri model\n",
        "    pretrained_resnet = keras.models.load_model('/content/drive/MyDrive/pretrained_models/pretrained_3D_resnet18.h5')\n",
        "    # Delete classifier\n",
        "    pretrained_resnet = keras.Model(pretrained_resnet.input, pretrained_resnet.layers[-2].output)\n",
        "    # Change input shape\n",
        "    pretrained_resnet = change_input_shape(pretrained_resnet, (121, 145, 121, 1), 'new_input')\n",
        "    pretrained_resnet.trainable = False\n",
        "    # Input for pet images\n",
        "    mri_input = keras.Input(shape=(121, 145, 121, 1), name='mri_input')\n",
        "    mri_features = pretrained_resnet(mri_input)\n",
        "\n",
        "    x = keras.layers.concatenate([pet_features, mri_features])\n",
        "    pred = keras.layers.Dense(3, name='label')(x)\n",
        "    model = keras.Model(inputs=[pet_input, mri_input], outputs = [pred])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFfJiHkf-i5"
      },
      "source": [
        "# Read tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od9zbOTFgVnx"
      },
      "source": [
        "# Now each tfrecord has two images and one label\n",
        "def read_tfrecord(example):\n",
        "    \n",
        "    tfrec_format = {\n",
        "        \"image_pet\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image_mri\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"one_hot_label\": tf.io.VarLenFeature(tf.float32)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, tfrec_format)\n",
        "    one_hot_label = tf.sparse.to_dense(example['one_hot_label'])\n",
        "    one_hot_label = tf.reshape(one_hot_label, [NUM_CLASSES])\n",
        "    image_pet = tf.reshape(tf.sparse.to_dense(example['image_pet']), PET_SHAPE)\n",
        "    image_mri = tf.reshape(tf.sparse.to_dense(example['image_mri']), MRI_SHAPE)\n",
        "\n",
        "    return {'pet_input':image_pet, 'mri_input':image_mri}, {'label':one_hot_label}\n",
        "\n",
        "def load_dataset(filenames, labels, no_order=True):\n",
        "    \n",
        "    # Allow order-altering optimizations\n",
        "    option_no_order = tf.data.Options()\n",
        "    option_no_order.experimental_deterministic = False\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    if no_order:\n",
        "        dataset = dataset.with_options(option_no_order)\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def get_dataset(filenames, labels=None, batch_size = 4, train=False, cache=True, no_order=True):\n",
        "    \n",
        "    dataset =  load_dataset(filenames, labels, no_order)\n",
        "    \n",
        "    if cache:\n",
        "        dataset = dataset.cache() # Do it only if dataset fits in ram\n",
        "    if train:\n",
        "        dataset = dataset.repeat()\n",
        "        dataset = dataset.shuffle(count_data_items(filenames))\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRCVE31ClL3Y"
      },
      "source": [
        "# Evaluation functions\n",
        "\n",
        "We have to redefine some functions because of the problems caused by using two\n",
        "inputs (I'll fix it in future)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-3jsaWLgvZE"
      },
      "source": [
        "# Training and evaluation\n",
        "def evaluate_model_kfold(model_builder, train_filenames, n_folds, batch_size, epochs, \n",
        "                         plot_fold_results = True, plot_avg_results = True, train_labels=None, \n",
        "                         stratify=False, shuffle=True, random_state=None, cbks=None):\n",
        "    \n",
        "    # np_rs = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(random_state)))\n",
        "    folds_histories = []\n",
        "\n",
        "    if stratify:\n",
        "        skf = StratifiedKFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "    else:\n",
        "        skf = KFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "\n",
        "    for fold, (idx_train, idx_val) in enumerate(skf.split(train_filenames, train_labels)):\n",
        "        if tpu != None:\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "        # np_rs.shuffle(idx_train)\n",
        "        X_train = train_filenames[idx_train]\n",
        "        X_val = train_filenames[idx_val]\n",
        "        y_train = train_labels[idx_train]\n",
        "        y_val = train_labels[idx_val]\n",
        "\n",
        "        # Build model\n",
        "        tf.keras.backend.clear_session()\n",
        "        with STRATEGY.scope():\n",
        "            model = model_builder()\n",
        "            # Optimizers and Losses create TF variables --> should always be initialized in the scope\n",
        "            OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "            LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "            model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS) #steps_per_execution=8)\n",
        "\n",
        "        # Train\n",
        "        print(f'Training for fold {fold + 1} of {n_folds}...')\n",
        "        history = model.fit(\n",
        "            get_dataset(X_train, y_train, train=True, batch_size=batch_size), \n",
        "            epochs = EPOCHS, callbacks = cbks,\n",
        "            steps_per_epoch = max(1, int(np.rint(count_data_items(X_train)/batch_size))),\n",
        "            validation_data = get_dataset(X_val, y_val, batch_size = batch_size, train=False) ,\n",
        "            validation_steps= max(1, int(np.rint(count_data_items(X_val)/batch_size))))\n",
        "    \n",
        "        if tf.__version__ == \"2.4.1\": # TODO: delete when tensorflow fixes the bug\n",
        "            scores = model.evaluate(get_dataset(X_train, y_train, batch_size = batch_size, train=False), \n",
        "                                    batch_size = batch_size, steps = max(1, int(np.rint(count_data_items(X_train)/batch_size))))\n",
        "            for i in range(len(model.metrics_names)):\n",
        "                history.history[model.metrics_names[i]][-1] = scores[i]\n",
        "            \n",
        "        folds_histories.append(history.history)\n",
        "        \n",
        "        if plot_fold_results:\n",
        "            plot_epochs_history(epochs, history.history)\n",
        "        \n",
        "    avg_history = avg_results_per_epoch(folds_histories)\n",
        "            \n",
        "    if plot_avg_results:\n",
        "        \n",
        "        plot_epochs_history(epochs, avg_history)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Results per fold')\n",
        "        for i in range(n_folds):\n",
        "            print('-'*80)\n",
        "            out = f\"> Fold {i + 1} - loss: {folds_histories[i]['loss'][-1]} - accuracy: {folds_histories[i]['accuracy'][-1]}\"\n",
        "            out += f\" - val_loss.: {folds_histories[i]['val_loss'][-1]} - val_accuracy: {folds_histories[i]['val_accuracy'][-1]}\"\n",
        "            print(out)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Average results over folds (on last epoch):')\n",
        "        print(f\"> loss: {avg_history['loss'][-1]}\")\n",
        "        print(f\"> accuracy: {avg_history['accuracy'][-1]}\")\n",
        "        print(f\"> cval_loss: {avg_history['val_loss'][-1]}\")\n",
        "        print(f\"> cval_accuracy: {avg_history['val_accuracy'][-1]}\")\n",
        "        print('-'*80)\n",
        "\n",
        "    return folds_histories\n",
        "\n",
        "def repeated_kfold(model_builder, train_filenames, n_folds, batch_size, epochs, reps=5, train_labels=None,\n",
        "                   stratify=True, shuffle=True, random_state=None, cbks=None):\n",
        "    \n",
        "    reps_histories = []\n",
        "    \n",
        "    for i in range(reps):\n",
        "        print(f'Repetition {i + 1}')\n",
        "        folds_histories = evaluate_model_kfold(model_builder, train_filenames, n_folds,\n",
        "                                             batch_size, epochs, train_labels=train_labels, stratify=stratify,\n",
        "                                             shuffle=shuffle, random_state=random_state, cbks=cbks)\n",
        "\n",
        "        reps_histories.append(folds_histories)\n",
        "\n",
        "    return reps_histories\n",
        "\n",
        "def test_model_rkfold(model_builder, results_filename):\n",
        "    # Evaluate model with repeated k-fold (because of the high variance)\n",
        "    reps_results = repeated_kfold(model_builder, X_train, FOLDS, BATCH_SIZE, EPOCHS, reps=REPS, train_labels=y_train,\n",
        "                   random_state=SEED, cbks=CBKS)\n",
        "    \n",
        "    # Save results to disk\n",
        "    f = open(results_filename, 'w' )\n",
        "    f.write(repr(reps_results))\n",
        "    f.close()\n",
        "    \n",
        "    \n",
        "def avg_results_per_epoch(histories):\n",
        "    \n",
        "    keys = list(histories[0].keys())\n",
        "    epochs = len(histories[0][keys[0]])\n",
        "    \n",
        "    avg_histories = dict()\n",
        "    for key in keys:\n",
        "        avg_histories[key] = [np.mean([x[key][i] for x in histories]) for i in range(epochs)]\n",
        "        \n",
        "    return avg_histories\n",
        "\n",
        "def avg_reps_results(reps_histories):\n",
        "    return avg_results_per_epoch([avg_results_per_epoch(history) for history in reps_histories])\n",
        "    \n",
        "def show_rkfold_results(results_file):\n",
        "    # Load results from disk\n",
        "    f = open(results_file, 'r')\n",
        "    reps_results = eval(f.read())\n",
        "    \n",
        "    reps_avgd_per_kfold = [avg_results_per_epoch(history) for history in reps_results]\n",
        "    reps_avg = avg_results_per_epoch(reps_avgd_per_kfold)\n",
        "    \n",
        "    # Plot final result over epochs\n",
        "    plot_epochs_history(EPOCHS, reps_avg)\n",
        "    \n",
        "    print('-'*80)\n",
        "    print('Results per repetition (on last epoch)')\n",
        "    for i in range(REPS):\n",
        "        print('-'*80)\n",
        "        print(f\"> Repetition {i + 1} - Loss: {reps_avgd_per_kfold[i]['val_loss'][-1]} - Accuracy : {reps_avgd_per_kfold[i]['val_accuracy'][-1]}\")\n",
        "\n",
        "    print('-'*80)\n",
        "    print('Average results over repetitions (on last epoch):')\n",
        "    print(f\"> Train Accuracy: {reps_avg['accuracy'][-1]}\")\n",
        "    print(f\"> Train Loss: {reps_avg['loss'][-1]}\")\n",
        "    print(f\"> CV accuracy: {reps_avg['val_accuracy'][-1]}\")\n",
        "    print(f\"> CV Loss: {reps_avg['val_loss'][-1]}\")\n",
        "    print('-'*80)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpZRhRjQ4mBA"
      },
      "source": [
        "# Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gadxiFghj32"
      },
      "source": [
        "SEED = 268 # arbitrary seed\n",
        "\n",
        "# Datasets\n",
        "TFREC_DATASETS = ['tfrec-ensemble_mri_pet']\n",
        "\n",
        "PET_SHAPE = (79, 95, 68, 1)\n",
        "MRI_SHAPE = (121, 145, 121, 1)\n",
        "\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 4\n",
        "REPS = 5\n",
        "FOLDS = 10\n",
        "LR = 0.00001\n",
        "CBKS = None\n",
        "\n",
        "CLASSES = ['NOR', 'AD', 'MCI']\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "METRICS = ['accuracy']\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "INPUT_DATAPATH = '/content/drive/MyDrive/data/'\n",
        "METADATA_PATH = '/content/drive/MyDrive/data/'\n",
        "\n",
        "\n",
        "def select_dataset(ds_id):\n",
        "    global DS, PET_SHAPE, DS_PATH, INPUT_DATAPATH\n",
        "    DS = TFREC_DATASETS[ds_id]\n",
        "    # IMG_SHAPE = SHAPES[ds_id]\n",
        "    if INPUT_DATAPATH == None:\n",
        "        user_secrets = UserSecretsClient()\n",
        "        user_credential = user_secrets.get_gcloud_credential()\n",
        "        user_secrets.set_tensorflow_credential(user_credential)\n",
        "        DS_PATH = KaggleDatasets().get_gcs_path(DS)\n",
        "    else:\n",
        "        DS_PATH = INPUT_DATAPATH + DS\n",
        "\n",
        "\n",
        "select_dataset(0)\n",
        "\n",
        "metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "\n",
        "X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "test_model_rkfold(build_model, 'ensemble_results.txt')\n",
        "show_rkfold_results('ensemble_results.txt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}