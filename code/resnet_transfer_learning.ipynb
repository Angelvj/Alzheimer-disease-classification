{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet_transfer_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+Sn2K77XKpv7peto1XO/G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/resnet_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqdvQPniOkCg"
      },
      "source": [
        "# Load data and test models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTVLQkcf3vSk",
        "outputId": "85df6ac9-12a3-4aeb-e447-c63ca4d3dded"
      },
      "source": [
        "kaggle = False\n",
        "\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import keras\n",
        "import os, shutil, re\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "if kaggle:\n",
        "    from kaggle_datasets import KaggleDatasets\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "else:\n",
        "    from google.colab import drive\n",
        "import nibabel as nib\n",
        "\n",
        "# Import the most used layers\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Input, BatchNormalization, Dropout\n",
        "\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OqBaTPgOy3u",
        "outputId": "b0b26f3d-4212-49d9-c3b4-65fc004948d4"
      },
      "source": [
        "DEVICE = 'TPU' # or TPU\n",
        "tpu = None\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU, setting default strategy')\n",
        "        tpu = None\n",
        "        STRATEGY = tf.distribute.get_strategy()\n",
        "elif DEVICE == 'GPU':\n",
        "    STRATEGY = tf.distribute.MirroredStrategy()\n",
        "    \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = STRATEGY.num_replicas_in_sync\n",
        "\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not connect to TPU, setting default strategy\n",
            "Number of accelerators: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx2vEweUPCnS"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "    \n",
        "    tfrec_format = {\n",
        "        \"image\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"one_hot_label\": tf.io.VarLenFeature(tf.float32)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, tfrec_format)\n",
        "    one_hot_label = tf.sparse.to_dense(example['one_hot_label'])\n",
        "    one_hot_label = tf.reshape(one_hot_label, [NUM_CLASSES])\n",
        "    image = tf.reshape(tf.sparse.to_dense(example['image']), IMG_SHAPE)\n",
        "    # TPU needs size to be known statically, so this doesn't work\n",
        "    #     image  = tf.reshape(example['image'], example['shape']) \n",
        "    return image, one_hot_label\n",
        "\n",
        "# =========================================================================\n",
        "# The three functions below are used for generating a dataset directly from\n",
        "# the filenames of the data (via a callable generator).\n",
        "# ==========================================================================\n",
        "\n",
        "def load_image(path):    \n",
        "\n",
        "    img = nib.load(path)\n",
        "    img = np.asarray(img.dataobj, dtype=np.float32)\n",
        "    img = np.expand_dims(img, axis=3) # Add channel axis\n",
        "    return img\n",
        "\n",
        "def parse_file(filename, label):\n",
        "\n",
        "    image = load_image(filename)\n",
        "    image = np.nan_to_num(image, copy=False)\n",
        "    label = np.eye(3, dtype=np.float32)[label]\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def generator_fn(filenames, labels):\n",
        "\n",
        "    def images_generator():\n",
        "\n",
        "        for X, y in zip(filenames, labels):\n",
        "            X, y = parse_file(X, y)\n",
        "            yield X, y\n",
        "            \n",
        "    return images_generator\n",
        "\n",
        "def load_dataset(filenames, labels, use_tfrec, no_order=True):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        # Allow order-altering optimizations\n",
        "        option_no_order = tf.data.Options()\n",
        "        option_no_order.experimental_deterministic = False\n",
        "        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "        if no_order:\n",
        "            dataset = dataset.with_options(option_no_order)\n",
        "        dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO)\n",
        "\n",
        "    else:\n",
        "        dataset = tf.data.Dataset.from_generator(generator_fn(filenames, labels),\n",
        "            output_signature=(\n",
        "                 tf.TensorSpec(shape=IMG_SHAPE, dtype=tf.float32),\n",
        "                 tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames, use_tfrec):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
        "            for filename in filenames]\n",
        "        return np.sum(n)\n",
        "    else:\n",
        "        return len(filenames)\n",
        "\n",
        "def get_dataset(filenames, labels=None, use_tfrec=True, batch_size = 4, train=False, augment=False, cache=True, no_order=True):\n",
        "    \n",
        "    dataset =  load_dataset(filenames, labels, use_tfrec, no_order)\n",
        "    \n",
        "    if cache:\n",
        "        dataset = dataset.cache() # Do it only if dataset fits in ram\n",
        "    if train:\n",
        "        dataset = dataset.repeat()\n",
        "        if augment:\n",
        "            raise NotImplementedError\n",
        "#             dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "        dataset = dataset.shuffle(count_data_items(filenames, use_tfrec))\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CIYXXcCPE5G"
      },
      "source": [
        "# Only for pretraining\n",
        "def get_lr_callback(batch_size=4, verbose=False):\n",
        "    lr_start = 0.00001\n",
        "    lr_max = 0.00004 * batch_size\n",
        "    lr_min = 0.00001\n",
        "    lr_rampup_epochs = 3\n",
        "    lr_sustain_epochs = 0\n",
        "    lr_exp_decay = 0.7\n",
        "\n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_rampup_epochs:\n",
        "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
        "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
        "            lr = lr_max\n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
        "        return lr\n",
        "    \n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = verbose)\n",
        "    return lr_callback\n",
        "\n",
        "# For training from scratch\n",
        "def get_lr_decay_callback(batch_size=4, verbose=False):\n",
        "    lr_max = 0.00001\n",
        "    lr_exp_decay = 0.9\n",
        "    lr_min = 0.000001\n",
        "    \n",
        "    def lrfn(epoch):\n",
        "        lr = (lr_max - lr_min) * lr_exp_decay**(epoch) + lr_min\n",
        "        return lr\n",
        "        \n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = verbose)\n",
        "    return lr_callback"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9fTXQVAPJTY"
      },
      "source": [
        "def plot_epochs_history(num_epochs, history):\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(np.arange(num_epochs), history['accuracy'], '-o', label='Train acc',\n",
        "            color = '#ff7f0e')\n",
        "    plt.plot(np.arange(num_epochs), history['val_accuracy'], '-o', label='Val acc',\n",
        "            color = '#1f77b4')\n",
        "    x = np.argmax(history['val_accuracy']); y = np.max(history['val_accuracy'])\n",
        "    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x, y, s=150, color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max acc\\n%.2f'%y,size=14)\n",
        "    plt.ylabel('ACC', size=14); plt.xlabel('Epoch', size=14)\n",
        "    plt.legend(loc=2)\n",
        "    plt2 = plt.gca().twinx()\n",
        "    plt2.plot(np.arange(num_epochs),history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
        "    plt2.plot(np.arange(num_epochs),history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
        "    x = np.argmin(history['val_loss'] ); y = np.min(history['val_loss'])\n",
        "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x,y,s=150,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
        "    plt.ylabel('Loss',size=14)\n",
        "    plt.legend(loc=3)\n",
        "    plt.show()  \n",
        "    \n",
        "def plot_cm(labels, predictions):\n",
        "    \n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dbTiOnzPNCY"
      },
      "source": [
        "def evaluate_model_kfold(model_builder, train_filenames, n_folds, batch_size, epochs, \n",
        "                         plot_fold_results = True, plot_avg_results = True, train_labels=None, \n",
        "                         stratify=False, shuffle=True, random_state=None, use_tfrec=True):\n",
        "    \n",
        "    # np_rs = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(random_state)))\n",
        "    folds_histories = []\n",
        "\n",
        "    if stratify:\n",
        "        skf = StratifiedKFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "    else:\n",
        "        skf = KFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)\n",
        "\n",
        "    for fold, (idx_train, idx_val) in enumerate(skf.split(train_filenames, train_labels)):\n",
        "        if tpu != None:\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "        # np_rs.shuffle(idx_train)\n",
        "        X_train = train_filenames[idx_train]\n",
        "        X_val = train_filenames[idx_val]\n",
        "        y_train = None if use_tfrec is None else train_labels[idx_train]\n",
        "        y_val = None if use_tfrec is None else train_labels[idx_val]\n",
        "\n",
        "        # Build model\n",
        "        tf.keras.backend.clear_session()\n",
        "        with STRATEGY.scope():\n",
        "            model = model_builder(input_shape=IMG_SHAPE)\n",
        "            # Optimizers and Losses create TF variables --> should always be initialized in the scope\n",
        "            OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "            LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "            model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS, steps_per_execution=8)\n",
        "\n",
        "        # Train\n",
        "        print(f'Training for fold {fold + 1} of {n_folds}...')\n",
        "        cbks = [get_lr_decay_callback(batch_size)] # TODO: poner bien\n",
        "        history = model.fit(\n",
        "            get_dataset(X_train, y_train,  use_tfrec, train=True, augment=False, batch_size=batch_size), \n",
        "            epochs = EPOCHS, callbacks = cbks,\n",
        "            steps_per_epoch = max(1, int(np.rint(count_data_items(X_train, use_tfrec)/batch_size))),\n",
        "            validation_data = get_dataset(X_val, y_val, use_tfrec, batch_size = batch_size, train=False), \n",
        "            validation_steps= max(1, int(np.rint(count_data_items(X_val,use_tfrec)/batch_size))))\n",
        "    \n",
        "        if tf.__version__ == \"2.4.1\": # TODO: delete when tensorflow fixes the bug\n",
        "            scores = model.evaluate(get_dataset(X_train, y_train, use_tfrec, batch_size = batch_size, train=False), \n",
        "                                    batch_size = batch_size, steps = max(1, int(np.rint(count_data_items(X_train, use_tfrec)/batch_size))))\n",
        "            for i in range(len(model.metrics_names)):\n",
        "                history.history[model.metrics_names[i]][-1] = scores[i]\n",
        "            \n",
        "        folds_histories.append(history.history)\n",
        "        \n",
        "        if plot_fold_results:\n",
        "            plot_epochs_history(epochs, history.history)\n",
        "        \n",
        "    avg_history = avg_results_per_epoch(folds_histories)\n",
        "            \n",
        "    if plot_avg_results:\n",
        "        \n",
        "        plot_epochs_history(epochs, avg_history)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Results per fold')\n",
        "        for i in range(n_folds):\n",
        "            print('-'*80)\n",
        "            out = f\"> Fold {i + 1} - loss: {folds_histories[i]['loss'][-1]} - accuracy: {folds_histories[i]['accuracy'][-1]}\"\n",
        "            out += f\" - val_loss.: {folds_histories[i]['val_loss'][-1]} - val_accuracy: {folds_histories[i]['val_accuracy'][-1]}\"\n",
        "            print(out)\n",
        "\n",
        "        print('-'*80)\n",
        "        print('Average results over folds (on last epoch):')\n",
        "        print(f\"> loss: {avg_history['loss'][-1]}\")\n",
        "        print(f\"> accuracy: {avg_history['accuracy'][-1]}\")\n",
        "        print(f\"> cval_loss: {avg_history['val_loss'][-1]}\")\n",
        "        print(f\"> cval_accuracy: {avg_history['val_accuracy'][-1]}\")\n",
        "        print('-'*80)\n",
        "\n",
        "    return folds_histories\n",
        "\n",
        "def repeated_kfold(model_builder, train_filenames, n_folds, batch_size, epochs, reps=5, train_labels=None,\n",
        "                   stratify=True, shuffle=True, random_state=None, use_tfrec=True):\n",
        "    \n",
        "    reps_histories = []\n",
        "    \n",
        "    for i in range(reps):\n",
        "        print(f'Repetition {i + 1}')\n",
        "        folds_histories = evaluate_model_kfold(model_builder, train_filenames, n_folds,\n",
        "                                             batch_size, epochs, train_labels=train_labels, stratify=stratify,\n",
        "                                             shuffle=shuffle, random_state=random_state, use_tfrec=use_tfrec)\n",
        "\n",
        "        reps_histories.append(folds_histories)\n",
        "\n",
        "    return reps_histories\n",
        "\n",
        "def test_model_rkfold(model_builder, results_filename):\n",
        "    # Evaluate model with repeated k-fold (because of the high variance)\n",
        "    reps_results = repeated_kfold(model_builder, X_train, FOLDS, BATCH_SIZE, EPOCHS, reps=REPS, train_labels=y_train,\n",
        "                   random_state=SEED)\n",
        "    \n",
        "    # Save results to disk\n",
        "    f = open(results_filename, 'w' )\n",
        "    f.write(repr(reps_results))\n",
        "    f.close()\n",
        "    \n",
        "def train_model(model_builder):\n",
        "    # Test model\n",
        "    with STRATEGY.scope():\n",
        "        OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "        LOSS = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.00)\n",
        "        model = model_builder(IMG_SHAPE)\n",
        "        model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
        "\n",
        "\n",
        "    cbks = [get_lr_decay_callback(BATCH_SIZE)] # TODO: poner bien\n",
        "\n",
        "    history = model.fit(\n",
        "        get_dataset(X_train, train=True, batch_size=BATCH_SIZE), \n",
        "        epochs = EPOCHS, callbacks = cbks,\n",
        "        steps_per_epoch = int(np.rint(count_data_items(X_train, use_tfrec=True)/BATCH_SIZE))\n",
        "        )\n",
        "    \n",
        "    return model\n",
        "    \n",
        "def avg_results_per_epoch(histories):\n",
        "    \n",
        "    keys = list(histories[0].keys())\n",
        "    epochs = len(histories[0][keys[0]])\n",
        "    \n",
        "    avg_histories = dict()\n",
        "    for key in keys:\n",
        "        avg_histories[key] = [np.mean([x[key][i] for x in histories]) for i in range(epochs)]\n",
        "        \n",
        "    return avg_histories\n",
        "\n",
        "def avg_reps_results(reps_histories):\n",
        "    return avg_results_per_epoch([avg_results_per_epoch(history) for history in reps_histories])\n",
        "    \n",
        "def show_rkfold_results(results_file):\n",
        "    # Load results from disk\n",
        "    f = open(results_file, 'r')\n",
        "    reps_results = eval(f.read())\n",
        "    \n",
        "    reps_avgd_per_kfold = [avg_results_per_epoch(history) for history in reps_results]\n",
        "    reps_avg = avg_results_per_epoch(reps_avgd_per_kfold)\n",
        "    \n",
        "    # Plot final result over epochs\n",
        "    plot_epochs_history(EPOCHS, reps_avg)\n",
        "    \n",
        "    print('-'*80)\n",
        "    print('Results per repetition (on last epoch)')\n",
        "    for i in range(REPS):\n",
        "        print('-'*80)\n",
        "        print(f\"> Repetition {i + 1} - Loss: {reps_avgd_per_kfold[i]['val_loss'][-1]} - Accuracy : {reps_avgd_per_kfold[i]['val_accuracy'][-1]}\")\n",
        "\n",
        "    print('-'*80)\n",
        "    print('Average results over repetitions (on last epoch):')\n",
        "    print(f\"> Train Accuracy: {reps_avg['accuracy'][-1]}\")\n",
        "    print(f\"> Train Loss: {reps_avg['loss'][-1]}\")\n",
        "    print(f\"> CV accuracy: {reps_avg['val_accuracy'][-1]}\")\n",
        "    print(f\"> CV Loss: {reps_avg['val_loss'][-1]}\")\n",
        "    print('-'*80)\n",
        "    \n",
        "def show_test_results(model):\n",
        "    results = model.evaluate(get_dataset(X_test))\n",
        "\n",
        "    predictions = model.predict(get_dataset(X_test, no_order=False))\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "    plot_cm(y_test, y_pred)\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ7iVkKWPS94",
        "outputId": "3ea10a1b-e4ff-4ac9-faf8-034747804876"
      },
      "source": [
        "SEED = 268 # Arbitrary seed\n",
        "\n",
        "# Name of different datasets used\n",
        "TFREC_DATASETS = ['tfrec-pet-spatialnorm-elastic-maxintensitynorm',\n",
        "                  'tfrec-mri_grey-standarized']\n",
        "\n",
        "# IMG_DATASETS = ['ad-preprocessed', None, None]\n",
        "\n",
        "# Shape of images on each dataset\n",
        "SHAPES = [(79, 95, 68, 1), (121, 145, 121, 1)]\n",
        "\n",
        "# Number of repetitions and folds for repeated k-fold\n",
        "REPS = 5\n",
        "FOLDS = 10\n",
        "\n",
        "# Different classes on the dataset\n",
        "CLASSES = ['NOR', 'AD', 'MCI']\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "METRICS = ['accuracy']\n",
        "\n",
        "if kaggle:\n",
        "    INPUT_DATAPATH = '/kaggle/input/' if tpu is None else None\n",
        "    METADATA_PATH = '/kaggle/input/'\n",
        "else:\n",
        "    drive.mount('/content/drive') \n",
        "    INPUT_DATAPATH = '/content/drive/MyDrive/data/'\n",
        "    METADATA_PATH = '/content/drive/MyDrive/data/'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnbo0bbmQbZL"
      },
      "source": [
        "def select_dataset(ds_id):\n",
        "    global DS, IMG_SHAPE, DS_PATH, INPUT_DATAPATH\n",
        "    DS = TFREC_DATASETS[ds_id]\n",
        "    IMG_SHAPE = SHAPES[ds_id]\n",
        "    if INPUT_DATAPATH == None:\n",
        "        user_secrets = UserSecretsClient()\n",
        "        user_credential = user_secrets.get_gcloud_credential()\n",
        "        user_secrets.set_tensorflow_credential(user_credential)\n",
        "        DS_PATH = KaggleDatasets().get_gcs_path(DS)\n",
        "    else:\n",
        "        DS_PATH = INPUT_DATAPATH + DS"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dodxf3QOTb0"
      },
      "source": [
        "# ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAvDQlP8HD9R"
      },
      "source": [
        "from __future__ import (\n",
        "    absolute_import,\n",
        "    division,\n",
        "    print_function,\n",
        "    unicode_literals\n",
        ")\n",
        "import six\n",
        "from math import ceil\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv3D,\n",
        "    AveragePooling3D,\n",
        "    MaxPooling3D\n",
        ")\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "# This code is an adaptation of https://github.com/raghakot/keras-resnet/blob/master/resnet.py \n",
        "# to 3D\n",
        "\n",
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\n",
        "        \"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv3D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, kernel_initializer=kernel_initializer,\n",
        "                      padding=padding,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv3D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, kernel_initializer=kernel_initializer,\n",
        "                      padding=padding,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = ceil(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS])\n",
        "    stride_height = ceil(input_shape[COL_AXIS] / residual_shape[COL_AXIS])\n",
        "    stride_depth = ceil(input_shape[DEPTH_AXIS] / residual_shape[DEPTH_AXIS])\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or stride_depth > 1 \\\n",
        "            or not equal_channels:\n",
        "        shortcut = Conv3D(\n",
        "            filters=residual_shape[CHANNEL_AXIS],\n",
        "            kernel_size=(1, 1, 1),\n",
        "            strides=(stride_width, stride_height, stride_depth),\n",
        "            kernel_initializer=\"he_normal\", padding=\"valid\",\n",
        "            kernel_regularizer=l2(1e-4)\n",
        "            )(input)\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def _residual_block(block_function, filters, kernel_regularizer, repetitions,\n",
        "                      is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            strides = (1, 1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                strides = (2, 2, 2)\n",
        "            input = block_function(filters=filters, strides=strides,\n",
        "                                   kernel_regularizer=kernel_regularizer,\n",
        "                                   is_first_block_of_first_layer=(\n",
        "                                       is_first_layer and i == 0)\n",
        "                                   )(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, strides=(1, 1, 1), kernel_regularizer=l2(1e-4),\n",
        "                is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv3D(filters=filters, kernel_size=(3, 3, 3),\n",
        "                           strides=strides, padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=kernel_regularizer\n",
        "                           )(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters,\n",
        "                                    kernel_size=(3, 3, 3),\n",
        "                                    strides=strides,\n",
        "                                    kernel_regularizer=kernel_regularizer\n",
        "                                    )(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3, 3),\n",
        "                                   kernel_regularizer=kernel_regularizer\n",
        "                                   )(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, strides=(1, 1, 1), kernel_regularizer=l2(1e-4),\n",
        "               is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv3D(filters=filters, kernel_size=(1, 1, 1),\n",
        "                              strides=strides, padding=\"same\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=kernel_regularizer\n",
        "                              )(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1, 1),\n",
        "                                       strides=strides,\n",
        "                                       kernel_regularizer=kernel_regularizer\n",
        "                                       )(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3, 3),\n",
        "                                   kernel_regularizer=kernel_regularizer\n",
        "                                   )(conv_1_1)\n",
        "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1, 1),\n",
        "                                   kernel_regularizer=kernel_regularizer\n",
        "                                   )(conv_3_3)\n",
        "\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "def _handle_data_format():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global DEPTH_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        ROW_AXIS = 1\n",
        "        COL_AXIS = 2\n",
        "        DEPTH_AXIS = 3\n",
        "        CHANNEL_AXIS = 4\n",
        "    else:\n",
        "        CHANNEL_AXIS = 1\n",
        "        ROW_AXIS = 2\n",
        "        COL_AXIS = 3\n",
        "        DEPTH_AXIS = 4\n",
        "\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "\n",
        "class ResnetBuilder(object):\n",
        "    \"\"\"ResNet.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions, reg_factor):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_data_format()\n",
        "        if len(input_shape) != 4:\n",
        "            raise ValueError(\"Input should have 4 dimensions\")\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        block_fn = _get_block(block_fn)\n",
        "\n",
        "        input = Input(shape=input_shape)\n",
        "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7, 7), strides=(2, 2, 2), kernel_regularizer=l2(reg_factor))(input)\n",
        "        pool1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "        block = pool1\n",
        "        filters = 64\n",
        "        for i, r in enumerate(repetitions):\n",
        "            block = _residual_block(block_fn, filters=filters, kernel_regularizer=l2(reg_factor), repetitions=r, is_first_layer=(i == 0))(block)\n",
        "            filters *= 2\n",
        "\n",
        "        # last activation\n",
        "        block = _bn_relu(block)\n",
        "        block_shape = K.int_shape(block)\n",
        "\n",
        "        # Classifier block\n",
        "        pool2 = AveragePooling3D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS], block_shape[DEPTH_AXIS]), strides=(1, 1, 1))(block)\n",
        "        flatten1 = Flatten()(pool2)\n",
        "        if num_outputs > 1:\n",
        "            dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\", activation=\"softmax\", kernel_regularizer=l2(reg_factor))(flatten1)\n",
        "        else:\n",
        "            dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\", activation=\"sigmoid\", kernel_regularizer=l2(reg_factor))(flatten1)\n",
        "\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "        return model\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet(num_layers, input_shape, num_outputs, reg_factor=1e-4):\n",
        "        \"\"\"Build resnet 18, 34, 50, 101 or 152\"\"\"\n",
        "\n",
        "        repetitions = {18: [2, 2, 2, 2], \n",
        "                       34: [3, 4, 6, 3],\n",
        "                       50: [3, 4, 6, 3],\n",
        "                       101: [2, 4, 23, 3],\n",
        "                       152: [3, 8, 36, 3]}\n",
        "\n",
        "        block_fn = {18: basic_block, 34: basic_block, 50: bottleneck,\n",
        "                    101: bottleneck, 152:bottleneck}\n",
        "\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, block_fn[num_layers],\n",
        "                                     repetitions[num_layers], reg_factor=reg_factor)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoySfiMUM2zf"
      },
      "source": [
        "# Load pretrained resnet\n",
        "\n",
        "Load pretrained resnet and freeze weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9FsCPBsbvlQ"
      },
      "source": [
        "# Load pretrained resnet\n",
        "base_model = tf.keras.models.load_model('/content/drive/MyDrive/pretrained_models/pretrained_3D_resnet18.h5')\n",
        "# Freeze all parameters\n",
        "base_model.trainable = False\n",
        "# Delete last layer (classifier)\n",
        "base_model = tf.keras.Model(base_model.input, base_model.layers[-3].output)\n",
        "\n",
        "\n",
        "def build_pretrained_resnet(input_shape):\n",
        "    pretrained_model = ResnetBuilder.build_resnet(18, input_shape, 3)\n",
        "    # Delete classifier\n",
        "    pretrained_model = keras.Model(pretrained_model.input, pretrained_model.layers[-3].output)\n",
        "    # Set pretrained weights\n",
        "    pretrained_model.set_weights(base_model.get_weights())\n",
        "    # Freeze convolutional base\n",
        "    pretrained_model.trainable = False\n",
        "    # Add classifier at the end\n",
        "    inputs = keras.Input(shape = input_shape)\n",
        "    x = pretrained_model(inputs, training = False)\n",
        "    x = keras.layers.GlobalAveragePooling3D()(x)\n",
        "    x = keras.layers.Dense(256)(x)\n",
        "    outputs = keras.layers.Dense(3, activation='softmax')(x)\n",
        "    pretrained_model = keras.Model(inputs, outputs)\n",
        "\n",
        "    return pretrained_model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edXi9NlEOY72"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcTfQynTM9Ug"
      },
      "source": [
        "# PET\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x1Y-kl9cHQW"
      },
      "source": [
        "# select_dataset(0)\n",
        "\n",
        "# metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "# metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "\n",
        "# X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "# y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "# X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "# y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "# LR = 0.00001\n",
        "# BATCH_SIZE = 4\n",
        "# EPOCHS = 50\n",
        "\n",
        "# test_model_rkfold(build_pretrained_resnet, 'pet-pretrained-resnet18.txt')\n",
        "# show_rkfold_results('pet-pretrained-resnet18.txt')\n",
        "# model = train_model(build_pretrained_resnet)\n",
        "# show_test_results(model)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj4eUCFrNfuy"
      },
      "source": [
        "# MRI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRkGjhgzcNhX",
        "outputId": "219a8127-05fa-48ef-a0aa-78ac99438f7f"
      },
      "source": [
        "select_dataset(1)\n",
        "\n",
        "metadata_train = pd.read_csv(METADATA_PATH + DS + '/train/train_summary.csv', encoding='utf-8')\n",
        "metadata_test = pd.read_csv(METADATA_PATH + DS + '/test/test_summary.csv', encoding='utf-8')\n",
        "\n",
        "X_train = DS_PATH + '/train/' + metadata_train.iloc[:, 0].to_numpy()\n",
        "y_train = np.argmax(metadata_train.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "X_test = DS_PATH + '/test/' + metadata_test.iloc[:, 0].to_numpy()\n",
        "y_test = np.argmax(metadata_test.iloc[:,-len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "LR = 0.00001\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "\n",
        "test_model_rkfold(build_pretrained_resnet, 'mri-pretrained-resnet18.txt')\n",
        "show_rkfold_results('mri-pretrained-resnet18.txt')\n",
        "model = train_model(build_pretrained_resnet)\n",
        "show_test_results(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Repetition 1\n",
            "Training for fold 1 of 10...\n",
            "Epoch 1/50\n",
            " 8/45 [====>.........................] - ETA: 11:10 - loss: 6.9694 - accuracy: 0.2812"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}