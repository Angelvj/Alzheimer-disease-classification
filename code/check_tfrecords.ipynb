{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "check_tfrecords.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/0IkyyEGZ/fp2X/DBbJGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/check_tfrecords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sv2EmILRh7i"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao_cM-jItjry"
      },
      "source": [
        "import numpy as np, os\n",
        "import tensorflow as tf\n",
        "import nibabel as nib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpGW0EszRlcn"
      },
      "source": [
        "# Kaggle only\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "from kaggle_secrets import UserSecretsClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0zdaqeKRoGZ"
      },
      "source": [
        "if os.path.exists('cloned_repo'):\n",
        "    shutil.rmtree('cloned_repo')\n",
        "    \n",
        "!git clone -l -s https://github.com/Angelvj/TFG.git cloned_repo\n",
        "\n",
        "# Imports from my github repo\n",
        "from cloned_repo.code.image_reading import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_IGr7H0RqwI"
      },
      "source": [
        "# Initialize TPU (if pressent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcw-HJ-GRuPP"
      },
      "source": [
        "DEVICE = \"TPU\"\n",
        "\n",
        "if DEVICE == \"TPU\":\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU')\n",
        "        tpu = None\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "        print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "AUTO     = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNPqox6fR2Td"
      },
      "source": [
        "# Acess GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcevuQp-R4_G"
      },
      "source": [
        "# Kaggle only\n",
        "user_secrets = UserSecretsClient()\n",
        "user_credential = user_secrets.get_gcloud_credential()\n",
        "\n",
        "user_secrets.set_tensorflow_credential(user_credential)\n",
        "\n",
        "GCS_DS_PATH = KaggleDatasets().get_gcs_path('ad-preprocessed-tfrecords-20skf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SadRUp2iR7Ji"
      },
      "source": [
        "# Google Colab\n",
        "GCS_DS_PATH = 'set path to GCS here'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7HjXLveR-13"
      },
      "source": [
        "# Read images and labels from TFRecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0u3bCclR919"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "    tfrec_format = {\n",
        "        \"image\": tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.float32, allow_missing=True),\n",
        "#         \"image\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"one_hot_label\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"shape\": tf.io.FixedLenFeature([4], tf.int64),\n",
        "        \"filename\": tf.io.FixedLenFeature([], tf.string) # Only for test, TODO: delete\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, tfrec_format)\n",
        "    one_hot_label = tf.sparse.to_dense(example['one_hot_label'])\n",
        "    one_hot_label = tf.reshape(one_hot_label, [3])\n",
        "    image  = tf.reshape(example['image'], example['shape'])\n",
        "#     label = example['label']\n",
        "\n",
        "    return image, one_hot_label\n",
        "\n",
        "def load_dataset(filenames):\n",
        "    # Allow order-altering optimizations\n",
        "    \n",
        "    option_no_order = tf.data.Options()\n",
        "    option_no_order.experimental_deterministic = False\n",
        "    \n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    dataset = dataset.with_options(option_no_order)\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjO8EFXdSGIe"
      },
      "source": [
        "def data_augment(image, one_hot_class):\n",
        "#     Call here image augmentation functions\n",
        "#     image = tf.image.random_flip_left_right(image)\n",
        "#     image = tf.image.random_saturation(image, 0, 2)\n",
        "    return image, one_hot_class\n",
        "\n",
        "def get_batched_dataset(filenames, batch_size = 4, train=False, augment=True):\n",
        "    dataset =  load_dataset(filenames)\n",
        "    dataset = dataset.cache() # Only if dataset fits in ram\n",
        "    if train:\n",
        "        dataset = dataset.repeat()\n",
        "#         if augment:\n",
        "#             dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "        dataset = dataset.shuffle(len(filenames)) # Not for shure\n",
        "    dataset = dataset.batch(batch_size * REPLICAS)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDmT_Cfw2WTE"
      },
      "source": [
        "# Create datasets (examples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prWex_LHmb4J"
      },
      "source": [
        "# GCS_DS_PATH = '/kaggle/input/ad-preprocessed-tfrecords-20skf'\n",
        "\n",
        "get_filenames = lambda pattern : tf.io.gfile.glob(pattern)\n",
        "\n",
        "pet_train = get_batched_dataset(get_filenames(GCS_DS_PATH + '/PET/train/*.tfrec'), train=True, batch_size=8)\n",
        "pet_test = get_batched_dataset(get_filenames(GCS_DS_PATH + '/PET/test/*.tfrec'), train=False, batch_size=8)\n",
        "mri_grey_train = get_batched_dataset(get_filenames(GCS_DS_PATH + '/MRI/grey/train/*.tfrec'), train=True, batch_size=8)\n",
        "mri_grey_test = get_batched_dataset(get_filenames(GCS_DS_PATH + '/MRI/grey/test/*.tfrec'), train=False, batch_size=8)\n",
        "mri_white_train = get_batched_dataset(get_filenames(GCS_DS_PATH + '/MRI/white/train/*.tfrec'), train=True, batch_size=8)\n",
        "mri_white_test = get_batched_dataset(get_filenames(GCS_DS_PATH + '/MRI/white/test/*.tfrec'), train=False, batch_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
