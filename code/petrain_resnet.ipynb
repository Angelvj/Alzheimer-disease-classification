{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "petrain_resnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPKfH0bcyG5e2ErI3G9XKbZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/petrain_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR2D7flG3_3Z"
      },
      "source": [
        "# Define ResNet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjckDTURxaeT"
      },
      "source": [
        "from __future__ import (\n",
        "    absolute_import,\n",
        "    division,\n",
        "    print_function,\n",
        "    unicode_literals\n",
        ")\n",
        "import six\n",
        "from math import ceil\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv3D,\n",
        "    AveragePooling3D,\n",
        "    MaxPooling3D\n",
        ")\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "# This code is an adaptation of https://github.com/raghakot/keras-resnet/blob/master/resnet.py \n",
        "# to 3D\n",
        "\n",
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\n",
        "        \"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv3D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, kernel_initializer=kernel_initializer,\n",
        "                      padding=padding,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv3D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, kernel_initializer=kernel_initializer,\n",
        "                      padding=padding,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = ceil(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS])\n",
        "    stride_height = ceil(input_shape[COL_AXIS] / residual_shape[COL_AXIS])\n",
        "    stride_depth = ceil(input_shape[DEPTH_AXIS] / residual_shape[DEPTH_AXIS])\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or stride_depth > 1 \\\n",
        "            or not equal_channels:\n",
        "        shortcut = Conv3D(\n",
        "            filters=residual_shape[CHANNEL_AXIS],\n",
        "            kernel_size=(1, 1, 1),\n",
        "            strides=(stride_width, stride_height, stride_depth),\n",
        "            kernel_initializer=\"he_normal\", padding=\"valid\",\n",
        "            kernel_regularizer=l2(1e-4)\n",
        "            )(input)\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def _residual_block(block_function, filters, kernel_regularizer, repetitions,\n",
        "                      is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            strides = (1, 1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                strides = (2, 2, 2)\n",
        "            input = block_function(filters=filters, strides=strides,\n",
        "                                   kernel_regularizer=kernel_regularizer,\n",
        "                                   is_first_block_of_first_layer=(\n",
        "                                       is_first_layer and i == 0)\n",
        "                                   )(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, strides=(1, 1, 1), kernel_regularizer=l2(1e-4),\n",
        "                is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv3D(filters=filters, kernel_size=(3, 3, 3),\n",
        "                           strides=strides, padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=kernel_regularizer\n",
        "                           )(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters,\n",
        "                                    kernel_size=(3, 3, 3),\n",
        "                                    strides=strides,\n",
        "                                    kernel_regularizer=kernel_regularizer\n",
        "                                    )(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3, 3),\n",
        "                                   kernel_regularizer=kernel_regularizer\n",
        "                                   )(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, strides=(1, 1, 1), kernel_regularizer=l2(1e-4),\n",
        "               is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv3D(filters=filters, kernel_size=(1, 1, 1),\n",
        "                              strides=strides, padding=\"same\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=kernel_regularizer\n",
        "                              )(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1, 1),\n",
        "                                       strides=strides,\n",
        "                                       kernel_regularizer=kernel_regularizer\n",
        "                                       )(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3, 3),\n",
        "                                   kernel_regularizer=kernel_regularizer\n",
        "                                   )(conv_1_1)\n",
        "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1, 1),\n",
        "                                   kernel_regularizer=kernel_regularizer\n",
        "                                   )(conv_3_3)\n",
        "\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "def _handle_data_format():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global DEPTH_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        ROW_AXIS = 1\n",
        "        COL_AXIS = 2\n",
        "        DEPTH_AXIS = 3\n",
        "        CHANNEL_AXIS = 4\n",
        "    else:\n",
        "        CHANNEL_AXIS = 1\n",
        "        ROW_AXIS = 2\n",
        "        COL_AXIS = 3\n",
        "        DEPTH_AXIS = 4\n",
        "\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "\n",
        "class ResnetBuilder(object):\n",
        "    \"\"\"ResNet.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions, reg_factor):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_data_format()\n",
        "        if len(input_shape) != 4:\n",
        "            raise ValueError(\"Input should have 4 dimensions\")\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        block_fn = _get_block(block_fn)\n",
        "\n",
        "        input = Input(shape=input_shape)\n",
        "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7, 7), strides=(2, 2, 2), kernel_regularizer=l2(reg_factor))(input)\n",
        "        pool1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "        block = pool1\n",
        "        filters = 64\n",
        "        for i, r in enumerate(repetitions):\n",
        "            block = _residual_block(block_fn, filters=filters, kernel_regularizer=l2(reg_factor), repetitions=r, is_first_layer=(i == 0))(block)\n",
        "            filters *= 2\n",
        "\n",
        "        # last activation\n",
        "        block = _bn_relu(block)\n",
        "        block_shape = K.int_shape(block)\n",
        "\n",
        "        # Classifier block\n",
        "        pool2 = AveragePooling3D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS], block_shape[DEPTH_AXIS]), strides=(1, 1, 1))(block)\n",
        "        flatten1 = Flatten()(pool2)\n",
        "        if num_outputs > 1:\n",
        "            dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\", activation=\"softmax\", kernel_regularizer=l2(reg_factor))(flatten1)\n",
        "        else:\n",
        "            dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\", activation=\"sigmoid\", kernel_regularizer=l2(reg_factor))(flatten1)\n",
        "\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "        return model\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet(num_layers, input_shape, num_outputs, reg_factor=1e-4):\n",
        "        \"\"\"Build resnet 18, 34, 50, 101 or 152\"\"\"\n",
        "\n",
        "        repetitions = {18: [2, 2, 2, 2], \n",
        "                       34: [3, 4, 6, 3],\n",
        "                       50: [3, 4, 6, 3],\n",
        "                       101: [2, 4, 23, 3],\n",
        "                       152: [3, 8, 36, 3]}\n",
        "\n",
        "        block_fn = {18: basic_block, 34: basic_block, 50: bottleneck,\n",
        "                    101: bottleneck, 152:bottleneck}\n",
        "\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, block_fn[num_layers],\n",
        "                                     repetitions[num_layers], reg_factor=reg_factor)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtC7sSic4GC9"
      },
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKonKA1426_M"
      },
      "source": [
        "import scipy\n",
        "import skimage.transform as transform\n",
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "def augment_image(img):\n",
        "\n",
        "    img = img.squeeze()\n",
        "    original_shape = img.shape\n",
        "    img = random_rotations(img, -20, 20)\n",
        "    # img = random_zoom(img, min=0.9, max=1.1)\n",
        "    # img = random_shift(img, max=0.2)\n",
        "    # img = random_flip(img)\n",
        "    img = downscale(img, original_shape)\n",
        "    img = np.expand_dims(img, axis=3) # Restore channel axis\n",
        "    return img\n",
        "\n",
        "def downscale(image, shape):\n",
        "    'For upscale, anti_aliasing should be false'\n",
        "    return transform.resize(image, shape, mode='constant', anti_aliasing=True)\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "def tf_augment_image(input):\n",
        "    \"\"\" Tensorflow can't manage numpy functions, we have to wrap our augmentation function \"\"\"\n",
        "    img = tf.numpy_function(augment_image, [input], tf.float32)\n",
        "    return img\n",
        "\n",
        "def random_rotations(img, min_angle, max_angle):\n",
        "    \"\"\"\n",
        "    Rotate 3D image randomly\n",
        "    \"\"\"\n",
        "    assert img.ndim == 3, \"Image must be 3D\"\n",
        "    rotation_axes = [(1, 0), (1, 2), (0, 2)]\n",
        "    # angle = np.random.randint(low=min_angle, high=max_angle+1)\n",
        "    angle= max_angle\n",
        "    axes_random_id = np.random.randint(low=0, high=len(rotation_axes))\n",
        "    axis = rotation_axes[axes_random_id] # Select a random rotation axis\n",
        "    return scipy.ndimage.rotate(img, angle, axes=axis)\n",
        "\n",
        "def random_zoom(img,min=0.7, max=1.2):\n",
        "    \"\"\"\n",
        "    Generate random zoom of a 3D image\n",
        "    \"\"\"\n",
        "    zoom = np.random.sample()*(max - min) + min # Generate random zoom between min and max\n",
        "    zoom_matrix = np.array([[zoom, 0, 0, 0],\n",
        "                            [0, zoom, 0, 0],\n",
        "                            [0, 0, zoom, 0],\n",
        "                            [0, 0, 0, 1]])\n",
        "    \n",
        "    return scipy.ndimage.interpolation.affine_transform(img, zoom_matrix)\n",
        "\n",
        "def random_flip(img):\n",
        "    \"\"\"\n",
        "    Flip image over a random axis\n",
        "    \"\"\"\n",
        "    axes = [0, 1, 2]\n",
        "    rand_axis = np.random.randint(len(axes))\n",
        "    img = img.swapaxes(rand_axis, 0)\n",
        "    img = img[::-1, ...]\n",
        "    img = img.swapaxes(0, rand_axis)\n",
        "    img = np.squeeze(img)\n",
        "    return img\n",
        "\n",
        "def random_shift(img, max=0.4):\n",
        "    \"\"\"\n",
        "    Random shift over a random axis\n",
        "    \"\"\"\n",
        "    (x, y, z) = img.shape\n",
        "    (max_shift_x, max_shift_y, max_shift_z) = int(x*max/2),int(y*max/2), int(z*max/2)\n",
        "    shift_x = np.random.randint(-max_shift_x, max_shift_x)\n",
        "    shift_y = np.random.randint(-max_shift_y,max_shift_y)\n",
        "    shift_z = np.random.randint(-max_shift_z,max_shift_z)\n",
        "\n",
        "    translation_matrix = np.array([[1, 0, 0, shift_x],\n",
        "                                   [0, 1, 0, shift_y],\n",
        "                                   [0, 0, 1, shift_z],\n",
        "                                   [0, 0, 0, 1]\n",
        "                                   ])\n",
        "\n",
        "    return scipy.ndimage.interpolation.affine_transform(img, translation_matrix)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLXDmYJA4zoP"
      },
      "source": [
        "# Load tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klh-1_4P4MpN"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "    \n",
        "    tfrec_format = {\n",
        "        \"image\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"one_hot_label\": tf.io.VarLenFeature(tf.float32)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, tfrec_format)\n",
        "    one_hot_label = tf.sparse.to_dense(example['one_hot_label'])\n",
        "    one_hot_label = tf.reshape(one_hot_label, [NUM_CLASSES])\n",
        "    image = tf.reshape(tf.sparse.to_dense(example['image']), IMG_SHAPE)\n",
        "\n",
        "    return image, one_hot_label\n",
        "\n",
        "\n",
        "def load_dataset(filenames, labels, use_tfrec, no_order=True):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        # Allow order-altering optimizations\n",
        "        option_no_order = tf.data.Options()\n",
        "        option_no_order.experimental_deterministic = False\n",
        "        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "        if no_order:\n",
        "            dataset = dataset.with_options(option_no_order)\n",
        "        dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO)\n",
        "\n",
        "    else:\n",
        "        dataset = tf.data.Dataset.from_generator(generator_fn(filenames, labels),\n",
        "            output_signature=(\n",
        "                 tf.TensorSpec(shape=IMG_SHAPE, dtype=tf.float32),\n",
        "                 tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames, use_tfrec):\n",
        "    \n",
        "    if use_tfrec:\n",
        "        n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
        "            for filename in filenames]\n",
        "        return np.sum(n)\n",
        "    else:\n",
        "        return len(filenames)\n",
        "\n",
        "def get_dataset(filenames, labels=None, use_tfrec=True, batch_size = 4, train=False, augment=False, cache=False, no_order=True):\n",
        "\n",
        "    dataset =  load_dataset(filenames, labels, use_tfrec, no_order)\n",
        "    \n",
        "    if cache:\n",
        "        dataset = dataset.cache() # Do it only if dataset fits in ram\n",
        "    if train:\n",
        "        dataset = dataset.repeat()\n",
        "\n",
        "        if augment:\n",
        "            dataset = dataset.map(lambda img, label: (tf_augment_image(img), label), num_parallel_calls=AUTO)\n",
        "\n",
        "        dataset = dataset.shuffle(count_data_items(filenames, use_tfrec))\n",
        "\n",
        "    dataset = dataset.map(lambda img, label: (tf_augment_image(img), label), num_parallel_calls=AUTO)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfEz-UX-22Db"
      },
      "source": [
        "model = ResnetBuilder.build_resnet(18, (128, 128, 64, 1), 2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ9zhbO85I8C"
      },
      "source": [
        "# Check augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjUhDBuf7ru7",
        "outputId": "8df46297-88ba-40e8-ab3b-32403d8ed4c8"
      },
      "source": [
        "DEVICE = 'TPU' # or TPU\n",
        "tpu = None\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU, setting default strategy')\n",
        "        tpu = None\n",
        "        STRATEGY = tf.distribute.get_strategy()\n",
        "elif DEVICE == 'GPU':\n",
        "    STRATEGY = tf.distribute.MirroredStrategy()\n",
        "    \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = STRATEGY.num_replicas_in_sync\n",
        "\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not connect to TPU, setting default strategy\n",
            "Number of accelerators: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac9R2sUq5L4a"
      },
      "source": [
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "\n",
        "\n",
        "SEED = 34\n",
        "NUM_CLASSES = 2\n",
        "IMG_SHAPE = (128, 128, 64, 1)\n",
        "LR = 0.00001\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 4\n",
        "USE_TFREC = True\n",
        "EPOCHS = 100\n",
        "CLASSES = ['normal', 'covid']\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DS_PATH = '/content/drive/My Drive/data/tfrec-covid19/' # or GCS path\n",
        "\n",
        "metadata = pd.read_csv(DS_PATH + '/covid_dataset_summary.csv', encoding='utf-8')\n",
        "\n",
        "X = DS_PATH + metadata.iloc[:, 0].to_numpy()\n",
        "y = np.argmax(metadata.iloc[:, -len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = SEED, stratify = y)\n",
        "y_train, y_val = None, None\n",
        "\n",
        "\n",
        "initial_learning_rate = 0.0001\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "\n",
        "with STRATEGY.scope():\n",
        "    OPT = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    LOSS = tf.keras.losses.BinaryCrossentropy()\n",
        "    model = ResnetBuilder.build_resnet(18, (128, 128, 64, 1), 2)\n",
        "    model.compile(optimizer = OPT, loss=LOSS, metrics= METRICS)\n",
        "\n",
        "\n",
        "cbks = [keras.callbacks.ModelCheckpoint(\n",
        "    \"pretrained_3D_resnet.h5\", save_best_only=True),\n",
        "        keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=15)\n",
        "    ]\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    get_dataset(X_train, None, USE_TFREC, train=True, augment=True, batch_size=BATCH_SIZE), \n",
        "    epochs = EPOCHS, callbacks = cbks,\n",
        "    steps_per_epoch = max(1, int(np.rint(count_data_items(X_train, USE_TFREC)/BATCH_SIZE))),\n",
        "    validation_data = get_dataset(X_val, None, USE_TFREC, batch_size = BATCH_SIZE, train=False), \n",
        "    validation_steps= max(1, int(np.rint(count_data_items(X_val, USE_TFREC)/BATCH_SIZE))))\n",
        "\n",
        "\n",
        "if tf.__version__ == \"2.4.1\": # TODO: delete when tensorflow fixes the bug\n",
        "    scores = model.evaluate(get_dataset(X_train, None, USE_TFREC, batch_size = BATCH_SIZE, train=False), \n",
        "                            batch_size = BATCH_SIZE, steps = max(1, int(np.rint(count_data_items(X_train, USE_TFREC)/BATCH_SIZE))))\n",
        "    for i in range(len(model.metrics_names)):\n",
        "        history.history[model.metrics_names[i]][-1] = scores[i]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}