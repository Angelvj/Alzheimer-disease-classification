{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "petrain_resnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPbZV/uy+pruOfrH3lgtkeU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/petrain_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hn7r5E0MPci"
      },
      "source": [
        "import shutil\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "if tf.io.gfile.exists('Alzheimer-disease-classification'):\n",
        "    shutil.rmtree('Alzheimer-disease-classification')\n",
        "! git clone https://github.com/Angelvj/Alzheimer-disease-classification.git\n",
        "\n",
        "sys.path.insert(0,'/content/Alzheimer-disease-classification/code')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tnpO2iwPsk7"
      },
      "source": [
        "from models import ResNet\n",
        "from functions.tfrec_loading import read_tfrecord, load_dataset, count_data_items\n",
        "from functions.data_augmentation import random_rotations, random_zoom, random_shift, downscale\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtC7sSic4GC9"
      },
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKonKA1426_M"
      },
      "source": [
        "def augment_image(img):\n",
        "\n",
        "    img = img.squeeze()\n",
        "    original_shape = img.shape\n",
        "    img = random_rotations(img, -20, 20)\n",
        "    # img = random_zoom(img, min=0.9, max=1.1)\n",
        "    # img = random_shift(img, max=0.2)\n",
        "    # img = random_flip(img)\n",
        "    img = downscale(img, original_shape)\n",
        "    img = np.expand_dims(img, axis=3) # Restore channel axis\n",
        "    return img\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "def tf_augment_image(input):\n",
        "    \"\"\" Tensorflow can't manage numpy functions, we have to wrap our augmentation function \"\"\"\n",
        "    img = tf.numpy_function(augment_image, [input], tf.float32)\n",
        "    return img"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLXDmYJA4zoP"
      },
      "source": [
        "# Load tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klh-1_4P4MpN"
      },
      "source": [
        "def get_dataset(filenames, img_shape, num_classes, autotune, batch_size = 4, \n",
        "                train=False, augment=False, cache=False, no_order=True):\n",
        "\n",
        "    dataset =  load_dataset(filenames, img_shape, num_classes, autotune, no_order)\n",
        "    if cache:\n",
        "        dataset = dataset.cache() # Do it only if dataset fits in ram\n",
        "    if train:\n",
        "        dataset = dataset.repeat()\n",
        "        if augment:\n",
        "            dataset = dataset.map(lambda img, label: (tf_augment_image(img), label), num_parallel_calls=autotune)\n",
        "\n",
        "        dataset = dataset.shuffle(count_data_items(filenames))\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510SUlJaA1tj"
      },
      "source": [
        "# Hardware config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjUhDBuf7ru7",
        "outputId": "d0fd0fcf-30c0-488e-cf0f-495bb3b8c04a"
      },
      "source": [
        "DEVICE = 'TPU' # or TPU\n",
        "tpu = None\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU, setting default strategy')\n",
        "        tpu = None\n",
        "        STRATEGY = tf.distribute.get_strategy()\n",
        "elif DEVICE == 'GPU':\n",
        "    STRATEGY = tf.distribute.MirroredStrategy()\n",
        "    \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = STRATEGY.num_replicas_in_sync\n",
        "\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not connect to TPU, setting default strategy\n",
            "Number of accelerators: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac9R2sUq5L4a"
      },
      "source": [
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "SEED = 34\n",
        "NUM_CLASSES = 2\n",
        "IMG_SHAPE = (128, 128, 64, 1)\n",
        "LR = 0.00001\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 4\n",
        "USE_TFREC = True\n",
        "EPOCHS = 100\n",
        "CLASSES = ['normal', 'covid']\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DS_PATH = '/content/drive/My Drive/data/tfrec-covid19/' # or GCS path\n",
        "\n",
        "metadata = pd.read_csv(DS_PATH + '/covid_dataset_summary.csv', encoding='utf-8')\n",
        "\n",
        "X = DS_PATH + metadata.iloc[:, 0].to_numpy()\n",
        "y = np.argmax(metadata.iloc[:, -len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = SEED, stratify = y)\n",
        "y_train, y_val = None, None\n",
        "\n",
        "\n",
        "initial_learning_rate = 0.0001\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "\n",
        "with STRATEGY.scope():\n",
        "    OPT = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    LOSS = tf.keras.losses.BinaryCrossentropy()\n",
        "    model = ResNet.ResnetBuilder.build_resnet(18, (128, 128, 64, 1), 2)\n",
        "    model.compile(optimizer = OPT, loss=LOSS, metrics= METRICS)\n",
        "\n",
        "\n",
        "cbks = [keras.callbacks.ModelCheckpoint(\n",
        "    \"pretrained_3D_resnet.h5\", save_best_only=True),\n",
        "        keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=15)\n",
        "    ]\n",
        "    \n",
        "\n",
        "history = model.fit(\n",
        "    get_dataset(X_train, IMG_SHAPE, NUM_CLASSES, AUTO, batch_size = BATCH_SIZE, train=True, augment=True, cache=True), \n",
        "    epochs = EPOCHS, callbacks = cbks,\n",
        "    steps_per_epoch = max(1, int(np.rint(count_data_items(X_train)/BATCH_SIZE))),\n",
        "    validation_data = get_dataset(X_val, IMG_SHAPE, NUM_CLASSES, AUTO, batch_size = BATCH_SIZE, train=False), \n",
        "    validation_steps= max(1, int(np.rint(count_data_items(X_val)/BATCH_SIZE))))\n",
        "\n",
        "\n",
        "if tf.__version__ == \"2.4.1\": # TODO: delete when tensorflow fixes the bug\n",
        "    scores = model.evaluate(get_dataset(X_train, IMG_SHAPE, NUM_CLASSES, AUTO, batch_size = BATCH_SIZE, train=False), \n",
        "                            batch_size = BATCH_SIZE, steps = max(1, int(np.rint(count_data_items(X_train, USE_TFREC)/BATCH_SIZE))))\n",
        "    for i in range(len(model.metrics_names)):\n",
        "        history.history[model.metrics_names[i]][-1] = scores[i]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}