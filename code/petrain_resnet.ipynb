{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "petrain_resnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOru/GnbJE82nkZbxDsjivT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelvj/Alzheimer-disease-classification/blob/main/code/petrain_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "panszd_DP1Gu"
      },
      "source": [
        "kaggle = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zgn0p6zQYiP"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hn7r5E0MPci"
      },
      "source": [
        "import shutil\n",
        "import sys\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "if not kaggle:\n",
        "    from google.colab import drive\n",
        "else:\n",
        "    from kaggle_datasets import KaggleDatasets\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "if tf.io.gfile.exists('Alzheimer-disease-classification'):\n",
        "    shutil.rmtree('Alzheimer-disease-classification')\n",
        "! git clone https://github.com/Angelvj/Alzheimer-disease-classification.git\n",
        "\n",
        "sys.path.insert(0,'/content/Alzheimer-disease-classification/code')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tnpO2iwPsk7"
      },
      "source": [
        "import functions.data_augmentation as augmentation\n",
        "import functions.io_utils as io\n",
        "import functions.lr_schedules as lr_schedules\n",
        "from models import ResNet\n",
        "from functions.model_evaluation import plot_epochs_history, get_dataset, count_data_items\n",
        "from functions.data_augmentation import random_rotations, random_zoom, random_shift, downscale"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510SUlJaA1tj"
      },
      "source": [
        "# Hardware config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjUhDBuf7ru7",
        "outputId": "f1fc32e8-4686-420d-a16d-4745d6810610"
      },
      "source": [
        "DEVICE = 'TPU' # or TPU\n",
        "tpu = None\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    except ValueError:\n",
        "        print('Could not connect to TPU, setting default strategy')\n",
        "        tpu = None\n",
        "        STRATEGY = tf.distribute.get_strategy()\n",
        "elif DEVICE == 'GPU':\n",
        "    STRATEGY = tf.distribute.MirroredStrategy()\n",
        "    \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = STRATEGY.num_replicas_in_sync\n",
        "\n",
        "print(f'Number of accelerators: {REPLICAS}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not connect to TPU, setting default strategy\n",
            "Number of accelerators: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PtyY6JVR2mu"
      },
      "source": [
        "# Pretrain resnet18 with COVID-19 data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac9R2sUq5L4a"
      },
      "source": [
        "def augment_image(img):\n",
        "    img = img.squeeze()\n",
        "    original_shape = img.shape\n",
        "    img = augmentation.random_rotations(img, -20, 20)\n",
        "    img = augmentation.random_zoom(img, min=0.9, max=1.1)\n",
        "    img = augmentation.random_shift(img, max=0.4)\n",
        "    img = augmentation.downscale(img, original_shape)\n",
        "    img = np.expand_dims(img, axis=3) # Restore channel's axis\n",
        "    return img\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "def tf_augment_image(input):\n",
        "    \"\"\" Tensorflow can't manage numpy functions, we have to wrap our augmentation function \"\"\"\n",
        "    img = tf.numpy_function(augment_image, [input], tf.float32)\n",
        "    return img\n",
        "\n",
        "TFREC_DATASET = 'tfrec-covid19/'\n",
        "SHAPE = (128, 128, 64, 1)\n",
        "CLASSES = ['normal', 'covid']\n",
        "\n",
        "if kaggle:\n",
        "    INPUT_DATAPATH = '/kaggle/input/' if tpu is None else None\n",
        "    METADATA_PATH = '/kaggle/input'\n",
        "else:\n",
        "    drive.mount('/content/drive')\n",
        "    INPUT_DATAPATH = '/content/drive/MyDrive/data/'\n",
        "    METADATA_PATH = '/content/drive/MyDrive/data/'\n",
        "\n",
        "if INPUT_DATAPATH == None:\n",
        "    user_secrets = UserSecretsClient()\n",
        "    user_credential = user_secrets.get_gcloud_credential()\n",
        "    user_secrets.set_tensorflow_credential(user_credential)\n",
        "    DS_PATH = KaggleDatasets().get_gcs_path(TFREC_DATASET)\n",
        "else:\n",
        "    DS_PATH = INPUT_DATAPATH + TFREC_DATASET\n",
        "\n",
        "metadata = pd.read_csv(METADATA_PATH + TFREC_DATASET + '/covid_dataset_summary.csv', encoding='utf')\n",
        "\n",
        "IMG_SHAPE = (128, 128, 64, 1)\n",
        "NUM_CLASSES = 2\n",
        "CLASSES = ['normal', 'covid']\n",
        "\n",
        "X = DS_PATH + metadata.iloc[:, 0].to_numpy()\n",
        "y = np.argmax(metadata.iloc[:, -len(CLASSES):].to_numpy(), axis=1)\n",
        "\n",
        "X_train, X_val, _, _ = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
        "\n",
        "LR = 0.00001\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 100\n",
        "\n",
        "with STRATEGY.scope():\n",
        "    OPT = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "    LOSS = tf.keras.losses.BinaryCrossentropy()\n",
        "    model = ResNet.ResnetBuilder.build_resnet(18, (128, 128, 64, 1), 2)\n",
        "    model.compile(optimizer = OPT, loss=LOSS, metrics= ['accuracy'])\n",
        "\n",
        "# Save best model in best epoch based on validation accuracy\n",
        "cbks = [keras.callbacks.ModelCheckpoint('pretrained_3D_resnet18.h5', save_best_only=True),\n",
        "        keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15),\n",
        "    ]\n",
        "\n",
        "history = model.fit(\n",
        "    get_dataset(X_train, IMG_SHAPE, NUM_CLASSES, AUTO, batch_size = BATCH_SIZE, train=True, augment=tf_augment_image, cache=True), \n",
        "    epochs = EPOCHS, callbacks = cbks,\n",
        "    steps_per_epoch = max(1, int(np.rint(count_data_items(X_train)/BATCH_SIZE))),\n",
        "    validation_data = get_dataset(X_val, IMG_SHAPE, NUM_CLASSES, AUTO, batch_size = BATCH_SIZE, train=False), \n",
        "    validation_steps= max(1, int(np.rint(count_data_items(X_val)/BATCH_SIZE))))\n",
        "\n",
        "tf.keras.models.save_model(model, 'pretrained_3D_resnet18.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}